软件 学报 
 JOURNAL   OF   SOFTWARE 
 2000 　 Vol.11 　 No.4 　 P.473 - 480 
 
 
 
 
 基于 统计 的 汉语 词性 标注 方法 的 分析 与 改进 
 魏欧 　 吴健 　 孙玉芳 
 　 摘要 　 从 词性 概率 矩阵 与 词汇 概率 矩阵 的 结构 和 数值 变化 等 方面 , 对 目前 常用 的 基于 统计 的 汉语 词性 标注 方法 中 训练 语料 规模 与 标注 正确率 之间 所 存在 的 非线性 关系 作 了 分析 . 为了 充分利用 训练 语料库 , 提高 标注 正确率 , 从 利用 词语 相关 的 语法 属性 和 加强 对 未知 词 的 处理 两个 方面 加以改进 , 提高 了 标注 性能 . 封闭 测试 和 开放 测试 的 正确率 分别 达到 96.5% 和 96% . 
 　 关键词 　 词性 标注 , n 元 语法 , 语料 , 语法 属性 . 
 　 中图法 分类号 　 TP18 
 Analysis   and   Improvement   of   Statistics - Based   Chinese   Part - of - Speech   Tagging 
 WEI   Ou 　 WU   Jian 　 SUN   Yu - fang 
 （ Institute   of   Software 　 The   Chinese   Academy   of   Sciences 　 Beijing 　 100080 ） 
 Abstract 　 　 In   this   paper ,   a   popular   statistics － based   training   and   tagging   method   for   Chinese   texts   is   studied ,   and   the   nonlinear   relation   between   training   set   and   tagging   accuracy   is   analyzed   from   the   aspects   of   the   structure   and   numerical   value   of   the   matrix   of   transition   probabilities   and   the   matrix   of   symbol   probabilities .   In   order   to   make   use   of   training   corpus   sufficiently   and   get   the   higher   tagging   accuracy ,   the   training   and   tagging   method   is   improved   from   two   aspects :   using   other   grammatical   attributes   of   words ,   and   strengthening   the   processing   of   unknown   words .   With   the   improved   method ,   open   test   and   close   test   showed   that   the   overall   accuracies   are   about   96.5%   and   96%   respectively . 
 Key   words 　 Part - of - Speech   tagging ,   n - gram ,   corpus ,   grammatical   attribute . 
 　 　   词性 标注 的 作用 就是 通过 采取 适当 的 方法 , 根据 上下文 的 语境 关系 , 消除 句子 中词 的 语法 兼类 , 使得 无论 一个 词 兼有 几种 词性 , 在 特定 的 场合 下 只 保留 其中 最合适 的 一种 . 词性 标注 在 许多 应用领域 中 都 是 一个 重要 的 实际 问题 , 在 自然语言 处理 中 也 是 一个 很 基础 的 课题 , 对于 词性 自动 标注 方法 的 研究 和 讨论 具有 重要 的 意义 . 近年来 , 随着 计算机技术 的 发展 , 可用 语料库 数量 的 不断 增大 , 基于 统计 的 自然语言 处理 方法 逐渐 成为 目前 计算 语言学 中 的 一个 研究 热点 . 对于 基于 统计 的 汉语 词性 标注 技术 , 国内 的 研究 人员 也 进行 了 很多 有益 的 探索 ［ 1 ］ . 
 本文 首先 根据 目前 常用 的 相对 频率 训练方法 获取 二元 语法 模型 的 参数 , 并 采用 Viterbi 算法 对 汉语 词性 标注 进行 实验 , 然后 从 词性 概率 矩阵 与 词汇 概率 矩阵 的 结构 和 数值 变化 等 方面 对 训练 语料 规模 与 标注 正确率 之间 所 存在 的 非线性 关系 作 了 分析 . 为了 充分利用 训练 语料 , 提高 标注 正确率 , 本文 从 利用 词语 相关 的 语法 属性 和 加强 对 未知 词 的 处理 两个 方面 加以改进 , 提高 了 标注 性能 . 封闭 测试 和 开放 测试 的 正确率 分别 达到 96.5% 和 96% . 
 1 　 词性 标注 的 统计 语言学 模型 
 1.1 　 n - 元 ( n - gram ) 语法 模型 
 　 　 设 W 是 词 汇集 , T 是 词性 标记 集 , 对于 一个 给定 的 词串 S = S1 , S2 , ... , St , ... , SM , 其中 任何 一个 词 St ∈ W . 我们 要 根据 一定 的 策略 , 从 S 产生 的 所有 可能 的 标记 串中 找出 最 适合 该 特定 词串 的 一个 标记 序列 CS = C1C2 ... Ct ... CM , Ct ∈ T . 
 　 　 记 P ( CS | S ) 为 在 给定 输入 词串 S 的 条件 下 所 产生 的 输出 标记 串 CS 的 后验 概率 . 根据 贝叶斯 公式 , 有 
 
 其中 P ( CS ) 是 标记 串 CS 的 先验概率 , P ( S | CS ) 是 在 标记 串 CS 已知 情况 下 词串 S 产生 的 条件 概率 , P ( S ) 是 词串 S 的 非 条件 概率 . 词性 标注 的 作用 就是 要 找到 这样 的 标记 串 C ′ S , 使得 
 　 　 为了 减少 参数 空间 的 规模 , 可以 假设 St 的 出现 只 与其 自身 的 词性 Ct 相关 , 而 与 前 t - 1 个 词 无关 . 另外 , 还 可以 假设 局部 的 上下文 信息 对于 Ct 的 出现 是 足够 的 , 认为 Ct 的 出现 只 与 紧接着 第 t 个 词 的 前面 的 很少 的 ( n - 1 ) ( n > 1 ) 个 词 的 词性 相关 . 这样 的 模型 称为 n 元 语法 模型 . 它 实际上 是 一个 n - 1 阶 的 马尔可夫 过程 , 如果 取 n = 2 , 这时 采用 的 就是 二元 语法 模型 ( bi - gram ) , 对于 二元 语法 模型 , 有 
 
 其中 P ( Ct | Ct - 1 ) 只 与 相邻 词 的 词性 有关 , 我们 称为 词性 概率 参数 ； P ( St | Ct ) 既 与 词性 相关 , 又 与 词 本身 相关 , 我们 称 此项 为 词汇 概率 参数 . 
 　 　 对于 词性 标记 集 T , 词 汇集 W , 假设 T 中共 有 NT 个 标记 , W 中共 有 NW 个 词汇 , 那么 所有 的 词性 概率 参数 组成 一个 NT × NT 的 二维 矩阵 ANT × NT , 其中 任一 元素 aij ( 1 ≤ i , j ≤ NT ) 表示 从 词性 标记 Ti 到 Tj 的 转移 概率 P ( Tj | Ti ) ； 所有 的 词汇 概率 参数 组成 一个 NT × NW 的 二维 矩阵 ANT × NT , 其中 任一 元素 bjk ( 1 ≤ j ≤ NT , 1 ≤ k ≤ NW ) 表示 ， 在 出现 词性 标记 Tj 时 产生 词汇 Wk 的 概率 P ( Wk | Tj ) . 
 1.2 　 参数 的 获取 和 标注 方法 
 　 　 利用 已经 标注 好 的 汉语 语料库 , 可以 采用 被 称为 相对 频率 训练 ( relative   frequency ， 简称 RF ) 的 方法 来 获取 词性 概率 和 词汇 概率 参数 ［ 2 ］ , 即令 
 
 其中 N ( Ti , Tj ) 是 在 训练 语料 中 词性 标记 Tj 紧跟 在 Ti 后 出现 的 次数 , N ( Ti ) 是 标记 Ti 出现 的 次数 , N ( Wk , Tj ) 是 在 训练 语料 中 词汇 Wk 的 词性 标记 为 Tj 的 次数 , N ( Tj ) 是 标记 Tj 出现 的 次数 . 
 为了 解决 由于 训练 语料 数量 有限 所 产生 的 数据 稀疏 问题 , 可以 采用 常数 约束 法 进行 参数 平滑 处理 ［ 3 ］ , 即 对 概率 参数 中 所有 可能 不为 0 , 但 由于 训练 语料 不足 而 为 0 的 参数值 , 令其 等于 一个 很小 的 常 数值 ε . 对于 ANT × NT , 设 ε A = min ( 1 / Nr , c / Nrsw ) ； 对于 BNT × NW , 设 ε B = min ( 1 / NW , c / Nrsw ) ； 其中 Nrsw 是 训练 语料 的 总 词数 , c = 0.1 , 即 把 这些 参数值 估计 成比 那些 在 训练 语料 中 只 出现 一次 的 事件 的 概率 大约 小 10 倍 的 值 . 
 　 　 为 一个 给定 的 词串 S = S1S2 ... St ... SM 寻找 满足 的 C ′ S 的 过程 也 就是 词性 标注 的 过程 . 在 实际 的 标注 系统 中 , 一般 选择 S1 和 SM 为 词性 唯一 的 词 或 标点符号 之间 的 词语 序列 作为 标注 单位 . 由于 对 其他 的 每个 词 ， St 最多 有 Nt 个 可能 的 词性 , 因此 ， 从 S1 到 SM 的 所有 可能 的 标记 路径 就 形成 一个 有向图 . 
 　 　 通过 对 标记 路径 有向图 的 结构 和 二元 语法 模型 的 特点 的 分析 , 目前 ， 一般 采用 基于 动态 规划 的 Viterbi 算法 来 进行 最优 标记 串 的 选择 , 其 基本 思想 是 把 求解 整个 问题 的 最佳解 归结为 求解 其子 问题 的 最佳解 . 假设 已知 S1 的 词性 A2 , 对应 的 标记 序号 为 n1 , SM 的 词性 为 TSM , 对应 的 标记 序号 为 nM , 用 fai ［ t , j ］ ( 1 ≤ t ≤ M , 1 ≤ j ≤ NT ) 来 表示 从 S1 的 TS1 到 St 的 词性 标记 为 Tj 的 最佳 路径 的 概率 权值 , 用 pesai ［ t , j ］ ( 1 ≤ t ≤ M , 1 ≤ j ≤ NT ) 记录 该 最佳 路径 在 St - 1 上 所 选择 的 词性 标记 值 , 用 C ［ 1 , M ］ 保存 最后 所 选择 的 最佳 标记 路径 的 值 . 对于 二元 语法 模型 的 Viterbi 算法 的 描述 如下 ： 
 　 　 ( 1 )   初始化 , j 从 1 到 NT , 
 　 　 　 　 ( a )   如果 j 等于 n1 , fai ［ 1 , j ］ = 1 , 否则 fai ［ 1 , j ］ = 0 ; 
 　 　 　 　 ( b )   pesai ［ 1 , j ］ = 0 ; 
 　 　 ( 2 )   t 从 2 到 ( M - 1 ) ， 转 ( 3 ) ; 
 　 　 ( 3 )   k 从 1 到 NT , 计算 : 
 　 　 　 　 ( a )   
 　 　 　 　 ( b )   
 　 　 ( 4 ) 
 　 　 ( 5 )   t 从 ( M - 2 ) 到 2 , 逆向 查找 最佳 路径 上 的 词性 标记 , C ［ t ］ = pesai ［ t , C ［ t + 1 ］ ］ . 
 2 　 汉语 词性 标注 的 实验 结果 及 分析 
 　 　 在 选取 词性 标记 集时 , 我们 以 文献 ［ 4 ］ 中 对 词语 的 分类 为 基础 , 采用 了 包含 26 个大类 , 82 个子 类 , 25 个 标点 、 符号 , 总共 107 个 标记 的 词 标记 集 . 所 使用 的 训练 语料 是 清华大学 语料库 中 的 一部分 内容 , 其中 包括 经济 、 军事 、 新闻 、 科学 、 计算机 这 5 个 方面 的 题材 , 共约 30 万 词次 . 这些 语料 已经 过 手工 标注 加工 . 根据 不同 的 题材 , 按 比例 选取 , 组成 了 4 万词 的 开放 测试 语料 , 以及 20 万词 、 15 万词 、 10 万词 、 7 万词 、 5 万词 和 3 万词 的 训练 及 封闭 测试 语料 . 并且 以 文献 ［ 4 ］ 中所收 词语 为 基础 ， 根据 所 采用 的 词性 标记 集 , 按 不同 词性 归类 , 建立 了 分类 语词 词典 , 共约 5 万词 . 使用 分类 语词 词典 主要 有 以下 两个 优点 ： ( 1 )   根据 词典 可以 对 训练 语料 中未 出现 的 词语 的 词性 通过 参数 平滑 处理 赋以 相应 的 词汇 概率 值 , 从而 与 单纯 根据 训练 语料 而 构造 的 词汇 概率 矩阵 相比 , 可以 提高 系统 的 标注 性能 ； ( 2 )   文献 ［ 4 ］ 中 选词 规范 , 覆盖面广 , 可以 降低 未知 词 的 出现 频率 . 
 　 　 根据 上面 的 讨论 , 我们 首先 实现 了 一个 目前 所 常用 的 基于 相对 频率 训练 和 Viterbi 算法 的 词性 自动 标注 处理 模式 RF - Basic , 从 不同 规模 的 训练 语料 出发 ， 对 汉语 词性 标注 进行 实验 , 所 得到 的 封闭 测试 和 开放 测试 的 结果 见表 1 . 
 Table   1 　 Tagging   results   using   different   training   sets 
 表 1 　 不同 训练 规模 下 的 标注 结果 
 
 Training   set 
 ( 10   thousand ) ① Tagging   accuracy 
 of   open   test ② Tagging   accuracy 
 of   close   test ③   ( % ) Tagging   accuracy   of   unknown 
 words   on   open   test ④   ( % ) Tagging   accuracy   of   unknown 
 words   on   close   test ⑤   ( % ) 
 393.796 . 542.358 . 0 
 594.596 . 243.652 . 5 
 794.696 . 047.552 . 0 
 1094.996 . 150.453 . 6 
 1595.195 . 952.250 . 7 
 2095.295 . 853.452 . 2 
 
 ① 训练 语料 规模 ( 万 ) , ② 开放 测试 标注 正确率 , ③ 封闭 测试 标注 正确率 , 
 ④ 开放 测试 中 未知 词 的 标注 正确率 , ⑤ 封闭 测试 中 未知 词 的 标注 正确率 . 
 　 　 下面 , 着重 从 开放 测试 的 结果 出发 , 分析 一下 训练 语料 与 标注 正确率 的 关系 . 从图 1 可以 看出 , 训练 语料 的 规模 与 正确率 的 提高 不是 线性关系 , 总的来说 , 训练 语料 的 规模 越大 , 所 获得 的 概率 参数 就 越 接近 真实 的 语言 现象 , 标注 的 正确率 也 会 增加 . 但是 , 当 训练 语料 规模 达到 一定 程度 后 , 标注 正确率 的 增加 幅度 越来越 小 , 系统 的 性能 改善 也 越来越 缓慢 , 几乎 达到 饱和状态 . 
 
 
 Fig.1 　 The   relation   between   training   test   and   tagging   errors 
 图 1 　 训练 语料 规模 与 标注 错误率 的 关系 
 2.1 　 训练 语料 与 词性 概率 矩阵 ANT × NT 的 关系 
 　 　 我们 首先 来 考察 不同 训练 语料 中 的 同现 标记 对 出现 的 情况 , 见表 2 . 
 在 n 元 语法 模型 中 , 训练 语料 的 大小 应使 平均 每个 可能 的 同现 标记 对 在 训练 语料 中 至少 出现 10 次 . 也 有人 认为 , 训练 语料 的 大小 应使 平均 每个 在 训练 语料 中 实际 出现 的 同现 元组 在 训练 中 获得 的 累计 次数 至少 为 10 次 . 事实上 , 我们 认为 训练 语料 的 大小 与 词性 同现 标记 对 之间 的 关系 是 由 标注 集 的 选择 、 训练 语料 内容 的 组成 等 多方面 因素 决定 的 , 片面 地 从 一个 角度 来 考虑 是 不 恰当 的 , 应 根据 具体情况 来 作出 判断 . 对于 我们 的 系统 , 从表 2 中 可以 看出 , 随着 训练 语料 的 增大 , 同现 标记 出现 的 个数 逐渐 变慢 , 平均 每 增加 1 万词 , 新 出现 的 同现 标记 对 的 个数 就 从 127 个 / 万 减少 到 40 个 / 万 . 而且 , 随着 语料 规模 的 增大 , 下降 幅度 在 减小 , 也就是说 , 同现 标记 对 逐渐 达到 稳定 . 
 Table   2 　 The   number   of   same   tagging   pair   in   different   training   corpora 
 表 2 　 不同 训练 语料 中 词性 同现 标记 对 的 个数 
 
 Training 
 corpora   set 
 ( 10   thousand ) ① Numbers   of 
 part - of - speech 
 same   agging   pai ② New   adding   numbers   of   same 
 tagging   pair   per   10 , 000   words 
 between   contiguous   corpora ③ The   average   appearance   number   of 
 new   adding   same   tagging   pair 
 between   contiguous   corpora ④ 
 31   514 　 　 
 51   7681273.11 
 71   9871102.40 
 102   141512.31 
 152   403521.45 
 202   063401.21 
 
 ① 训练 语料 规模 ( 万 ) , ② 出现 的 词性 同现 标记 对 的 个数 , ③ 相邻 训练 语料 之间 平均 每万词 新增 的 同现 标记 对 个数 , ④ 相邻 训练 语料 之间 新增 的 同现 标记 对 平均 每个 所 出现 的 次数 . 
 　 　 再 来看 这些 新 增加 的 同现 标记 对 所 出现 的 次数 与 训练 语料 规模 的 关系 . 从表 2 中 可以 看出 , 从 3 万到 5 万新 增加 的 254 个 标记 对 , 出现 的 总 次数 为 791 , 平均 每 一个 标记 对 出现 了 3.11 次 ; 而 从 15 万到 25 万所新 增加 的 200 个 标记 对 , 总共 出现 了 242 次 , 平均 每个 标记 对 出现 1.21 次 . 也就是说 , 随着 训练 语料 规模 的 逐渐 增大 , 新 增加 的 同现 标记 对 不仅 在 个数 上 逐渐 减少 , 而且 在 训练 语料 中 出现 的 次数 也 逐渐 减少 , 所 占 的 比重 越来越 小 . 因此 , 我们 可以 得出结论 ： 随着 训练 语料 规模 的 增大 , 同现 标记 对 的 组成 逐渐 趋于稳定 , 词性 概率 矩阵 的 结构 逐渐 稳定 . 
 　 　 进一步 分析 ANT × NT 矩阵 在 数值 上 的 变化 情况 , 对 两个 矩阵 A1 , A2 , 我们 用 
 
 表示 从 A1 到 A2 的 矩阵 变化 . 其中 
 
 训练 语料 从 3 万到 20 万所 对应 的 词性 概率 矩阵 的 变化 情况 见表 3 . 
 Table   3 　 The   variety   of   part - of - speech   probability   matrix   in   different   training   corpora 
 表 3 　 不同 训练 语料 下 词性 概率 矩阵 的 变化 
 
 ANT × NTA2 γ A1 
 A3WA5W0.66 
 A5WA7W0.66 
 A7WA10W0.53   
 A10WA15W0.32 
 A15WA20W0.23 
 
 　 　 训练 语料 从 3 万到 5 万 的 变化 为 66% , 而 从 15 万到 20 万 的 变化 为 23% , 变化率 下降 了 65% 之多 . 也就是说 , 词性 概率 矩阵 在 数值 上 也 逐渐 趋于稳定 . 
 　 　 由此 可以 看出 , 在 训练 语料 规模较 小时 , 词性 概率 矩阵 从 结构 和 数值 上 来看 变化 都 比较 大 , 但 当 训练 语料 达到 一定 规模 后 , 词性 概率 矩阵 已 逐渐 趋向 稳定 , 训练 语料 的 增大 对 其 影响 越来越 小 .   
 2.2 　 训练 语料 与 词汇 概率 矩阵 BNT × NT 的 关系 
 　 　 我们 再 来看 训练 语料 的 增加 对 词汇 概率 矩阵 BNT × NT 的 影响 . 
 　 　 随着 训练 语料 的 增大 , 其中 出现 的 词语 总数 将 逐渐 增加 . 新 增加 的 词语 包括 两类 , 一类 是 词典 中 已 包含 的 已知 词 , 另一类 是 未知 词 . 我们 分别 对 它们 进行 统计 , 结果 见表 4 . 对于 未知 词 ( UNKNOWNWORD ) , 我们 视其为 一个 兼类 词 进行 处理 . 对于 词典 中 已 包括 的 词语 , 可以 看出 , 与 同现 标记 对 的 出现 情况 相 类似 , 在 训练 语料 达到 一定 规模 后 , 新 增加 的 词语 的 幅度 越来越 小 , 训练 语料 中 的 词语 的 出现 逐渐 达到 饱和状态 . 
 Table   4 　 The   appearance   number   of   words   in   different   training   corpora 
 表 4 　 不同 训练 语料 中 词语 的 出现 个数 
 
 Training   corpora 
 set   ( 10   thousand ) ① The   numbers   of   
 known   words ② New   adding   numbers   of   known   word   pair   per 
 10 , 000   words   between   contiguous   corpora ③ The   appearance   number   of   
 unknown   words ④ 
 34   245 　 732 
 55   0271   3911   306 
 76   7228481   961 
 107   5722832   778 
 159   1743203   874 
 2010   3342325   298 
 
 ① 训练 语料 规模 ( 万 ) , ② 已知 词 出现 的 个数 , ③ 相邻 训练 语料 之间 平均 每万词 新 增加 的 已知 词 个数 , ④ 未知 词 出现 的 次数 . 
 　 　 用 前面 使用 的 衡量 矩阵 变化 的 γ 值 进一步 分析 词汇 概率 矩阵 的 数值 变化 情况 , 结果 见表 5 . 和 词性 概率 矩阵 一样 , 训练 语料 的 增大 对 词汇 概率 矩阵 的 数值 变化 的 影响 越来越 小 . 我们 知道 ： 词汇 概率 矩阵 实际上 由 两 部分 组成 ： 一部分 是 兼类 词 的 词汇 概率 , 另 一部分 是非 兼类 词 的 词汇 概率 . 从 Viterbi 算法 中 可以 看出 , 非 兼类 词 的 词汇 概率 对于 标注 的 意义 不大 . 我们 进一步 考察 兼类 词 的 词汇 概率 的 变化 情况 , 从表 5 中 可以 看出 , 实际上 , 随着 训练 语料 规模 的 增大 , 词汇 概率 矩阵 中 兼类 词 的 词汇 概率 部分 的 变化 更 小 , 更加 接近 稳定 状态 . 
 Table   5 　 The   variety   of   word   probability   matrix   in   different   training   corpora 
 表 5 　 不同 训练 语料 下 词汇 概率 矩阵 的 变化 
 
 B1B2 γ B1Probability   part   of   pluralistic   part - of - speech   words   in   B ① 
 B3WB5W0.390 . 30 
 B5WB7W0.36   0.26 
 B7WB10W0.300 . 22   
 B10WB15W0.190 . 08 
 B15WB20W0.150 . 07 
 
 ① γ （ B 中 兼类 词 词汇 概率 部分 ) . 
 　 　 从 上 面对 训练 语料 与 词性 概率 矩阵 和 词汇 概率 矩阵 的 分析 可知 , 当 训练 语料 规模 在 某 一 范围 内时 , 训练 后 所 得到 的 相邻 的 统计 概率模型 有 较大 的 变化 , 系统 的 标注 正确率 也 有 较大 的 提高 . 但 当 训练 语料 的 规模 达到 一定 程度 后 , 相邻 模型 的 就 越来越 接近 , 变化 的 幅度 也 越来越 小 , 模型 趋于稳定 . 因此 , 系统 的 标注 正确率 的 提高 就 非常 缓慢 , 错误率 的 下降 越来越 小 . 从而 使 训练 语料 的 规模 大小 与 标注 正确率 的 提高 之间 呈现出 非线性 关系 . 
 3 　 对 训练 标注 方法 的 改进 
 　 　 从 上面 的 分析 可以 看出 , 当 训练 语料 的 规模 达到 一定 程度 后 , 仅仅 通过 扩充 训练 语料 规模 的 大小 来 提高 标注 正确率 是 不 适当 的 . 因此 , 研究 如何 在 训练 语料 的 规模 不变 的 情况 下 尽可能 地 提高 标注 正确率 , 就是 一件 很 有 意义 的 事情 . 我们 在 这方面 作 了 一些 尝试 , 从 以下 两个 方面 对 原来 的 训练 标注 方法 作 了 一些 改进 , 试图 充分利用 已有 的 训练 语料 来 改善 标注 性能 . 
 3.1 　 根据 词语 的 属性 对 有关 的 词性 间 的 组合 予以 优先 处理 
 　 　 我们 知道 , 词类 划分 的 依据 是 词 的 语法 功能 , 但是 词 的 语法 功能 只是 一个 词 的 语法 属性 的 一个 方面 , 仅仅 从 这 一个 方面 来 分析 认识 一个 词是 不够 的 . 根据 词类 划分 词语 虽然 简洁 、 清晰 , 信息 密度 大 , 但是 属于 同一 词类 的 各个 具体 的 词语 的 语法 属性 还是 有 差别 的 , 只有 从 词语 的 语法 属性 的 多个 角度 出发 , 才能 比较 全面 地 认识 一个 词 . 我们 试图 利用 词语 的 有关 语法 属性 来 帮助 对 词语 的 词性 的 判断 . 在 文献 ［ 4 ］ 中 , 不仅 对 词语 按照 词类 进行 了 分类 , 而且 还 按类 对 每个 词语 的 语法 属性 给予 了 详细描述 , 这 就 使 我们 有 了 正确 的 词语 语法 属性 信息 作为 依据 . 
 　 　 我们 来 分析 一种 产生 错误 标注 的 情况 , “ vgo ( 动词 不带 宾语 ) + n ( 名词 ) ” 与 “ vgn ( 动词 带 名词 宾语 ) + n ( 名词 ) ” . 在 “ 新 时期 的 领导 干部 要 认真学习 管理科学 ” 与 “ 作为 一名 班主任 , 她 的 任务 是 教育 管理 学生 ” 这 两句话 中 , “ 管理 ” 一词 前面 的 词 的 词性 都 是 vgn , 而 后面 所 跟 的 词 都 是 名词 . 但是 , 在 第 1 句 中 , “ 管理 ” 修饰 的 是 “ 科学 ” , 其 词性 是 vgo , 而 在 第 2 句 中 , “ 学生 ” 是 “ 管理 ” 的 宾语 , “ 管理 ” 的 词性 应为 vgn . 因此 , 如果 用 原来 的 标注 方法 , 肯定 就 会 有 一个 被 标注 错 . 
 　 　 在 文献 ［ 4 ］ 中 , 对于 名词 词语 有 一项 描述 该 名词 词语 能否 受 动词 直接 修饰 ( 不带 “ 的 ” ) 构成 定中结构 的 “ 前动 ” 属性 . 在 上述 例子 中 , “ 科学 ” 是 属于 可以 受 动词 直接 修饰 的 名词 词语 , 而 “ 学生 ” 则 属于 不能 受 动词 直接 修饰 的 名词 词语 . 我们 还 注意 到 , 在 文献 ［ 4 ］ 的 动词 库中 , 也 有 一条 用于 描述 动词 词语 后面 是否 可以 直接 修饰 名词 以 构成 定中结构 的 “ 后名 ” 属性 . 而 “ 管理 ” 一词 恰恰 具有 这样 的 属性 . 这样 , 有 理由 相信 , 在 第 1 句 中 , “ 管理 ” 作为 vgo 修饰 “ 科学 ” 的 概率 值 应该 大于 其 在 第 2 句 中 作为 vgo 修饰 “ 学生 ” 的 概率 . 这样 处理 也 有助于 避免 一些 兼有 动词 词性 的 名词 被 错 标成 动词 . 由于 动词 与 名词 的 兼类 是 汉语 兼类 现象 中 比重 最大 的 一种 情况 , 解决 好 这个 问题 对于 提高 标注 正确率 会 很 有 好处 . 
 　 　 因此 , 根据 文献 ［ 4 ］ 中 所 提供 的 这样 的 属性 , 我们 把 原来 名词 中 的 两个 子类 ： 无量 名词 ( 不 受 任何 量词 修饰 , nf ) 和 一般 名词 ( 受名 量词 修饰 , nr ) 按 “ 前动 ” 属性 再 分成 无量 名词 - 前动 可 ( nfqdk ) 、 无量 名词 - 前动否 ( nfqdf ) 、 一般 名词 - 前动 可 ( nrqdk ) 、 一般 名词 - 前动否 ( nrqdf ) 这 4 个小类 , 把 动词 不带 宾语 ( vgo ) 再 按 “ 后名 ” 属性 分成 动词 不带 宾语 - 后名 可 ( vgohmk ) 和 动词 不带 宾语 - 后 名否 ( vgohmf ) 两个 小类 . 在 Viterbi 算法 中 , 当 计算 fai ［ t ， j ］ = 时 , 我们 增加 如下 的 处理 ： 
 　 　 对于 aij , 若 Ti 是 vgo , 且 Tj 是 nf 或者 nr , 那么 判断 是否 有 St - i 属于 vgohmk , 且 St 属于 nfqdk 或 nrqdk ； 若 是 , 则 用 aij 。 τ 替换 aij 进行 运算 , τ 是 一个 优先 因子 , 用于 放大 vgohmk 与 nfqdk 或者 vgohmk 与 nrqdk 之间 的 优先 组合 的 概率 关系 , τ 的 值 可以 通过 对 实验 结果 的 比较 来 选取 . 
 　 　 需要 注意 的 是 , 采用 这种 方法 后 , 对于 某些 具有 歧义 的 “ 动词 + 名词 ” 的 结构 , 可能 会 降低 它们 的 标注 正确率 . 例如 ， 对于 “ 学习 文件 ” , 它 既 可能 是 “ 学习 ” 作为 不带 宾语 的 动词 修饰 “ 文件 ” 构成 定中结构 , 也 可能 是 “ 文件 ” 作为 “ 学习 ” 的 宾语 而 构成 “ 述宾 结构 ” . 由于 “ 学习 ” 具有 “ 后名 ” 属性 , “ 文件 ” 也 具有 “ 前动 ” 属性 , 所以 , “ 学习 ” 将 更 有 可能 被 标注 成 vgo . 但是 , 从 最后 的 实验 结果 来看 , 利用 词语 的 这种 语法 属性 辅助 进行 词性 标注 , 还是 可以 比较 有效 地 提高 标注 正确率 的 . 在 以后 的 工作 中 ， 我们 也 将 会 继续 这方面 的 分析 , 使 这种 方法 更加 完善 . 
 3.2 　 改进 与 未知 词 相关 的 词性 概率 的 估计 
 　 　 在 本 系统 中 , 根据 对 语料库 的 统计 , 一般 会 有 2 ～ 3% 的 词语 属于 未知 词 . 从表 1 中 可以 看出 , 在 开放 测试 下 对 未知 词 的 标注 正确率 大约 仅为 50% . 对于 专业性 较强 的 真实 文本 , 未知 词 的 出现 率 可能 会 更 高 , 成为 影响 系统 标注 正确率 的 一个 重要 因素 , 因此 , 必须 采取 更加 有效 的 手段 对 未知 词 进行 标注 . 我们 的 做法 是 加强 与 未知 词 相关 的 词性 概率 的 估计 . 
 　 　 对于 词串 S = S1S2 ... St ... SM , 假设 St 是 未知 词 , 在 计算 从 St - 1 到 ( St , Tj ) 的 最佳 路径 的 概率 权值 时 , 原来 的 做法 是 ： 
 
 现在 , 当 St 是 未知 词时 , 用 P ( T ( UW ) = Tj | Tt - ( UW ) ) ( UW 表示 未知 词 UNKOWNWORD ) 替换 原式 中 的 aij = P ( Tj | Ti ) 再 进行 计算 , P ( T ( UW ) = Tj | Tt - ( UW ) ) 是 标记 Ti 后面 出现 的 未知 词 的 词性 标记 为 Tj 的 概率 值 . 由于 将 未知 词 看成 是 一个 特殊 的 兼类 词 , 因而 用 针对 它 的 词性 转移 概率 代替 一般 的 词性 转移 概率 值 所 对应 的 语言 现象 更 少 , 可以 更加 准确 地 反映 出 不同 词性 对 未知 词所 产生 的 影响 , 因此 对于 预测 未知 词 的 词性 也 会 更加 有效 . 
 　 　 在 计算 P ( T ( UW ) = Tj | Tt - ( UW ) ) 时 , 考虑 到 数据 稀疏 所 带来 的 影响 , 采用 插值 估计 法 ［ 3 ］ 进行 参数 平滑 处理 , 对 P ( T ( UW ) = Tj | Tt - ( UW ) ) 的 计算 如下 ： 
 P ( T ( UW ) = Tj | Tt - ( UW ) ) = λ 1P ( T ( UW ) = Tj | Tt - ( UW ) ) + λ 2P ( T ( UW ) = Tj ) . 
 其中 
 
 N ( Tt - ( UW ) , T ( UW ) = Tj ) 是 在 训练 语料 中 标记 Tt 后面 出现 的 未知 词 的 词性 标记 Tj 的 次数 , N ( Tt - ( UW ) ) 标记 Tt 后面 紧跟着 出现 未知 词 的 次数 , N ( T ( UW ) = Tj ) 是 未知 词 的 标记 为 Tj 的 次数 , N ( UW ) 是 训练 语料 中 所有 未知 词 的 个数 . 
 　 　 以上 是从 两个 方面 对 原有 的 训练 标注 方法 所 进行 的 改进 . 我们 的 目的 就是 要 充分利用 已有 的 训练 语料 , 改进 统计 训练 和 标注 方法 , 从 更 细 的 角度 来 使 统计 参数 ， 更加 准确 地 反映 出 语言 现象 中 所 存在 的 词语 的 语法 功能 的 概率分布 规律 . 改进 后 的 方法 没有 对 原来 的 训练 和 标注 算法 的 时间 或 空间 复杂度 产生 较大 的 影响 , 同样 地 ， 利用 原来 的 训练 和 测试 语料 进行 实验 后 所 得到 的 结果 见表 6 . 
 Table   6 　 Part - of - Speech   tagging   result   after   improving 
 表 6 　 改进 后 的 词性 标注 实验 结果 
 
 Training   corpora 
 set   ( 10   thousand ) ① Tagging   accuracy 
 of   open   test ②   ( % ) Tagging   accuracy 
 of   close   test ③   ( % ) Tagging   accuracy   of   unknown 
 words   on   open   test ④   ( % )   Tagging   accuracy   of   unknown 
 words   on   close   test ⑤   ( % ) 
 395.097 . 258.268 . 9 
 595.697 . 055.661 . 3 
 795.896 . 863.162 . 1 
 1096.096 . 866.765 . 2 
 1596.096 . 664.461 . 1 
 2096.196 . 565.565 . 3 
 
 ① 训练 语料 规模 ( 万 ) , ② 开放 测试 标注 正确率 , ③ 封闭 测试 标注 正确率 , 
 ④ 开放 测试 中 未知 词 的 标注 正确率 , ⑤ 封闭 测试 中 未知 词 的 标注 正确率 . 
 　 　 从表 6 中 可以 看出 , 与 原来 的 方法 相比 , 改进 后 在 开放 测试 下 , 标注 错误率 下降 了 20% , 未知 词 的 错误率 下降 了 18% ; 在 封闭 测试 下 , 标注 错误率 下降 了 25% , 未知 词 的 错误率 下降 了 22% . 基于 3 万词 的 训练 语料 所 获得 的 开放 测试 的 正确率 几乎 达到 了 原来 基于 10 万词 的 效果 , 有效 地 提高 了 标注 的 正确率 . 
 4 　 结束语 
 　 　 本文 从 目前 常用 的 基于 统计 的 汉语 词性 标注 方法 出发 , 对 不同 训练 语料 规模 下 的 实验 结果 , 从 词性 概率 矩阵 与 词汇 概率 矩阵 的 结构 和 数值 变化 等 方面 对 训练 语料 规模 与 标注 正确率 之间 所 存在 的 非线性 关系 作 了 分析 ， 并 对 其 加以改进 , 得到 一个 增强 的 处理 模式 RF - Enhanced , 有效 地 提高 了 自动 标注 的 正确率 . 
 　 　 结合 工作 中 的 体会 , 我们 认为 , 对于 汉语 词性 标注 , 可以 综合 运用 词语 的 其他 已知 属性 辅助 词性 标注 . 词语 所 包含 的 信息 可以 说 是 一个多 维空间 , 不同 属性 之间 会 产生 相互 的 影响 . 在 进行 词性 标注 时 , 如果 只 考虑 词性 与 词性 、 词性 与 词语 之间 的 关系 是 不 全面 的 , 词语 的 其他 属性 ( 不仅仅 是 语法 属性 ) 也 可以 用来 辅助 判断 词性 . 
 　 　 另外 , 可以 通过 对 标注 错误 现象 的 统计分析 , 对 一些 易错 的 特殊 词性 或者 词 , 根据 它们 的 语法 功能 的 特点 , 相应 地 加强 对 与 它们 有关 的 概率 信息 的 统计 , 作 特殊 处理 . 甚至 可以 对 一些 现象 进行 总结 , 制订 成 规则 , 用于 标注 处理 . 事实上 , 我们 认为 , 基于 规则 的 标注 方法 与 基于 统计 的 方法 两者之间 并不矛盾 , 规则 其实 就是 人们 对 一些 发生 的 概率 值 比较 大 的 语言 现象 的 总结 . 对于 基于 统计 的 方法 , 由于 受 统计 模型 本身 所 固有 的 缺点 和 语料 中 其他 语言 现象 的 影响 , 训练 后 所 得到 的 概率 参数 并不一定 能够 有效 地 反映 出 语言 本身 的 一些 确定性 的 语法 特点 , 而 利用 规则 却 可以 弥补 这方面 的 缺陷 . 
 　 　 在 以后 的 工作 中 , 我们 将 继续 对 这些 问题 进行 研究 , 希望 能 充分利用 语言学 知识 , 结合 统计分析 方法 , 使 标注 性能 获得 更进一步 的 提高 . 
 魏欧 （ 中国科学院 软件 研究所 　 北京 　 100080 ） 　 
 吴健 （ 中国科学院 软件 研究所 　 北京 　 100080 ） 　 
 孙玉芳 （ 中国科学院 软件 研究所 　 北京 　 100080 ） 
 参考文献 
 1 ， Zhuo   Qiang .   Chinese   corpus   tagging   using   rule   techniques   and   statistics   techniques .   Journal   of   Chinese   Information   Processing ,   1995 , 9 ( 2 ) : 1 ～ 10 
 ( 周强 . 规则 与 统计 相结合 的 汉语 词类 标注 方法 . 中文信息 学报 , 1995 , 9 ( 2 ) : 1 ～ 10 ) 
 2 ， Bernard   Merialdo .   Tagging   English   text   with   a   probabilistic   model .   Computational   Linguistics ,   1994 , 20 ( 2 ) : 155 ～ 171 
 3 ， Zhuo   Qiang .   Corpus - Based   and   statistics - oriented   natural   language   processing   techniques .   Computer   Science ,   1995 , 22 ( 4 ) : 36 ～ 40 
 ( 周强 . 基于 语料库 和 面向 统计学 的 自然语言 处理 技术 介绍 . 计算机科学 , 1995 , 22 ( 4 ) : 36 ～ 40 ) 
 4 ， Yu   Shi － wen ,   Zhu   Xue － feng ,   Wang   Hui   et   al .   The   Detail   of   Modern   Chinese   Syntax   Information   Dictionary .   Beijing :   Tsinghua   University   Press ,   1998 
 ( 俞士汶 , 朱学峰 , 王惠 等 . 现代汉语 语法 信息 词典 详解 . 北京 ： 清华大学出版社 , 1998 ) 
 
  
