自动化 学报 
 ACTA   AUTOMATICA   SINICA 
 1999 年 　 第 25 卷 　 第 5 期 　 Vol.25   No.5   1999 
 
 
 
 广义 LVQ 神经网络 的 性能 分析 及其 改进 1 ) 
 张志华 　 郑 南宁 　 王天树 
 摘 　 要 　 首先 从 理论 上 分析 了 广义 学习 矢量 量化 ( GLVQ ) 网络 的 GLVQF 算法 的 性能 ， GLVQF 算法 在 一定 程度 上 克服 了 GLVQ 算法 存在 的 问题 . 然而 ， 它 对 获胜 表现型 的 学习 具有 好 的 性能 ， 对于 其它 的 表现型 ， 性能 却 十分 不 稳定 . 分析 了 产生 这个 问题 的 原因 ， 直接 从 表现型 的 学习 率 出发 ， 提出 了 选取 学习 率 的 准则 ， 并 给出 了 两种 改进 的 算法 . 最后 ， 使用 IRIS 数据 验证 了 算法 的 性能 ， 改进 算法 较之 GLVQF 算法 具有 明显 的 稳定性 和 有效性 . 
 
 关键词 　 亏损 因子 ， 模糊 度 因子 ， 学习 率 ， IRIS 数据 . 
 BEHAVIORAL   ANALYSIS   AND   IMPROVING   OF 
 GENERALIZED   LVQ   NEURAL   NETWORK 
 ZHANG   Zhihua 　 ZHENG   Nanning 　 WANG   Tianshu 
 ( Institute   of   AI   and   Robotics ,   Xi ' an   Jiaotong   University ,   Xi ' an 　 710049 ) 
 Abstract 　 　 In   this   paper ,   the   performance   of   GLVQ - F   algorithm   of   GLVQ   network   is   theoretically   analyzed .   The   GLVQF   algorithm ,   to   some   extent ,   has   overcome   the   shortcomings   that   GLVQ   algorithm   possesses .   But ,   there   are   some   problems   in   GLVQF   algorithm ,   for   example ,   the   algorithm   has   good   performance   on   the   winning   prototype ,   and   on   other   prototypes ,   its   performance   is   very   unstable .   In   this   paper ,   the   reasons   of   the   problem   are   discussed .   The   rules   of   choosing   the   learning   rates   are   proposed ,   and   two   modified   algorithms   are   developed   therefrom .   Finally ,   the   performance   of   the   modified   algorithms   is   verified   with   IRIS   data ,   which   shows   the   modified   algorithms   are   more   stable   and   effective   than   GLVQF   algorithm . 
 Key   words 　 Loss   factor ,   fuzzy   degree   factor ,   learning   rate ,   IRIS   data . 
 1 　 引 　 言 
 　 　 近几年 ， 由于 理论 上 和 网络结构 上 表现 出 广泛 的 活力 ， Kohonen 关于 聚类 算法 ［ 1 , 2 ］ 的 研究 工作 变得 十分 流行 . Kohonen 的 工作 ［ 2 ］ 主要 集中 在 两 方面 ： 一是 学习 矢量 量化 ( LVQ ) 算法 ， 二是 他 的 自 组织 特征 映射 ( SOFM ) 网络 . 但是 LVQ 存在 神经元 未 被 充分利用 以及 输入 样本 和 竞争 单元 之间 的 信息 被 浪费 等 两个 主要 问题 ［ 3 ］ . SOFM 同样 也 存在 两大 缺点 ［ 4 ］ ： 一是 哪些 节点 应 被 考虑 ； 二是 每个 非 获胜 节点 应 怎样 发挥 影响 . Huntsberger 和 Hjjimarangsee ［ 5 ］ 首次 把 SOFM 同 LVQ 相结合 ， 为 试图 克服 两者 上述 各自 的 问题 提出 了 一种 新 的 思路 . Pal 等 ［ 6 ］ 在 这个 思想 上 提出 了 一种 广义 的 学习 矢量 量化 ( GLVQ ) 网络 . Gonzalez 等 ［ 8 ］ 对 GLVQ 进行 了 性能 分析 ， 发现 它 仍然 存在 一些 问题 . Karayiannis 等 ［ 9 ］ 修正 了 GLVQ ， 提出 了 GLVQ - F 算法 . GLVQ - F 算法 在 一定 程度 上 解决 了 GLVQ 算法 存在 的 问题 . 但 同时 GLVQ - F 算法 对于 不同 的 模糊 度 因子 聚类 性能 不 稳定 . 本文 从 数学 上 分析 了 GLVQ - F 算法 的 性能 ， 并 提出 了 两种 改进 的 算法 . 
 2 　 广义 学习 矢量 量化 ( GLVQ ) 网络 和 GLVQ - F 算法 
 　 　 记 X = ｛ x1 , x2 , … , xp ｝ Rn 为 输入 样本 集合 ， 设 X 中 的 类数 ( 表现型 的 个数 ) 为 c .   GLVQ 网络 的 学习 规则 是从 最优化 一个 目标 函数 导出 的 . 该 目标 函数 定义 为 输入 样本 x 的 一个 亏损 函数   Lx 
 　 ( 1 ) 
 其中 表现型 ν = ｛ v1 , v2 , … , vc ｝ Rn 是 我们 要 寻找 的 矢量 量化 器 ， gr 是 x 相对 表现型 的 亏损 因子 ， 其 不同 的 定义 就 会 导出 不同 的 学习 算法 . 在 GLVQ 算法 中 ， 由下式 定义 ： 
 . 　 ( 2 ) 
 　 　 由于 GLVQ 算法 定义 gr 为式 ( 2 ) ， 在 实际 应用 中 ， 使得 算法 背离 了 初衷 ［ 7 ,   8 ］ . 为此 ， 文献 ［ 8 ］ 引入 模糊 C - 均值 中 的 隶属 度 公式 ［ 1 ］ 来 定义 gr ， 即 
 　 ( 3 ) 
 其中 m ∈ （ 0 ， 1 ) 是 一个 常数 ， 表示 模糊 度 因子 . 此时 ， 用 梯度 下降 法 求解 目标 函数 Lx 的 最小值 ， 导出 了 GLVQF 算法 ， 对于 输入 x ， 第 t 次 的 学习 规则 是 
 vj ( t ) = vj ( t - 1 ) + 2 α ( t ) . h ( j , m ) . ( x - v ( t - 1 ) ) , j = 1 , … , c , 　 ( 4 ) 
 这里 把 h ( j , m ) 称为 学习 率 
 h ( j , m ) = gj + F ( j , m ) , j = 1 , 2 , … , c , 　 ( 5 ) 
 　 ( 6 ) 
 　 ( 7 ) 
 3 　 GLVQ - F 算法 的 性能 分析 
 　 　 为了 方便 起 见 ， 令 vi 是 相对 于 输入 向量 x 的 获胜 表现型 ， 即 ， vl 是 相对 于 输入 向量 x 的 竞争 最小 的 表现型 ， 即 . 本文 其余部分 都 作 这样 假设 . 现在 分析 GLVQF 的 性能 . 首先 
 　 　 因为 δ ir ≤ 1 , δ - 1ir - 1 ≥ 0 , r = 1 , 2 , … ， c , 所以 F ( i , m ) ≥ 0 , 于是 有 gi + F ( i , m ) ≥ gi . 
 又 δ lk ≥ 1 , δ - 1lk - 1 ≤ 0 , F ( l , m ) ≤ 0 , 所以 gl + F ( l , m ) ≤ gl . 
 　 　 可以 看出 F ( k , m ) 是 个 扰动 参数 （ k = 1 , 2 , … ， c ) ， 对于 获胜 节点 ， 它 加大 了 学习 步长 ， 而 对 竞争 最小 节点 ， 它 减弱 了 学习 步长 . 进一步 比较 h ( i , m ) , h ( k , m ) ( k = 1 , 2 , … ， c ； k ≠ i ) 的 大小 . 首先 由式 ( 3 ) , ( 4 ) 经过 简单 计算 ， 有 
 　 ( 8 ) 
 　 　 定理 1.1 ） h ( k , m ) ≤ h ( i , m ) , ( k = 1 , 2 , … , c ; k ≠ i ) ； 2 ) F ( k , m ) ≤ F ( i , m ) , ( k = 1 , 2 , … , c , k ≠ i ) . 
 　 　 从 直观 上 看 ， 作者 希望 当 ‖ x - vk ‖ 2 ≥ ‖ x - vj ‖ 2 时 ， 算法 也 能 满足 h ( k , m ) ≤ h ( j , m ) . 然而 ， 遗憾 的 是 该 结论 即使 对于 竞争 最小 的 表现型 也 不能 成立 . 举例说明 
 　 　 例 . 设 , c = 3 .   令 ‖ x - v1 ‖ 2 = , ‖ x - v2 ‖ 2 = 3 , ‖ x - v3 ‖ 2 = 4 . 
 经过 简单 计算 有 显然 ‖ x - v2 ‖ 2 ＜ ‖ x - v3 ‖ 2 ， 而 . 由此 例 还 可见 h ( k , m ) 不 一定 大于 零 ( k ≠ i ) .   但 当 m ≥ 2 时 ， 有 下述 结论 成立 ： 
 　 　 定理 2 . 设 m ≥ 2 ， 如果 ‖ x - vk ‖ 2 ≥ ‖ x - vj ‖ 2 ， 则 0 ＜ h ( k , m ) ≤ h ( j , m ) . 
 　 　 证明 . 　 因为 m ≥ 2 , ∴ . 因而 当 ‖ x - vk ‖ 2 ≥ ‖ x - vj ‖ 2 ， 有 
 
 又 
 
 所以 h ( k , m ) - h ( j , m ) ≤ 0 . 于是 定理 得证 . 
 　 　 从 另 一个 角度 来 分析 GLVQ - F 算法 ， 即 固定 δ kr ， 把 h ( k , m ) = gk + F ( k , m ) 当作 m 的 函数 进行 分析 . 首先 给出 一个 引理 . 
 　 　 引理 1 ［ 7 ］ . 1 ) 2 ) 
 3 ) 
 　 　 这里 给出 h ( k , m ) , gk ( k = 1 , 2 , … ， c ) 关于 m 的 导数 ： 
 　 ( 9 ) 
 　 ( 10 ) 
 　 　 根据上述 两 式 、 引理 1 和 文献 ［ 1 ］ ， 很 容易 得到 下面 几个 定理 ： 
 　 　 定理 3 . gi 为 m 在 区间 ( 1 , ∞ ) 的 递减 函数 ， 并且 
 　 　 定理 4 . gl 为 m 在 区间 ( 1 , ∞ ) 的 递增 函数 ， 并且 
 　 　 定理 5 . h ( i , m ) 为 m 在 区间 ( 1 , ∞ ) 的 递减 函数 ， 并且 h ( i , m ) = 1 , h ( i , m ) = 
 　 　 定理 5 表明 h ( i , m ) 有 类似 定理 3 的 结果 ， 但 遗憾 的 是 本文 不能 证明 h ( l , m ) 有 类似 定理 4 的 结论 成立 . 作者 的 目标 是 希望 找到 满足 下述 两个 条件 的 h ( k , m ) ： 
 　 　 1 ) 当 , h ( i , m ) 为 m 的 递减 函数 ， 且 h ( i , m ) = , h ( i , m ) = 1 ； 
 　 　 2 ) 当 k ≠ i , h ( k , m ) 为 m 的 递增 函数 ， 且 h ( k , m ) = h ( k , m ) = 0 . 
 这样 就 可以 采用 类似 于 文献 ［ 9 ］ 中 的 方法 ， 随着 学习 次数 的 增加 ， m 由大 逐渐 减小 ， 即 开始 时 ， 所有 表现型 的 学习 步长 都 均等 ， 而 最后 ， 只有 获胜 表现型 激活 ， 其它 表现型 逐渐 抑制 . 从 直观 上 看 ， 这是 合理 的 . 
 4 　 GLVQ - F 的 改进 算法 
 　 　 作者 的 目的 是 构造 满足 上述 条件 的 h ( k , m ) ( k = 1 , 2 , … ， c ) . 文献 ［ 8 ］ h ( k , m ) 的 构造 是从 gk 着手 的 . 其实 在 学习 时 ， 只 需要 h ( k , m ) , 因而 可 直接 从 h ( k , m ) 着手 ， 而 不 考虑 gk . 
 　 　 h ( k , m ) 是 gk 加上 了 一个 扰动 项 F ( k , m ) ， 对于 获胜 表现型 它 增大 了 其 激活 程度 ； 而 对于 竞争 最小 的 表现型 它 增大 了 其 抑制 程度 ； 但 对 其它 的 表现型 ， F ( k , m ) 这个 扰动 量 有时 会 产生 很 差人意 的 效果 （ 正如 上节 的 例子 所 分析 的 ） . 略去 F ( k , m ) ， 直接 令 
 h ( k , m ) = gk , 　 ( 11 ) 
 对于 获胜 表现型 和 竞争 最小 的 表现型 来说 ， 降低 了 它们 的 激活 或 抑制 程度 ， 但 对于 其它 表现型 ， 稳定 了 其 效果 ， 故 整个 算法 相对而言 要 稳定 些 . 由 定理 3 和 定理 4 ， 可知 h ( k , m ) 满足 上面 的 条件 1 ) ， 但 条件 2 ) 只是 对 竞争 最小 的 表现型 成立 . 我们 把 基于 式 ( 11 ) 导出 的 算法 简称 改进 算法 1 . 
 　 　 下面 讨论 另 一种 改进 （ 简称 改进 算法 2 ） . 令 
 　 ( 12 ) 
 由 上式 首先 可 得 　 　 h ( k , m ) ＜ gk ( k = 1 , 2 , … , c ) . 
 因为 
 所以 　  
 所以  
 由式 ( 13 ) 可得 0 ＜ h ( k , m ) ＜ gk ＜ gi = h ( i , m ) = 1 ( k = 1 , 2 , … ， c ； k ≠ i ) . 再 根据 式 ( 12 ) ， 显然 有 h ( k , m ) = h ( k , m ) = 0 , ( k = 1 , 2 , … ， c ； k ≠ i ) . 
 　 　 又 由于 
 
 ( k = 1 , 2 , … , c ; k ≠ i ) , 　 　 　 　 
 所以 　 h ( k , m ) ( k = 1 , 2 , … , c ; k ≠ i ) 是 m 在 区间 ( 0 , ∞ ) 上 的 递增 函数 . 于是 两个 条件 满足 . 
 5 　 数值 实验 及 结果 分析 
 　 　 这里 用 著名 的 IRIS 数据 ［ 9 ］ 来 验证 上述 改进 算法 的 性能 . IRIS 数据 经常 被 用来 检验 聚类 ( 无 监督 ) 或 分类 ( 有 监督 ) 模式识别 方法 的 性能 ， 它 总共 有 150 个 数据 ， 每个 数据 又 由 4 个 分量 组成 . IRIS 数据 分为 3 类 ， 每类 有 50 个 数据 . 对 有 监督 设计标准 的 错误 个数 为 0 — 5 ， 而 无 监督 设计标准 的 个数 在 15 左右 . 这里 作者 用 GLVQ - F 算法 ， 改进 算法 1 ， 2 分别 对 IRIS 数据 进行 聚类 . 为了 更好 地 比较 这 三个 算法 的 性能 ， 分别 取 不同 的 m 值 进行 实验 ， 其 结果 见表 2 . 表 1 给出 了 表现型 的 初始值 ， 实验 都 是 根据 这个 初始值 进行 的 . 为了 有 一个 比较 准则 ， 用 1 - NP ( 最近 邻 ) 算法 直接 对 样本 进行 聚类 ， 结果 也 见表 1 . 
 表 1 　 数值 实验 中 表现型 的 初试 值及 最近 邻 算法 实验 结果 
 
 初始 质心 最后 质心 混淆 矩阵 错误率 
 5.006 , 3.428 ,   1.462 ,   0.2465 . 006 , 3.428 , 1.462 , 0.2465000 
 5.936 , 2.770 ,   4.260 ,   1.3265 . 936 , 2.770 ,   4.260 ,   1.32604647 . 3% 
 6.588 , 2.974 ,   5.552 ,   2.0266 . 588 , 2.974 ,   5.552 ,   2.0260743 
 
 　 　 在 表 2 中 每 一行 上部 代表 聚类 的 混淆 ( Confusion )   矩阵 ， 下部 表示 聚类 的 错误率 . 从表 2 可以 看到 ， GLVQ - F 算法 在 m ＞ 2 时 ， 聚类 的 误差 很大 ， 在 m 值 趋近 1 时 算法 溢出 ， 以至于 不能 进行 聚类 . 改进 算法 1 比 GLVQF 算法 聚类 效果 有 一定 的 提高 ， 但 相应 的 问题 同样 存在 . 其 原因 正如 第三节 所 分析 的 ， 是 由于 式 ( 5 ) , ( 11 ) 定义 的 学习 率 h ( j , m ) ( j = 1 , 2 , … , c ) 随着 m 取 不同 的 值 ， 其 性能 不 稳定 ， 从而 严重破坏 了 算法 的 聚类 效果 . 改进 算法 2 完全 满足 两个 条件 ， 该 算法 在 m = 5 以及 m = 3 时聚类 的 错误 个数 为 11 ， 低于 15 ， 而 此时 GLVQF 算法 和 改进 算法 1 的 错误 数都 很 高 . 当 m 的 值 趋近 1 时 ， 比如 m = 1.01 ， 其它 两种 算法 已经 无法 进行 聚类 ， 而 改进 算法 2 仍然 能 有效 地 对 数据 样本 进行 聚类 ， 这 表明 该 算法 的 聚类 性能 是 十分 稳定 ， 所以 它 是 十分 有效 的 . 
 表 2 　 三种 学习 算法 实验 结果 比较 
 
 m5.03 . 02.01 . 51.21 . 11.01 
 GLVQF 算法 005050005000500050005000 溢 　 出 
 0050491024620464043704010 无 　 法 
 83210740301139024801490149 聚 　 类 
 93.3% 64.0% 10.0% 4.0% 5.3% 7.3% 
 改进 
 算法 1500050005000500050005000 溢 　 出 
 2921054501463046404550446 无 　 法 
 11138010400545024801490149 聚 　 类 
 27.3% 10.0% 6.0% 4.0% 4.0% 4.7% 
 改进 
 算法 25000500050005000500050005000 
 3443344304640446043704370437 
 0545054502480149014901490149 
 7.3% 7.3% 4.0% 4.7% 5.3% 5.3% 5.3% 
 
 　 　 该 实验 的 结果 与 前文 的 理论 分析 是 吻合 的 . 它 进一步 验证 了 基于 两个 条件 选取 学习 率 h ( k , m ) ( k = 1 , 2 , … ， c ) 而 导出 的 学习 算法 是 有效 的 . 
 6 　 结 　 论 
 　 　 GLVQ 算法 是从 最优化 一个 目标 函数 而 导出 的 ， 该 算法 构造 新颖 ， 为 克服 学习 矢量 量化 算法 存在 的 问题 提供 了 一个 新 的 思路 . Karayiannis 等 通过 引入 模糊 c 均值 的 隶属 度 函数 ， 修正 了 GLVQ 算法 ， 提出 一类 新 的 学习 算法 GLVQF . 本文 的 工作 为 GLVQF 算法 提供 一定 的 数学 理论 基础 . 完善 学习 矢量 量化 算法 的 理论 ， 以及 基于 这些 理论 设计 出 更 高效 的 聚类 算法 并 把 算法 应用 于 图象处理 中将 是 作者 进一步 研究 的 课题 . 
 1 )   国家自然科学基金 重点 资助 项目 ( No.69735010 ) . 
 作者简介 ： 张志华 　 讲师 ， 现在 西安交通大学 攻读 博士学位 . 主要 从事 模式识别 ， 图象处理 ， 模糊 逻辑系统 及 神经计算 智能 等 领域 的 研究 . 
 　 　 　 　 　 郑 南宁 　 博士 ， 博士生 导师 . 主要 从事 模式识别 ， 图象处理 及 计算机 视觉 等 领域 的 理论 与 应用 研究 ， 现已 在 国内外 重要 学术期刊 及 会议 上 发表 论文 一 百余篇 . 
 作者 单位 ： 西安交通大学 人工智能 与 机器人 研究所 　 西安 　 710049 
 参考文献 
 1 　 Bezdek   J .   Pattern   Recognition   with   Fuzzy   Objective   Function   Algorithms .   New   York :   Pelnum ,   1981 
 2 　 Kohonen   T .   Selforganization   maps .   Berlin :   SpringerVerlag ,   1995 
 3 　 Chung   F   L ,   Lee   T .   Fuzzy   competitive   learning .   Neural   Networks ,   1994 ,   7 ( 3 ) :   539 - 551 
 4 　 Tsao   E   C   K ,   Bezdek   J   C   ,   Pal   N   R .   Fuzzy   Kohonen   clustering   networks .   Pattern   Recognition ,   1994 ,   27 ( 5 ) :   757 - 764 
 5 　 Huntsberger   T ,   Ajjimarangsee   P .   Parallel   selforganizing   feature   maps   for   unsupervised   pattern   recognition .   Int .   J .   General   Systems ,   1989 ,   16 ( 2 ) :   357 - 372 
 6 　 Pal   N   R ,   Bezdek   J   C ,   Tsao   E   C   K .   Generalized   clustering   networks   and   Kohonen ' s   selforganizing   scheme .   IEEE   Trans .   on   Neural   Networks ,   1993 ,   4 ( 4 ) :   549 - 558 
 7 　 Gonzalez   A   Z ,   Grana   M ,   Anjar   A   D .   An   analysis   of   the   GLVQ   algorithm .   IEEE   Trans .   on   Neural   Networks ,   1995 ,   6 ( 4 ) :   1012 - 1016 
 8 　 Karayiannis   N   B ,   Bezdek   J   C ,   Pal   N   R ,   Hatharay   R   J .   Repair   to   GLVQ :   a   new   family   of   competitive   learning   schemes .   IEEE   Trans .   on   Neural   Networks ,   1996 ,   7 ( 5 ) :   1062 - 1071 
 9 　 Bezdek   J   C ,   Pal   N   R .   Two   soft   relatives   of   learning   vector   quantization .   Neural   Networks ,   1995 ,   8 ( 5 ) :   729 - 743 
 收稿 日期 ： 1998 - 07 - 03 
 修稿 日期 ： 1998 - 12 - 10 
 
  
