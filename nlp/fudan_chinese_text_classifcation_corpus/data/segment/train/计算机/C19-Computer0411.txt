信息 与 控制 
 Information   and   Control 
 1999 年 　 第 28 卷 　 第 5 期 　 Vol.28 　 No.5 　 1999 
 
 
 
 基于 多元 对应 分析 的 KNN 分类器 组合 
 韩 　 宏 　 杨静宇 　 胡 钟山 
 
 　 　 摘 　 要 ：   本文 提出 一种 基于 多元 相应 分析 的 KNN 分类器 组合 方法 ( MCA - KNN ) ， 并 以 手写体 识别   为例 ， 用 KNN 分类器 在 同一 样本 集合 得到 的 不同 特征 集上 进行 分类 ， 再 通过 多元 对应 分析 对   这些 分类器 的 结果 进行 组合 ， 以 得到 最终 的 分类 结果 ． 实验 结果表明 ， 此种 分类器 组合 方法   能 显著 减少 分类 错误率 ． 
 　 　 关键词 ： K 近邻 分类器 （ KNN ） ， 多元 对应 分析 ,   字符识别 
 　 　 中 图 分类号 ： TP13 　 　 　 　 　 　 文献 标识码 ： B 
 
 COMBINATION   OF   KNN   CLASSIFIERS   BASED   ON 
 MULTIPLE   CORRESPONDENCE   ANALYSIS 
 HAN   Hong   YANG   Jing - yu   HU   Zhong - shan 
 ( Department   of   Computer ,   Nanjing   University   of   Science   &   Technolo   gy ,   Nanjing   210094 ) 
 Abstract 　 This   paper   presents   a   KNN   classifier   combination   me   thod   based   on   multiple   correspondence   analysis   ( MCA - KNN ) .   This   combination   meth   od   is   applied   to   written   character   recognition .   Four   kinds   of   features   are   extra   cted   from   same   sample   set   and   four   result   sets   are   obtained   from   these   feature   s   ets   through   KNN   classifier .   Through   MCA - KNN ,   the   four   result   sets   are   combined   to   get   the   final   result .   The   experimental   results   in   this   paper   demonstrate   MCA   - KNN ' s   capability   to   reduce   classifying   error   rate .   
 Key   words 　 KNN   Classifier ,   Multiple   Correspondence   Analysis ,   Cha   racter   Recognition 
 
 1 　 引言 
 　 　 在 模式识别 中 ， 最为 常用 的 分类 方法 为 KNN （ K 近邻 ） 方法 ． KNN 的 基本 思想 是 基于 以下 假设 ： 相似 样本 的 特征向量 值 具有 相似性 ． 其 基本 步骤 ： 若 对 一个 测试 样本 分类 ， 先 将 其 与 每个 被 存储 的 原型 （ 即 聚类 中心 ） 进行 比较 获取 相似性 测度 值 ， 然后 对此 相似性 测度 值 进行 排序 ， 取前 K 个 分类 结果 ， 再 利用 投票 原则 （ 或 加权 方法 ） 获得 各类 别的 票数 （ 或 得分 ） ， 将 此 样本 划分 为 票数 （ 或 得分 ） 最高 的 类别 ． 
 　 　 此 方法 存在 以下 问题 ． 不 相关 的 特征 可能 会 对 分类 起 负作用 ， 这是 由于 其 在 相似性 测度 中所 占 的 比重 过大 造成 的 ． 而且 KNN 在 特征 维数 较大 或 存储 的 原型 较 多 的 情况 下 存在 着 运算 量过大 的 现象 ， 这 对于 实际 中 遇到 的 有些 实时 分类 问题 是 一个 较大 的 困难 ． 虽然 这个 问题 可以 通过 特征 或 原型 选择 得到 解决 ， 但 需要 很大 的 预处理 工作 ． 本文 主要 讨论 KNN 分类器 组合 ． 
 2 　 分类器 组合 
 　 　 分类器 组合 应 满足 下述 三 原则 ： 
 　 　 ( 1 )   成员 分类器 的 精确性 要 高 ． 否则 组合 出 的 分类器 的 精确度 也 不会 很 高 ． 
 　 　 ( 2 )   成员 分类器 的 分类 结果 应 具有 多样性 ． 因为 用 对 相同 样本 犯 同样 分类 错误 的 成员 分类器   进行 组合 ， 对 组合 分类器 的 精确度 不能 带来 任何 提高 ． 
 　 　 ( 3 )   组合 分类器 的 效率 要 高 ． 这 对于 用于 实际 应用 的 分类 任务 尤其 重要 ． 
 　 　 然而 分类器 组合 的 最大 困难 就是 很难 同时 满足 精确性 和 多样性 ． 
 　 　 通常 从 两个 方面 考虑 分类器 组合 ： 
 　 　 ( 1 )   样本 的 不同 性质 的 特征 集 ． 如 识别 手写体 ， 可以 同时 利用 其 统计 特征 和 结构特征 ． 两者   对 最终 的 识别 结果 具有 互补 作用 ． 
 　 　 ( 2 )   在 样本 的 相同 特征 集上 ， 用 具有 不同 特性 的 分类器 进行 分类 ． 因为 在 样本 的 不同 空间 上   ， 它们 分类 结果 的 正确性 是 不同 的 ， 因此 可 根据 不同 分类器 的 分类 效果 来 组合 分类器 ． 例如   ， 用 多个 具有 不同 参数 的 神经网络 分类器 分别 在 特征 集 的 不同 子集 上 训练 ， 再用 一个 分类器   学习 这些 子 分类器 的 分类 特点 ， 从而 在子 分类器 的 分类 结果 上 最终 获得 未知 样本 的 类别 ． 
 3 　 基于 多元 对应 分析 的 KNN 分类器 组合 方法 （ MCA - KNN ）   
 　 　 假设 样本 共有 N 类 ， 其 类别 由 Ci ,   i = 1 , 2 , … , N 表示 ． 用 M 个 KNN 分类器 进行 分类 ， 记为 KNNi , i = 1 , 2 , … , M .   本文 中 M = 4 ， 每个 KNN 分类器 的 K = 5 ． 对 某个 测试 样本 Tj   ,   KNNi 分类器 得到 的 结果 为 rji1 , rji2 , … , rjik , i = 1 , 2 , …   , M ． 其中 rjik ∈ ( Cl ) ,   l = 1 , 2 , … , N ,   k = 1 , 2 , … , K 为 KNNi 分类器 依次 得到 的   第 l 个 原型 所属 的 类别 ． 
 　 　 本文 首先 用 实验 样本 集中 的 A 作为 原型 集 ， B 作为 测试 集 获得 分类 结果 R1 ， 再用 实验 样   本 集中 的 B 作为 原型 集 ， A 作为 测试 集 获得 分类 结果 R2 ． 集合 A 、 B 的 样本容量 相等 为 n ． 
 　 　 对于 R1 ， 构成 以下 的 列联表 X1 = ( xjl ) j = 1 , 2 , … , n ,   l = 1 , 2 , … , ( M + 1 ) * N . 构成 过程 为 ： 
 　 　 对于 R1 中 的 第 j 行 rj11 , rj12 , … , rj1k , rj21 , rj   22 , … , rj2K , … , , rjM1 , rjM2 , … , rjMK , 其中 rj   11 , rj12 , … , rj1K 是 KNN1 分类器 对 测试 样本 Bj 得到 的 结果   ， 由 其 获得 列联表 X1 中 的 ( xjl ) ,   l = 1 , 2 , … , N ; 
 　 　 rj21 , rj22 , … , rj2K 是 KNN1 分类器 对 测试 样本 Bj 得   到 的 结果 ， 由 其 获得 列联表 X1 中 的 ( xjl ) ,   l = N + 1 ,   N + 2 , … , 2 * N ; 
 　 　 rjM1 , rjM2 , … , rjMK , 是 KNNM 分类器 对 测试 样本 Bj 得   到 的 结果 ， 由 其 获得 列联表 X1 中 的 ( xjl ) ,   l = ( M - 1 ) * N + 1 ,   ( M - 1 ) * N + 2 , … , M * N . 
 对于 i = 1 , 2 , … , M ,   k = 1 , 2 , … , K ,   l = ( i - 1 ) * N + 1 , ( i - 1 ) * N + 2 , … , i * N , 
 　 　 　 ( 1 ) 
 j = 1 , 2 , … , n 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 
 对于 l = M * N + 1 , M * N + 2 , … , ( M + 1 ) * N , 
 　 　 　 　 　 　 　 ( 2 ) 
 j = 1 , 2 , … , n 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 
 　 　 若 测试 样本 Bj 的 真实 类别 为 C ， 假设 存在 一个 最佳 KNN 分类器 ， 记为 KNN - BEST ， 则 KNN -   BEST 对 Bj 的 分类 结果 依次 应全 为 C ． ( 2 ) 式 实际上 是 由 KNN - BEST 对 Bj 得到 的 结果   获得 列联表 X1 中 的 ( xjl ) ,   l = M * N + 1 , M ) * N + 2 , … , M * N + N . 
 　 　 对于 R2 , 同样 构成 列联表 X2 = ( xjl )   j = 1 , 2 , … , n ,   l = 1 , 2 , … , ( M + 1 ) * N . 　 
 　 　 下面 用 实验 数据 作一 示例 ， 取 M = 4 ， K = 5 ,   N = 10 ． 
 　 　 设对 测试 样本 T , KNNi ,   i = 1 , 2 , … , M 获得 表 1 中 的 结果 ． 
 　 　 示例 所 对应 的 行向量 为 ： 
 ( 15   0   0   0   0   0   0   0   0   0 ,   7   0   0   0   0   0   0   0   8   0 , 
 　 14   0   1   0   0   0   0   0   0   0 ,   12   0   0   3   0   0   0   0   0   0 , 
 　 15   0   0   0   0   0   0   0   0   0 ) 　 　 　 　 　 　 　 　 　 　 　 
 　 　 由于 KNN 分类器 得到 的 分类 结果 与 测试 样本 的 相似 度 随 k 值 增大 而 递减 , 故对 结果 按 K   , K - 1 , … , 1 记分 ． 由 公式 （ 1 ） ， （ 2 ） 依次 得到 各 测试 样本 对 所有 类别 的 得分 ， 构成 列联表中 相应 的 行向量 ． 
 　 　 令 X = ( X1   X2 ) ′ ,   对 X 进行 多元 对应 分析 ， 获取 各 KNN 分类器 对 所有 类别 的 权值   ． 
 表 1   对 类别 为 C1 ( 即 为 数字 0 ) 的 测试 样本 T ， 各 KNN 分类器 结果 
 
 　 对 测试 样本 T 
 KNN 分类器 KNN1KNN2KNN3KNN4KNN - BEST 
 K 值 1234512345123451234512345 
 分类 结果 0000080088000020030000000 
 得分 5432154321543215432154321 
 真实 类别 C1 
 
 　 
 　 　 训练 算法 如下 ： 
 ( 1 )   对 各 KNN 分类器 的 分类 结果 , 通过 公式 ( 1 ) , ( 2 ) 得到 相应 的 列联表 ． 
 X = ( xij ) n × p , n 为 测试 样本数 ， p = ( M + 1 ) * N , 
 　 　 式 中 N 为 类别 总数 ， M 为 参与 组合 的 KNN 分类器 数目 
 ( 2 )   对 矩阵 X ， 有 ： 
 　 　 　 　 　 
 　 　 　 　 ( 3 ) 
 式 中 diag ( a ) 表示 向量 a 构成 的 对角 矩阵 
 
 
 ( 3 )   对 矩阵 F 进行 奇异 值 分解 ， 得到 
 F = U * Λ * V ′ 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　   
 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 
 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 ( 4 ) 
 ( 4 )   对 获得 的 投影 矩阵 P = ( pij ) P × p 取 P 的 p 个 行向量 的 前 q 个值 作为 拟合 后 的 投   影 矩阵 Project ， 即 Project = ( pij ) p × q ． 
 　 　 对 （ 3 ） 中 矩阵 Z ， 有 各类 别的 聚类 中心 矩阵 
 C = ( cij ) N × n ,   cij = Zij ,   i ′ = M * N + i , 
 　 　 式 中 N 为 类别 总数 ， M 为 参与 组合 KNN 分类器 数目 ， n 为 测试 样本数 ． 
 　 　 取 C 的 N 个 行向量 的 前 q 个值 作为 拟合 后 的 聚类 中心 矩阵 Cluster ， 
 　 　 即 Cluster = ( cij ) N × q ． 
 　 　 MCA - KNN 分类器 组合 算法 的 实质 ： 对 训练 集中 的 所有 样本 构成 一个 最佳 KNN 分类   器 KNN - BEST ， 对 参与 组合 的 KNN 分类器 及 KNN - BEST 分类器 的 分类 结果 构成 列联表 ． 通过 多元 对应 分析 ， 获得 在 Rp （ 即列 向量 ） 中 的 q 维 拟合 ， 即 求得 以下 投影 向量 
 
 使得 
 　 　 　 　 　 　 　 　 　 ( 5 ) 
 达到 最大 ， 这 等同于 对 矩阵 F 进行 奇异 值 分解 ． 为 参与 组合 的 KNN 分类器 在 各类 别上   赋予 相应 权值 来 反映 它们 与 KNN - BEST 分类器 之间 的 关系 ． 从 训练 算法 中 ， 得到 的 投影 矩   阵 Project 为此 权值 ； 得到 的 聚类 中心 矩阵 Cluster 为此 KNN - BEST 分类器 投影 后 在 各   类别 的 数值 ． 
 　 　 由 A 、 B 作为 原型 集 ， 各 KNN 算法 对 T 进行 分类 得到 结果 RT ． 对 第 j 行向量 （ 即 集合 T   中 第 j 个 测试数据 Tj 的 分类 结果 ） 
 rj = ( rj11 , rj12 , … , rj1K ,   rj21 , rj22 ,   … , rj2K ,   … , rjM1 ,   rjM2 , … , rjMK ) 
 获得 相应 的 N 个 类别 的 测试 向量 ． 
 　 　 　 ( 6 ) 
 
 l1 = ( j - 1 ) * N + 1 , ( j - 1 ) * N + 2 , … , j * N , 
 l2 = M * N + 1 , M * N + 2 , … , M * N + N , 
 j = 1 , 2 , … , M , 式 中 N 为 总 类别 数 ， M 为 参与 组合 的 KNN 分类器 个数 , K   为 KNN 分类器 中取 的 K 值 . 
 对 数据 Tj 获得 的 N 个 测试 向量 有 ， 
 Re   sulti = Testi * Project , 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 
 di = ‖ Resulti - Clusteri ‖ , 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 ( 7 ) 
 i = 1 , 2 , … , N 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 
 dmin = min ( di ) ,   min (   ) 获得 变量 的 最小值 ， 最终 的 分类 结果 Class   = min ． 
 4 　 MCA - KNN 分类器 组合 在 手写体 识别 中 的 应用   
 4.1   测试 样本 集 
 　 　 本文 使用 的 实验 样本 集 取自 Concordia   University 的 CENPARMI 数据库 中 的 美国 邮政编码 集 ． 它 大约 包含 17000 个二值 化过 的 数字图像 （ 分 属于 数字 0 ， 1 ， …   ， 9 这 十类 ） ． 该 邮政编码 集中 的 样本 不是 均匀分布 的 ， 如 其中 有 697 个 “ 9 ” ， 3595 个 “ 1 ” ． 为 使 不同 分类器 的 分类 结果 具有 可比性 ， 用此 数据 集 产生 了 3 个 标准集 ， 每个 标准集 包含 2000 个 样本 ， 且 关于 类别 的 分布 是 均匀 的 ． 这 三个 标准 集中 的 两个 被 用来 作为 训练 集 ， 分别 记为 A 和 B ， 第三个 被 用来 测试 ， 记为 T ． 
 　 　 对 实验 数据 进行 特征提取 获得 如下 四种 特征 ： 
 　 　 ( 1 )   Gabor 特征 ， 采用 8 * 8 采样 点 ， 4 个 方向 ( 0 ° ,   45 ° ,   90 ° ,   135 ° ) ． 抽取 特征 前 ， 图像 被 细化 成 骨架 图像 ． 每个 样本 用 一 256 维 的 特征向量 代表 ． 
 　 　 ( 2 )   Legendre 矩 ， 其 X ， Y 各 计算 到 10 阶 ． 图像 的 预处理 过程 同 2 ． 每个 样本 用 一 121 维 的 特征向量 代表 ． 
 　 　 ( 3 )   Pseudo - Zernike 矩 ， 其 被 计算 到 8 阶 ． 图像 的 预处理 过程 同 2 ． 每个 样本 用 一 36 维 的 特征向量 代表 ．   
 　 　 ( 4 )   Zernike 矩 ， 其 被 计算 到 第 9 阶 ． 抽取 特征 前 ， 图像 矩阵 经过 简单 的 归 正 处理 ， 将 矩阵 中 表示 数字 笔画 的 值 的 个数 归正为 300 个 ． 每个 样本 用 一 30 维 的 特征向量 代表 ． 
 4.2   KNN 分类器 中 K 的 选取 
 　 　 对 各 特征 集上 KNN 算法 对 不同 的 K 值得 到 的 分类 结果 见表 2 ． 将 训练 集 A 与 B 作为 原型 存储 ， 再 对 测试 集 T 进行 投票 得到 结果 ， 即将 未知 样本 划分 为 获得 最 多 票数 的 类别 ． 若 存在 最 多 票数 一样 的 情况 ， 则 判为 排 在 前面 的 类别 ． 
 
 表 2   当 K ( 1 - 10 ) ， KNN 算法 对 四种 特征 集 得到 的 分类 错误率 
 
 特征 集 ＼ KNN 分类 错误率 ( % ) K 
 12345678910 
 Gabor 特征 17.8517 . 8517.1016 . 7017.6518 . 0519.1018 . 9019.6519 . 75 
 Legendre 矩 8.758 . 758.58 . 88.758 . 98.88 . 959.49 . 5 
 Pseudo - Zernike 矩 23.6523 . 6522.0022 . 1521.2521 . 0020.5520 . 2020.8520 . 90 
 Zernike 矩 25.4525 . 4522.5021 . 4020.9021 . 1020.9021 . 0521.0521 . 55 
 合计 18.92518 . 92517.52517 . 26317.13817 . 26317.33817 . 27517.73817 . 925 
 
 
 　 　 由 实验 可 得到 如下 结论 ： 
 　 　 ( 1 )   对于 不同 的 特征 集 ， 随 K 的 增大 ， KNN 算法 分类 错误率 非 一致 递减 ． 
 　 　 ( 2 )   K 大于 等于 3 后 ， KNN 算法 分类 错误率 递减 较慢 ． 
 　 　 关于 第一个 结论 ， 我们 认为 随着 K 的 增大 ， 得到 的 相应 原型 与 测试 样本 的 相似性 逐渐 减小 ， 即 它们 之间 的 距离 逐渐 增大 ， 该 测试 样本 属于 相应 原型 的 可信度 逐渐 减小 ， 可能 带来 越来越 多 错误类别 的 投票 ， 故 KNN 算法 分类 错误率 在 K 增大 到 一定 程度 时 （ 如 大于 7 后 ） ，   开始 逐渐 变 大 ． 对 第二个 结论 ， 由于 K 较大 后 ， 尽管 出现 错误 原型 的 概率 在 增大 ， 但 鉴于 前面 已 存在 的 正确 原型 ， 错误 原型 的 影响 速度 较慢 ． 
 　 　 如表 2 所示 ， 当 K = 5 时 ， KNN 分类器 的 效果 最好 ． 故 在 以下 组合 算法 中 ， K 取值 为 5 ． 我们 用 KNN - FEATURE1 ， KNN - FEATURE2 ， KNN - FEATURE3 ， KNN - FEATURE4   分别 代表 在 Gabor 特征 集 、 Legendre 矩 特征 集 、 Pseudo - Zernike 矩 特征 集及 Zernike 矩 特征 集上 的 KNN 分类 算法 ． 
 4.3   分类器 组合 
 　 　 分类器 组合 的 框架 见图 1 ． 
 
 图 1 　 分类器 组合 框架 
 　 　 为 充分利用 样本 的 信息 ， 首先 用 一个 严格 结构 分类器 进行 预 识别 ． 并 保证 这 一层 分类 的   正确性 ， 即 有 很 高可靠性 ． 然后 再 利用 样本 的 四种 不同 特征 集做 四个 KNN 分类器 ， 对 它   们 进行 组合 ， 对 被 预 识别 拒识 的 字 进行 识别 ． 
 　 　 图 1 中 结构 分类器 的 识别率 为 53.95% ， 其余 全部 拒识 ． 虽然 拒识率 很 高 ， 但是 可靠性 达   到 100% ． 此 结构 分类器 能 识别 出 一些 非常简单 的 数字 ， 例如 当 输入 图像 是 一个 简单 闭合 圈时   ， 它 一定 识为 0 ． 用来 预 识别 的 分类器 不 要求 完善 ， 只 需能 将 测试 样本 中 的 一部分 进行 分类   ， 且 可靠性 高 即可 ． 
 　 　 在 第二层 上 ， 对 结构 分类器 不能 识别 的 测试 样本 ， 用 上述 讨论 的 KNN 分类器 进行 识别 ． 这 四个 分类器 得到 的 结果 分别 是 与 所 测试 样本 最为 相近 的 前 五个 原型 的 所属 类别 ． 
 　 　 第三层 是 将 第二层 由 四个 KNN 分类器 得到 的 分类 结果 采用 MCA - KNN 算法 进行 组合 ，   从而 得到 最终 的 分类 结果 ． 
 　 　 表 3 为 MCA - KNN 算法 中 拟合 维数 q 变化 时 得到 的 结果 ． q 的 最大值 为 矩阵 F （ 公式 （ 3 ） ） 的 秩 ， 此 实验 中为 45 ． 
 
 表 3   拟合 维数 q 变化 时 ， 组合 算法 得到 的 结果 
 
 q12345678910 
 分类 错误率 （ % ） 25.3018 . 9012.358 . 407.456 . 454.153 . 353.053 . 05 
 奇异 值 比率 （ % ） 6.9913 . 8920.3926 . 6332.7738 . 6844.3349 . 8755.3757 . 53 
 
 q11121315202530354045 
 分类 错误率 （ % ） 3.003 . 003.003 . 003.003 . 003.003 . 003.003 . 00 
 奇异 值 比率 （ % ） 59.5561 . 5463.4367 . 0274.6981 . 2187.1592 . 3896.73100 . 0 
 
 　 
 　 　 奇异 值 比率 = （ 前 q 个 奇异 值 的 和 ） / （ 全部 奇异 值 的 和 ） * 100 ． 
 　 　 当 q 从 1 递增 到 11 时 ， 分类 错误率 由 25.30% 递减 到 3.0% ； q 从 11 递增 到 最大 时 ， 分类 错误率 保持 3.0% 不变 ． 这 表明 q = 11 时 ， 列 向量 的 拟合 结果 就 已 达到 最好 的 分类 效果 ， 继续 增大 q 值 ， 并 不会 提高 分类 的 精度 ． 
 　 　 表 4 列出 了 以上 四种 KNN 分类器 （ 包含 或 不 包含 结构 分类器 的 预处理 ） 不同 组合 情况 下 ， 通过 对应 分析方法 及 简单 的 记分 投票 方法 （ VOTING ） 得到 的 最好 结果 ． VOTING 方法 是 将 参与 组合 的 KNN 分类器 对 各类 的 得分 分别 求和 ， 最终 分类 结果 为 得分 最高 的 类别 ． 
 表 4   不同 组合 情况 下 ， 对应 分析方法 与 VOTING 方法 分类 错误率 比较   
 
 　 无 结构 分类器 预处理 结构 分类器 预处理 
 分类 错误率 （ % ） KNN - FEATURE1 
 KNN - FEATURE3 
 KNN - FEATURE4KNN - FEATURE1 
 KNN - FEATURE2 
 KNN - FEATURE3 
 KNN - FEATURE4KNN - FEATURE1 
 KNN - FEATURE3 
 KNN - FEATURE4KNN - FEATURE1 
 KNN - FEATURE2 
 KNN - FEATURE3 
 KNN - FEATURE4 
 VOTING 方法 10.956 . 456.353 . 85 
 对应 分析方法 8.504 . 755.203 . 00 
 
 　 
 　 　 表 4 中 ， 对应 分析方法 显然 优于 简单 记分 投票 方法 ． 加入 结构 分类器 进行 预处理 减小 了 分类 错误率 ， 这 表明 结构 分类器 与 其余 四种 非 结构特征 集 存在 着 互补 的 关系 ． 
 5 　 结论 
 　 　 基于 多元 对应 分析 的 KNN 分类器 组合 方法 （ MCA - KNN ） 通过 将 所有 分类器 的 分类 结果 逼近 一个 最佳 KNN 分类器 ， 给出 各 分类器 组合 时 相应 的 权值 ． 此 算法 能 显著 降低 分类 错误率 ， 如 本文 实验 中 所 得到 的 那样 ， 分类 错误率 从 参与 组合 效果 最好 的 KNN - FEATURE2 分   类器 的 8.75% 降到 3.00% ． 对于 如何 选取 参与 组合 的 分类器 有待 继续 研究 ． 
 作者简介 ： 韩 　 宏 ， 女 ， 1973 年生 ， 博士 研究生 ． 研究 领域 为 图象处理 、 模式识别 ． 
 　 　 　 　 　 杨静宇 ， 男 ， 1941 年生 ， 教授 ， 博士生 导师 ． 研究 领域 为 模式识别 、 计算机 视觉 ． 
 　 　 　 　 　 胡 钟山 ， 男 ， 1973 年生 ， 博士 研究生 ． 研究 领域 为 图象处理 、 模式识别 ． 
 作者 单位 ： 南京 理工大学 计算机系 　 南京 　 210094 
 参考文献 
 1 　 任若恩 .   多元 统计 数据分析 — 理论 、 方法 、 实例 .   1997 
 2 　 Oivind   D   T ,   Anil   K   J ,   Torfinn   T .   Feature   Extraction   Methods   for   Charac   ter   Recognition ?   A   Survey .   Pattern   Recognition ,   1996 ,   29 ( 4 ) : 641 ～ 662   
 3 　 宣国荣 ， 柴佩琪 .   基于 巴氏 距离 的 特征选择 .   模式识别 与 人工智能 ,   1996 ,   9 ( 4 ) : 324 ～ 329 
 4 　 Ching   Y   Suen ,   Christine   Nadal ,   Raymond   Legault ,   Tuan   A .   Mai   and   Louisa   Lam .   Computer   Recognition   of   Unconstrained   Handwritten   Numerals .   Proceedings   of   IEEE ,   1992 ,   80 ( 7 ) : 1162 ～ 1179 
 收稿 日期 ： 1999 - 04 - 27 
