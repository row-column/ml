自动化 学报 
 ACTA   AUTOMATICA   SINICA 
 1997 年 　 第 23 卷 　 第 6 期 　 Vol.23 　 No.6 　 1997 
 
 
 
 
 一种 前向 神经网络 快速 学习 算法 
 及其 在 系统 辨识 中 的 应用 
 王正欧 　 林晨 
 　 　 摘 　 要 　 提出 一种 基于 最小 二乘 的 前 向 神经网络 快速 学习 算法 . 与 现有 同类 算法 相比 ， 该 算法 无需 任何 矩阵 求逆 ， 计算 量 小 ， 较 适于 需 快速 学习 的 系统 辨识 和 其他 应用 . 文中 推导 了 算法 ， 并 给出 一种 更为 简便 的 局部 化 算法 . 系统 辨识 的 仿真 实例 表明 了 算法 的 优良 性能 . 
 　 　 关键词 　 前向 神经网络 ， 快速 学习 ， 系统 辨识 . 
 A   FAST   LEARNING   ALGORITHM   OF   FEEDFORWARD   NEURAL 
 NETWORKS   AND   ITS   APPLICATION   TO 
 SYSTEM   INDENTIFICATION 
 WANG   ZHENG ' OU 
 ( Institute   of   Systems   Engineering , Tianjin   University , Tianjin 　 300072 ) 
 LIN   CHEN 
 ( Fujian   Asia   Bank   Limited , Fuzhou 　 350001 ) 
 Abstract 　 In   this   paper , we   propose   a   fast   learning   algorithm   of   feedforward   networks   based   on   the   least   squares . Compared   with   existing   similar   algorithms , the   present   algorithm   does   not   require   any   matrix   inversion , therefore , it   has   a   less   computational   cost   and   can   be   better   suited   for   system   indentification   and   other   areas   where   fast   learning   is   required . We   derive   the   algorithm   and   also   give   an   even   simpler   and   more   convenient   localized   algorithm . Simulation   results   for   system   identification   show   the   effectiveness   of   the   algorithm . 
 Key   words 　 Feedforward   neural   networks , 　 fast   learning , 　 system   identification . 
 1 　 引言 
 　 　 自前 向 神经网络 BP 算法 ［ 1 ］ 提出 以来 ， 己 在 控制 界 和 其他 领域 得到 广泛应用 ［ 2 ， 3 ］ . 由于 BP 算法 存在 收敛 慢 和 局部 极小 点 等 问题 ， 迄今已 提出 大量 改进 算法 ， 其中 采用 增广 卡尔曼滤波 的 学习 算法 ［ 4 ］ 引起 了 广泛 地 重视 . 这类 算法 无需 对 学习 速率 和 势态 项 系数 进行 猜测 ， 且 收敛 速度 快 、 精度高 . 其 基本 思想 是 把 网络 权值 作为 一个 相应 动态 系统 的 状态 ， 运用 增广 卡尔曼滤波 估计 ， 即可 得到 好 的 效果 . 然而 这 类 算法 由于 维数 过 高且 需 矩阵 求逆 ， 限制 了 网络 的 规模 . 多种 局部 化 算法 ［ 5 ， 6 ］ 的 提出 ， 虽 降低 了 维数 ， 但 仍 免不了 矩阵 求逆 . 本文 提出 一种 基于 最小 二乘 的 快速 学习 算法 ， 无需 任何 矩阵 求逆 ， 其 相应 的 局部 化 算法 更 极大 地 减少 了 计算 量 ， 提高 了 计算速度 ， 较 适于 系统 辨识 等 实际 应用 场合 . 
 2 　 网络 和 相应 的 动态 系统 
 　 　 考虑 一个 任意 的 具有 一个 输入 层 、 一个 输出 层 和 若干 隐层 的 前 向 网络 ， 设此 网络 包含 一个 由 全部 连接 权 构成 的 权 向量 θ ∈ RM ， 并 认为 一个 单元 的 阈值 是 由 单位 强度 输入 连接 到 该 单元 的 连接 权 . 假定 网络 中 单元 的 激活 函数 ( 除 输入 层外 ) 是 S 形 函数 φ ( x ) = 1 / ( 1 + e - x ) ， 其中 x 为 该 单元 的 纯 输入 ， 它 等于 输入 到 该 单元 信号 的 加权 和 . 现在 的 问题 是 要 确定 θ ， 使得 对于 一组 给定 的 输入 向量 i ( k ) ∈ Rn ( k = 1 , … , p ) ， 网络 产生 的 实际 输出 向量 o ( k ) ∈ Rm ( k = 1 , … , p ) 应 尽可能 接近 期望 的 输出 向量 d ( k ) ∈ Rm ( k = 1 , … , p ) . 这里 考虑 在线 学习 算法 ， 即 输入 模式 依次 输入 至 输入 层 ， 权值 则 递推 地 改进 . 将 θ 视为 如下 非线性 动态 系统 的 状态 
 θ ( k + 1 ) = θ ( k ) = θ 0 , 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 ( 1 ) 
 d ( k ) = f ( θ 0 , i ( k ) ) + e ( k ) . 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 ( 2 ) 
 这里 f ( θ 0 , i ( k ) = o ( k , θ 0 ) 表示 网络 的 输入 、 权值 和 输出 关系 的 非线性 函数 ， e ( k ) 为 模型 误差 . f ( θ 0 , i ( k ) ) 可 在 当前 估计值 ( k - 1 ) 附近 展开 ， 故 ( 1 ) ， ( 2 ) 式 可 改写 为 
 θ ( k + 1 ) = θ ( k ) = θ 0 , 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 ( 3 ) 
 d ( k ) = f ( ( k - 1 ) , i ( k ) ) + GT ( k ) ( θ 0 - ( k - 1 ) ) + ρ ( k ) + e ( k ) . 　 　 　 　 　 　 　 　 ( 4 ) 
 这里 G ( k ) 是 M × m 维 梯度 矩阵 
 　 　 　 　 　 　 　 　 　 　 　 ( 5 ) 
 ρ ( k ) 是 f 的 展开式 的 高阶 余项 . 若设 
 ξ ( k ) = f ( ( k - 1 ) , i ( k ) ) - GT ( k ) ( k - 1 ) + ρ ( k ) , 　 　 　 　 　 　 　 　 　 　 ( 6 ) 
 则 ( 4 ) 式 可 简写 为 
 d ( k ) = GT ( k ) θ 0 + ξ ( k ) + e ( k ) . 　 　 　 　 　 　 　 　 　 　 　 　 　 ( 7 ) 
 如 将 网络 中 单元 由 1 至 N 排列 ， 并取 权值 向量 为 θ = [ θ T1 　 θ T2 … θ TN ] T ， 则 ( 7 ) 式 可 写 为 
 　 　 　 　 　 ( 8 ) 
 这里 θ i 为 连接 到 第 i 个 单元 的 权 向量 ， c ( k ) = ξ ( k ) + e ( k ) . G ( k ) 中 的 偏 导数 可 通过 网络 反传 o ( k ) 求得 . 
 3 　 学习 算法 
 3.1 　 全局 递推 最小 二乘 算法 ( GRLS ) 
 　 　 基于 模型 ( 8 ) 给定 一个 输入 / 输出 序列 对 ( i ( j ) , d ( j ) ) ( j = 1 , … , k ) ， 选择 如下 的 目标 函数 
 　 　 　 　 　 　 　 　 　 ( 9 ) 
 这里 ‖ . ‖ 表示 向量 的 欧几里得 范数 ， ei ( j ) 是 ( 8 ) 式 中 第 i 个 建模 误差 ， λ 为 遗忘 因子 0 ＜ λ ≤ 1 ， 估计值 ( k ) 是 由 使 ε ( k ) 达到 极小 而 得 的 . 此解 可 从 下列 正则方程 推导 
 　 　 　 　 　 ( 10 ) 
 其中 1 × M 维 向量 gTi ( j ) 是 矩阵 GT ( j ) 的 第 i 行 . 由 ( 10 ) 式 可得 
 ( k ) = S - 1 ( k ) t ( k ) . 　 　 　 　 　 　 　 　 　 　 　 　 　 　 ( 11 ) 
 其中 　 　 　 　 　 　 　 　 　 　 　 　 　   　 　 　 　 　 　 　 　 　 　 ( 12 ) 
 　 　 　 　 　 　 　 　 　 ( 13 ) 
 由 ( 11 ) 式 可 推得 GRLS 算法 如下 ： 
 　 　 　 ( 14 ) 
 　 　 　 　 　 ( 15 ) 
 算法 初值 可取 p ( 0 ) = r - 1I ， r 为任 一小 正数 ， I 为 单位 阵 ， ( 0 ) 可为 非 零 的 随机 向量 . ( 14 ) ， ( 15 ) 式 的 推导 见 附录 . 
 　 　 注意 ， 上述 算法 要求 存储 和 改进 一个 M × M 维 矩阵 ， 但 不 要求 任何 矩阵 求逆 . 
 3.2 　 局部 化 递推 最小 二乘 算法 ( LRLS ) 
 　 　 在 GRLS 算法 中 P 阵 的 维数 相当 高 ， 仍难 实用 . 一种 有效 的 简化 方法 是 将 系统 分解成 若干 较 小 的 子系统 ， 例如 分解 到层 一级 、 神经元 一级 乃至 到 单个 连接 权 一级 等 . 这里 拟 在 神经元 一级 推导 LRLS 算法 . 
 　 　 考虑 网络 中 第 j 个 单元 ， 假设 它 具有 Mj + 1 维 连接 权 向量 θ j 和 Mj + 1 维 的 输入 向量 ， θ j 中 包含 了 该 单元 的 阈值 . 该 单元 对 总 输出 O ( k ) 的 影响 可用 梯度 矩阵 GjT ( k ) 描述 ， 它 是 GT ( k ) 的 第 j 个子 块 
 　 　 　 　 　 　 　 　 　 　 　 　 　 　 ( 16 ) 
 此 矩阵 的 第 i 行是 ， 用 gjTi ( k ) 表示 . 如 相对 于 第 j 个 神经元 的 权值 线性化 f ， 则 类似 于 ( 4 ) 式 有 
 d ( k ) = f ( ( k - 1 ) , i ( k ) ) + GjT ( k ) [ θ j - j ( k - 1 ) ] + ρ j ( k ) + e ( k ) = GjT ( k ) θ j + ξ j ( k ) + e ( k ) . 　 　 ( 17 ) 
 其中 　 　 　 　 　 ξ j ( k ) = f ( ( k - 1 ) , i ( k ) ) - GjT ( k ) j ( k - 1 ) + ρ j ( k ) . 　 　 　 　 　 　 　 　 　 　 　 　 ( 18 ) 
 这里 台劳 展开 仅 对 第 j 个 单元 的 权值 进行 ， 其余 权值 采用 k - 1 时刻 以前 的 估计值 . 此 非线性 函数 的 线性化 可 对 每个 单元 平行 地 进行 . 
 　 　 对于 每个 单元 改进 权值 的 局部 化 算法 类似 于 ( 14 ) ， ( 15 ) 式 可得 
 　 　 ( 19 ) 
 　 　 　 　 　 　 　 　 　 ( 20 ) 
 注意 到 LRLS 仅 要求 存储 和 改进 一个 ( Mj + 1 ) × ( Mj + 1 ) 维 矩阵 ( 随 单元 不同 Mj 可 不同 ) ， 此 矩阵 维数 大大 小于 GRLS 中 相应 矩阵 的 维数 ， 故 LRLS 的 存储 和 计算 量 远 小于 GRLS . 
 4 　 LRLS 在 系统 辨识 中 的 应用 
 　 　 系统 辨识 通常 和 自 适应控制 相 联系 ， 要求 快速 及时 地 辨识 以利于 控制 ， 但 通常 的 学习 算法 受 收敛 速度限制 ， 效果 难以 理想 . LRLS 算法 可较 好 地 弥补 这一 缺陷 . 
 　 　 为 研究 该 算法 在 系统 辨识 中 的 性能 ， 这里 将 它 同 原始 BP 算法 作 了 仿真 比较 . 为 实用 计 ， 只 研究 LRLS 的 性能 . 由于 前向 多层 网络 可 对 任意 线性 和 非线性 函数 映射 . 不失 一般性 ， 这里 仅 研究 线性系统 辨识 例子 . 为 简单 计 ， 在 仿真 中 略去 了 系统 模型 误差 ， 以 突出 说明 算法 的 收敛性 和 精度 . 由 经验 选取 λ = 0.995 ， 以利于 算法 收敛 . 以下 用 4 个 例子 说明 新 算法 的 优越性 . 
 　 　 例 1 . 考虑 简单 单 变量 系统 
 y ( k ) = 0.8 y ( k - 1 ) + 0.2 u ( k ) , 
 这里 输入 u ( k ) 是 在 ( 0 ， 1 ) 区间 中 均匀分布 的 随机变量 ， 网络结构 为 两个 输入 u ( k ) 和 y ( k - 1 ) 、 一个 15 单元 隐层 及 一个 输出 y ( k ) ( 2 - 15 - 1 ) . 初始 权值 为 0 与 0.01 间 均匀分布 的 随机 值 ， Pj   初始 阵中 r = 0.001 . 原始 BP 算法 中 学习 速率 和 势态 项 系数 分别 取为 0.9 和 0.3 . 训练样本 数为 30 . 训练 停止 准则 为 ei < 0.05 ， 其中 e 表示 30 个 样本 中 输出 误差 最大值 ， 下标 i 表示 迭代 次数 . 图 1 表示 采用 LRLS 和 原始 BP 算法 在 不同 初始条件 下 10 次 训练 的 平均误差 随 迭代 次数 的 变化 曲线 . 
 
 图 1 　 对例 1 采用 LRLS 和 BP 的 平均 输出 误差 曲线 
 　 　 例 2 . 考虑 动态 系统 
 y ( k ) = 1.368 y ( k - 1 ) - 0.368 y ( k - 2 ) + 0.368 u ( k - 1 ) + 0.364 u ( k - 2 ) , 
 这里 u ( k ) 是 0 和 1 间 均匀分布 随机变量 ， 网络结构 为 4 - 10 - 1 ， 初始条件 和 停止 准则 同例 1 ， 训练样本 为 30 . 图 2 表示 LRLS 和 原始 BP 算法 收敛 速度 比较 ， 其中 BP 算法 参数 取法 同例 1 . 
 
 图 2 　 对例 2 采用 LRLS 和 BP 的 平均 输出 误差 曲线 
 　 　 例 3 . 考虑 单 输入 二 输出 系统 
 y1 ( k ) = u ( k - 1 ) , 
 y2 ( k ) = u ( k - 1 ) + u ( k - 2 ) . 
 网络 取为 双隐层 结构 2 - 10 - 10 - 2 ， u ( k ) 取为 0 — 0.5 间 均匀 随机变量 . Pj 初始 阵中取 r = 1000 ， 网络 初始 权重 取为 0 — 0.5 间 均匀 随机数 ， 阈值 全部 取为 0.5 . 判断 收敛 的 误差 函数 取为 均方 误差 其中 样本数 n = 30 ， ei 为 第 i 个 样本 的 学习 误差 . 图 3 表示 LRLS 与 原始 BP 算法 收敛 速度 的 比较 ， 其中 BP 算法 取 学习 速率 为 0.83 ， 势态 项 系数 为 0.23 . 
 
 图 3 　 对例 3 采用 LRLS 和 BP 的 均 方 输出 误差 曲线 
 注 . LRLS 迭代 至 129 次 精度 达 0.004979 ， 而 BP 到 18700 次 精度 才 达 0.01347 . 
 例 4 . 考虑 二 输入 二 输出 动态 系统 
 y1 ( k ) = 0.8 y1 ( k - 1 ) + 3u1 ( k - 1 ) + u2 ( k - 1 ) , 
 y2 ( k ) = - 0.8 y2 ( k - 1 ) - 4u1 ( k - 1 ) + u2 ( k - 1 ) . 
 网络 取为 双隐层 结构 4 - 10 - 10 - 2 ， 其中 u1 ( k ) , u2 ( k ) 均 取为 0 — 1 间 均匀 随机变量 . 模型 初始值 取为 y1 ( 0 ) = y2 ( 0 ) = 0 ， 网络 和 算法 的 初始值 同例 3 ， 判断 收敛 用 的 准则 仍为 均方 误差 . 图 4 显示 了 LRLS 与 原始 BP 算法 收敛 速度 的 比较 ， 其中 BP 算法 的 参数 同例 3 . 
 
 图 4 　 对例 4 采用 LRLS 和 BP 的 均 方 输出 误差 曲线 
 注 . LRLS 迭代 至 112 次 精度 达 0.007984 ， 而 BP 到 9651 次 精度 才 达 0.007981 . 
 　 　 从 上述 四个 例子 可见 LRLS 具有 极好 的 性能 ， 其 收敛 速度 远比 原始 BP 算法 快 . 
 5 　 结论 
 　 　 本文 推导 了 一种 前向 网络 的 快速 学习 算法 GRLS ， 给出 了 一种 更为 简便 的 局部 化 算法 LRLS . 该 算法 避免 了 矩阵 求逆 ， 减少 了 计算 量 ， 其中 LRLS 算法 更 使 存储量 和 计算 量 大为 降低 . 仿真 结果表明 了 这种 算法 具有 收敛 快速 、 精度高 的 特点 ， 是 一种 实用 的 前 向 网络 快速 学习 的 有效 算法 ， 适用 于 系统 辨识 等 实际 应用 场合 . 
 1 ) 　 天津市 自然科学 基金 资助 课题 . 
 作者简介 ： 王正欧 　 1961 年 毕业 于 天津大学 自动化 仪表 专业 . 现为 该校 教授 和 美国纽约 科学院 成员 及   IEEE   成员 ， 长期 从事 系统 辨识 、 系统 建模 理论 及 应用 的 教学科研 工作 . 近年来 在 国内外 发表 论文 60 余篇 . 近期 研究 方向 为 神经网络 理论 及 应用 、 人工智能 等 . 
 　 　 　 　 　 林 　 晨 　 1969 年生 ， 1991 年 毕业 于 天津大学 ， 1994 年于 天津大学 系统工程 研究所 获工学 硕士学位 . 现 任职 于 福建 亚洲 银行 . 研究 兴趣 为 神经网络 、 系统 辨识 和 控制 、 经济 金融 等 领域 . 
 作者 单位 ： 王正欧 ； 天津大学 系统工程 研究所 　 天津 　 300072 
 　 　 　 　 　 林 　 晨 ： 福建 亚洲 银行 　 福州 　 350001 
 参考文献 
 ［ 1 ］ 　 Rumelhart   D   E ， Hinon   D   E , Williams   R   J . Learning   internal   representations   by   error   propagation , parallel   distributed   processing : Explorations   in   the   microstructure   of   cognition , Cambridge   MA : MIT   Press , 1986 , 318 — 362 . 
 [ 2 ] 　 Narendra   K   S , Parthasarathy   K . Identification   and   control   of   dynamical   systems   using   neural   networks . IEEE   Trans .   Neural   Networks , 1990 , 1 ( 1 ) : 4 — 27 . 
 [ 3 ] 　 Chen   S , Billings   S   A , Grant   P   M . Non - linear   system   identification   using   neural   networks . Int . J . Control ,   1990 , 51 ( 6 ) : 1191 — 1214 . 
 [ 4 ] 　 Singhal   S , Wu   L . Training   feed - forward   networks   with   the   extended   Kalman   algorithm . In : Proc . of   the   IEEE   Int . Conf . on   Acoustics , Speech   and   Singal   Processing , Glasgow , 1989 , 1187 — 1190 . 
 [ 5 ] 　 Kollias   S , Amastassion   D . An   adaptive   least   squares   algorithm   for   the   efficient   training   of   artificial   neural   networks . IEEE   Trans . Circuits   and   Systems , 1990 , 36 : 1092 — 1101 . 
 [ 6 ] 　 Shah   S , Palmieri   F , Datum   M . Optimal   filtering   algorithms   for   fast   learning   in   feed - forward   neural   networks . Neural   Networks , 1992 , 5 : 779 — 787 . 
 收稿 日期 　 1995 - 03 - 13 
 附 　 录 
 算法 ( 14 ) ， ( 15 ) 的 推导 
 　 　 从 ( 12 ) 式 有 
 　 　 　 　 　 　 　 　 ( A1 ) 
 同理 可 得 
 　 　 　 　 　 　 　 ( A2 ) 
 假定 下面 的 等式 成立 
 　 　 　 　 　 　 　 　 　 　 　 ( A3 ) 
 这里 h ( k ) 有 与 gi ( k ) 相同 的 维数 . 需要 指出 ， 严格 地说 ， ( A3 ) 式 不能 精确 成立 ， 此处 仅为 推导 方便 起 见 ； 后面 ， h ( k ) hT ( k ) 仍 将 用 ∑ gi ( k ) gTi ( k ) 替代 ， 故 不会 出现 误差 . 应用 矩阵 求逆 公式 
 A = B - 1 + CCT , 　 A - 1 = B + BC ( I + CTBC ) - 1CTB , 
 并设 B = S - 1 ( k - 1 ) λ - 1 及 C = h ( k ) . 由 ( A1 ) 和 ( A3 ) 式 可得 
 S - 1 ( k ) = λ - 1S - 1 ( k - 1 ) - λ - 2S - 1 ( k - 1 ) h ( k ) hT ( K ) × S - 1 ( k - 1 ) [ 1 + λ - 1hT ( k ) S - 1 ( k - 1 ) h ( k ) ] - 1 . 　 ( A4 ) 
 定义 P ( k ) = S - 1 ( k ) ， 则 ( A4 ) 式 可 写 为 
 P ( k ) = λ - 1P ( k - 1 ) - λ - 2P ( k - 1 ) h ( k ) hT ( k ) P ( K - 1 ) × [ 1 + λ - 1hT ( k ) P ( k - 1 ) h ( k ) ] - 1 . 　 ( A5 ) 
 由 ( A3 ) 式 知 
 　 　 　 　 　 　 　 　 　 　 　 　 ( A6 ) 
 必 成立 ， 即 h ( k ) hT ( k ) 的 对角 元素 之 和 等于 所有 构成 h ( k ) hT ( k ) 的 矩阵 的 对角 元素 之 和 ， 因此 有 
 　 　 　 　 　 　 　 　 ( A7 ) 
 由 ( A3 ) ， ( A5 ) 及 ( A7 ) 式 可得 
 　 　 　 　 ( A8 ) 
 定义 
 a ( k ) = λ - 1P ( k - 1 ) h ( k ) [ 1 + λ - 1hT ( k ) P ( k - 1 ) h ( k ) ] - 1 . 　 　 　 　 　 　 　 　 ( A9 ) 
 由 ( A5 ) 式 得 
 P ( k ) = λ - 1P ( k - 1 ) - λ - 1a ( k ) hT ( k ) P ( k - 1 ) , 　 　 　 　 　 　 　 　 　 　 　 ( A10 ) 
 则 ( A9 ) 式 可 写 为 
 a ( k ) + λ - 1a ( k ) hT ( k ) P ( k - 1 ) h ( k ) = λ - 1P ( k - 1 ) h ( k ) 　 　 　 　 　 　 　 　 ( A11 ) 
 或 　 　 　 　 　 　 a ( k ) = [ λ - 1P ( k - 1 ) - λ - 1a ( k ) hT ( k ) P ( k - 1 ) ] h ( k ) = P ( k ) h ( k ) . 　 　 　 　 　 　 　 　 ( A12 ) 
 另外 ， ( 11 ) 式 中 的 参数估计 ( k ) 可写 为 
 　 　 ( A13 ) 
 把 ( A10 ) 式 代入 ( A13 ) 式 得 
 　 　 　 　 　 ( A14 ) 
 由 ( 11 ) ， ( A12 ) 和 ( A14 ) 式 可得 
 　 ( A15 ) 
 略去 高阶 项 ρ i ( k ) , i = 1 , … , m , 即得 
 　 　 　 　 ( A16 ) 
 从 上面 的 推导 中 已 得到 了 ( 14 ) 和 ( 15 ) 式 . 
