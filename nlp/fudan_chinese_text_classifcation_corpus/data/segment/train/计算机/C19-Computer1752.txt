软件 学报 
 JOURNAL   OF   SOFTWARE 
 1999 年   第 19 卷   第 3 期     Vol.19   No.3   1999 
 
 
 
 语言 模型 中 一种 改进 的 最大 熵 方法 及其 应用 
 李 涓 子 　 黄 昌宁 
 摘 　 要 　 最大 熵 方法 是 建立 统计 语言 模型 的 一种 有效 的 方法 , 具有 较强 的 知识 表达能力 . 但是 , 在 用 现有 的 最大 熵 方法 建立 统计 模型 时 存在 计算 量 大 的 问题 . 针对 这一 问题 , 提出 了 一种 改进 的 最大 熵 方法 . 该 方法 使用 互信息 的 概念 , 通过 Z - 测试 进行 特征选择 . 将 该 方法 应用 于 汉语 的 义项 排歧 中 , 实验 表明 ， 该 算法 具有 较 高 的 计算 效率 和 正确率 . 
 关键词 　 语言 模型 , 最大 熵 模型 , 参数估计 , 特征选择 , 互信息 , Z - 测试 . 
 中图法 分类号 　 TP18 
 An   Improved   Maximum   Entropy   Language   Model   and   Its   Application 
 LI   Juan - zi   HUANG   Chang - ning 
 ( Department   of   Computer   Science   and   Technology   Tsinghua   University   Beijing   100084 ) 
 ( State   Key   Laboratory   of   Intelligent   Technology   and   Systems   Tsinghua   University   Beijing   100084 ) 
 Abstract 　   The   maximum   entropy   approach   is   proved   to   be   expressive   and   effective   for   the   statistics   language   modeling ,   but   it   suffers   from   the   computational   expensiveness   of   the   model   building .   An   improved   maximum   entropy   approach   which   makes   use   of   mutual   information   of   information   theory   to   select   features   based   on   Z - test   is   proposed .   The   approach   is   applied   to   Chinese   word   sense   disambiguation .   The   experiments   show   that   it   has   higher   efficiency   and   precision . 
 Key   words 　 Language   model ,   maximum   entropy   model ,   parameter   estimation ,   feature   selection ,   mutual   information ,   Z - test . 
 　 　 语言 模型 试图 反映 、 记录 并 使用 自然语言 中 存在 的 规律 ［ 1 ］ . 近几年 在 自然语言 处理 的 研究 过程 中 发现 , 最大 熵 方法 是 一种 建立 统计 语言 模型 的 有效 方法 , 具有 较强 的 知识 表达能力 . 但是 , 在 用 现有 的 最大 熵 方法 建立 统计 模型 时 存在 计算 量过大 的 问题 ［ 2 ］ . 本文 针对 这一 问题 , 提出 了 一种 新 的 特征选择 算法 . 算法 使用 互信息 的 概念 , 通过 Z - 测试 的 方法 进行 特征选择 . 在 特征选择 过程 中 , 用 新建 模型 与 引用 模型 的 Kullback - Leibler 距离 来 调整 所 选出 的 特征 . 实验 表明 , 这种 算法 具有 较 高 的 计算 效率 . 
 　 　 本文 首先 叙述 最大 熵 原理 , 介绍 已有 的 参数估计 和 特征选择 方法 , 并 对 其 进行 评价 ; 然后 给出 使用 Z - 测试 的 特征选择 算法 , 最后 将 这种 改进 的 最大 熵 方法 应用 于 汉语 的 义类 排歧 中 . 
 1   最大 熵 原理 
 　 　 最大 熵 原理 最初 是 由 E . T . Jayness 在 1950 年 提出 的 , Della   Pietra 等 人于 1992 年 首次 将 它 应用 于 自然语言 处理 的 语言 模型 建立 中 ［ 1 ］ . 本文 只是 简单 介绍 最大 熵 原理 , 更 详细 的 叙述 请 见 参考文献 ［ 3 , 4 ］ . 
 　 　 直觉 上 讲 , 最大 熵 原理 的 基本 思想 是 : 给定 训练 数据 即 训练样本 , 选择 一个 与 所有 的 训练 数据 一致 的 模型 . 比如 在 英语 中 , 对于 一个 具有 词性 歧义 的 词条 , 如果 发现 一个 名词 前 为 一个 冠词 的 概率 为 50% , 而 在 名词 前 为 一个 形容词 的 概率 为 30% , 则 最大 熵 模型 应 选择 与 这些 观察 一致 的 概率分布 . 而 对于 除此之外 的 情况 , 模型 赋予 的 概率分布 为 均匀分布 . 
 1.1   问题 描述 
 　 　 设 随机 过程 P 所有 的 输出 值 构成 有限 集 Y , 对于 每个 输出 y ∈ Y , 其 生成 均 受 上下文 信息 x 的 影响 . 已知 与 y 有关 的 所有 上下文 信息 组成 的 集合 为 X , 则 模型 的 目标 是 : 给定 上下文 x ∈ X , 计算 输出 为 y ∈ Y 的 条件 概率 , 即 对 p ( y ｜ x ) 进行 估计 . p ( y ｜ x ) 表示 在 上下文 为 x 时 , 模型 输出 为 y 的 条件 概率 , 其中 y ∈ Y 且 x ∈ X . 如 : 对于 义类 歧义 问题 , 集合 Y 是 具有 义类 歧义 的 某 一词 W 的 所有 可能 义类 组成 的 集合 , 集合 X 为 对 词 W 的 每次 出现 , 为 其 选定 的 上下文 环境 所 组成 的 集合 . 
 1.2   训练 数据 
 　 　 模型 输入 是 经过 人工 排歧 或 从 已 标注 过 的 语料库 中 抽取 出 的 大量 ( x , y ) 训练样本 , 即 对 在 语料库 中有 歧义 的 对象 的 每次 出现 , 都 已有 确定 的 取值 y 及其 对应 的 上下文 环境 x . 可以 用 概率分布 的 极大 似然 对 训练样本 进行 表示 . 即 
 ( 1 ) 
 其中 freq ( x , y ) 是 ( x , y ) 在 样本 中 出现 的 次数 . 
 1.3   特征 、 特征函数 及 约束 
 　 　 由 问题 描述 可知 , 随机 过程 P 与 上下文 信息 x 有关 , 但 如果 考虑 所有 与 y 同现 的 上下文 信息 , 建立 的 模型 会 很 繁琐 , 而且 从 语言 知识 上 来讲 , y 的 生成 只 与其 上下文 中 的 部分 信息 有关 . 因此 , 从 x 中 找出 对 y 的 取值 有用 的 知识 才 是 模型 所 追求 的 目标 . 而 这些 有用 的 知识 正是 最大 熵 模型 所 要 寻找 的 特征 . 
 　 　 定义 1 .   特征 
 　 　 设 x ∈ X 且 x = w1w2 ... wn , 而 c 是 x 的 一个 子串 （ 长度 ≥ 1 ） , 若 c 对 y ∈ Y 具有 表征 作用 , 则 称 （ c , y   ） 为 模型 的 一个 特征 . 
 　 　 特征 分为 原子 特征 和 复合 特征 . 若串 c 的 长度 为 1 , 则 称 （ c , y ） 为 原子 特征 , 否则 , 称 （ c , y   ） 为 复合 特征 . 
 　 　 定义 2 .   特征函数 
 　 　 特征函数 是 一个 二值 表征 函数 , 表示 （ x ′ , y ′ ) 是否 与 特征 （ c , y ） 有关 , 定义 （ x ′ , y ′ ) 关于 特征 （ c , y ） 的 特征函数 为 
 （ 2 ） 
 　 　 由 以上 定义 可以 看出 , 样本 中 出现 在 歧义 对象 周围 的 所有 的 词 和 该 对象 的 确定 值 一起 都 可以 作为 模型 的 特征 , 因此 , 与 模型 有关 的 候选 特征 组成 的 集合 会 很大 . 但 模型 选出 的 特征 只是 真正 对模型 有用 的 特征 , 是 候选 特征 集合 的 一个 子集 , 它 能 较完整 地 表达 训练 语料 中 的 数据 . 由此 引入 约束 . 
 　 　 定义 3 .   约束 
 　 　 设 ( f ) 为 特征 f 对于 经验 概率分布 ( x , y ) 的 数学 期望 , 表示 为 
 , （ 3 ） 
 p ( f ) 为 特征 f 对于 由 模型 确定 的 概率 p ( x , y ) 的 数学 期望 , 表示 为 
 , （ 4 ） 
 而 p ( x , y ) = p ( x ) p ( y ｜ x ) , 令 p ( x ) = ( x ) , 则 限定 所求 模型 的 概率 为 在 样本 中 观察 到 事件 的 概率 , 而 不是 所有 可能 出现 的 事件 的 概率 . 若 f 对模型 有用 , 则 令 
 p ( f ) = ( f ) , 　 　 （ 5 ） 
 称式 ( 5 ) 为 约束 . 
 1.4   最大 熵 原理 
 　 　 假设 存在 n 个 特征 fi （ i = 1 , 2 , ... , n ） , 则 模型 属于 约束 所 产生 的 模型 集合 , 即 
 C = { p ∈ P ｜ p ( fi ) = ( fi ) , i ∈ { 1 , 2 , ... , n } } , （ 6 ） 
 而 满足 约束条件 的 模型 有 很多 , 模型 的 目标 是 产生 在 约束 集下 具有 最 均匀分布 的 模型 , 而 条件 概率 p ( y ｜ x ) 均匀 性 的 一种 数学 测量方法 为 条件 熵 , 定义 为 
 , （ 7 ） 
 其中 0 ≤ H ( p ) ≤ log ｜ y ｜ . 
 　 　 最大 熵 原理 .   若 在 允许 的 概率分布 C 中 选择 模型 , 具有 最大 熵 的 模型 px ∈ C 即为 所选 模型 . 即 
 . （ 8 ） 
 2   参数估计 及 特征选择 
 　 　 利用 最大 熵 建立 语言 模型 的 过程 分为 两步 : 特征选择 和 参数估计 . 特征选择 的 任务 是 选出 对模型 有 表征 意义 的 特征 ; 参数估计 用 最大 熵 原理 对 每 一个 特征 进行 参数 估值 , 使 每 一个 参数 与 一个 特征 相对 应 , 以此 建立 所求 模型 . 
 2.1   参数估计 
 　 　 Danroch 和 Ratcliff 于 1972 年 提出 一个 称为 GIS （ generalized   iterative   scaling   algorithm ） 的 算法 , 该 算法 是 一般 的 迭代 算法 . Della   Pietra 等 人于 1995 年 根据 所 处理 的 问题 对 算法 作 了 进一步 改进 , 提出 了 IIS （ improved   iterative   scaling   algorithm ） 算法 , 算法 设 满足 最大 熵 条件 的 概率 p ( x , y ) 具有 Gibbs 分布 的 形式 
 ， （ 9 ） 
 其中 
 , （ 10 ） 
 Z λ ( x ) 为 归 一 常量 , 保证 对 所有 x , . 
 2.2   特征选择 
 　 　 无论 GIS 还是 IIS 的 参数估计 方法 提供 的 均 是 求解 λ 值 的 方法 , 保证 以 λ 建立 的 模型 不 含有 任何 额外 的 假设 . 这 两个 算法 并 不能 保证 模型 所 含 特征 是 具有 良好 表征 意义 的 特征 , 因此 , 在 建立 模型 中 十分 重要 的 一部分 工作 是 特征选择 . Della   Pietra 等 人 提出 的 原子 特征 算法 思想 是 : 开始 设 特征 集 S 为空 , 此后 不断 向 S 中 增加 特征 , 每次 增加 的 特征 由 训练 数据 决定 . 以 训练 数据 的 对数 似然 作为 特征选择 的 依据 , 即 若 S 为 已 选中 的 特征 集 , f ～ 为 候选 特征 , 用 L ( ps ) 表示 由 S 决定 的 模型 的 对数 似然 , 则 每次 选出 的 f ～ 应该 为 使 公式 
 Δ L ( s , ) ≡ L ( ps ∪ ) - L ( Ps ) , （ 11 ） 
 增加 最多 的 特征 . 其中 
 . （ 12 ） 
 　 　 该 算法 的 致命 弱点 是 计算 量 大 , 每选 一个 特征 都 需要 对 所有 的 候选 特征 调用 IIS 算法 , 对 λ 重新 计算 , 并且 要 对 训练 数据 的 对数 似然 进行 计算 , 然后 选出 一个 使 模型 的 对数 似然 增加 最多 的 特征 , 这 几乎 是 不可 操作 的 . 为了 使 特征选择 过程 可行 , Della   Pietra 等 人 又 给出 了 一系列 优化 算法 , 如 在 向 模型 中 加入 一个 新 的 特征 时 , 保持 前面 IIS 过程 估计 的 λ 值 不变 , 只用 IIS 计算 新 加入 特征 的 对应 值 , 当 所有 特征 选出 后 , 重新 调用 一次 IIS 过程 , 对 所有 λ 进行 一次 重新 计算 , 这种 方法 虽然 可以 加快 特征选择 的 过程 , 但 不能 保证 每次 加入 模型 的 特征 是 最好 的 . 
 3   使用 Z - 测试 的 特征选择 算法 
 　 　 建立 最大 熵 模型 的 关键 是 要 选出 具有 预期 作用 的 特征 , 只有 这样 才能 保证 得到 的 解是 对模型 最 有用 的 解 . 虽然 Della   Pietra 等 人 的 原子 特征选择 方法 , 可以 选出 最好 的 有 预期 作用 的 特征 , 但 这种 方法 完全 建立 在 数学 运算 的 基础 之上 , 存在 着 计算 量 大 的 问题 . 
 既然 特征选择 的 目的 是 要 选出 对模型 具有 预期 作用 的 上下文 信息 , 则 这个 信息 与 所要 预期 的 值 具有 较 密切 的 搭配 关系 . 本文 正是 从 这 一 假设 出发 , 提出 一种 使用 互信息 概念 , 采用 Z - 测试 的 方法 来 进行 特征选择 的 算法 . 
 3.1   原子 特征选择 的 问题 描述 
 　 　 已知 训练 数据 中 的 N 个 训练样本 ( x1 , y1 ) , ( x2 , y2 ) , ... , ( xN , yN ) , 其中 xi ∈ X 且 yi ∈ Y , 设 A = ｛ aj ｜ aj 是 xi 的 子串 , 且 aj 的 长度 为 1 , i = 1 , 2 , ... , N ｝ , 则 原子 特征选择 的 意义 是从 A 中 选出 能够 充分 表征 Y 的 不同 取值 的 最小 特征 集 F = ｛ f1 , f2 , ... , fn ｝ , 其中 fi = ( ai , yi ) （ ai ∈ A , yi ∈ Y , i = 1 , 2 , ... , n ） 为 原子 特征 . 
 　 　 在 此 , 我们 采用 Kullback - Leibler 距离 来 测定 特征 所 确定 模型 的 质量 . 设是 由 训练 语料 确定 的 概率模型 , p 为 由 特征 集 确定 的 模型 , 则 Kullback - Leibler 距离 定义 为 
   . ( 13 ) 
 最终 要 找 的 模型 p 为 
 . ( 14 ) 
 3.2   利用 Z - 测试 进行 原子 特征选择 的 依据 
 　 　 ( 1 )   互信息 ［ 5 ］ 可 衡量 搭配 的 强度 
 特征选择 的 目的 是 要 选出 对模型 具有 预期 作用 的 上下文 信息 , 所以 这个 信息 应 与 所 预期 的 值 具有 较 密切 的 搭配 关系 , 而 信息论 中 的 互信息 正是 测量 搭配 强度 的 一个 物理量 . 对应 于 我们 要 解决 的 问题 为 : 若 某 一 上下文 信息 对 y 有 表征 意义 , 则 y 与 该 上下文 的 互信息 较大 . 
 　 　 ( 2 )   Z - 测试 ［ 6 ］ 可 作为 互信息 的 一个 测度 
 虽然 互信息 可以 作为 描述 搭配 强度 的 物理量 , 但是 , 如果 特征选择 直接 确定 选择 互信息 大于 某一 阈值 的 上下文 信息 为 特征 时 , 则 对 不同 互信息 的 分布 , 设定 的 阈值 也 不 相同 , 这样 , 算法 难以 操作 . 而 Z - 测试 可以 将 互信息 的 分布 进行 标准 变换 , 将 其 变为 标准 的 正态分布 , 这样 , 不论 互信息 如何 分布 , 都 可以 从 一个 统一 的 阈值 开始 进行 求解 . 
 3.3   原子 特征选择 算法 
 　 　 输入 : 训练样本 （ x1 , y1 ） , （ x2 , y2 ） , ... , （ xN , yN ） ; 
 　 　 输出 : 特征 矩阵 Dm × n . 其中 m = ｜ Y ｜ , 即 y 的 所有 可能 取值 的 个数 ; n = ｜ ｛ aj ｜ aj 是 xi 的 子串 , 且 aj 的 长度 为 1 , i = 1 , 2 , ... , N ｝ ｜ , 即 与 y 的 不同 值 同现 的 候选 特征 集合 中 的 元素 个数 . 
   . 
 　 　 过程 
 　 　 步骤 1 . 
 　 　 . 由 样本 （ x1 , y1 ） , （ x2 , y2 ） , ... , （ xN , yN ） 得到 Y 中 元素 与 A 中 元素 的 同现 次数 矩阵 Fm × n , 其中 
 　 　 fij = f ( yi , aj ) = ｜ { ( xk , yk ) ｜ aj 是 xs 的 子串 , 且 长度 为 1 , yk = yi , s = 1 , 2 , ... , N } ｜ . 
 　 　 . 计算 互信息 矩阵 Im × n , 其中 
   , 
 其中 M 为 语料库 的 大小 , 而 f ( yi ) 和 f ( aj ) 分别 为 yi 和 aj 在 语料库 中 出现 的 次数 . 由 互信息 的 定义 可知 : 当 Iij  0 时 , yi 与 aj 完全 并列 , 因此 , （ aj , yi ） 可 作为 模型 的 特征 . 
 　 　 步骤 2 . 
 　 　 对 每个 yi , 
 　 　 . 计算 yi 的 互信息 均值 
 ; ( 15 ) 
 　 　 . 计算 其均 方差 
 . ( 16 ) 
 　 　 步骤 3 . 
 　 　 用 Z - 测试 对 每个 yi 生成 表征 向量 ( di1 , di2 , ... , dim ) ; 
 　 　 . 对 每个 yi （ i = 1 , 2 , ... , n   ） , 
 　 　 . 对 每个 aj , 计算 
   , ( 17 ) 
 　 　 *   若 zij ＞ T , 则 aj 与 yi 为 选出 的 一个 特征 , 令 dij = 1 ; 否则 dij = 0 ; 
 　 　 步骤 4 . 
 　 　 . 用 选出 的 原子 特征 集合 S = { f1 , f2 , ... , fk } 调用 IIS 算法 , 得到 〈 Z , λ 1 , λ 2 , ... , λ k 〉 ; 
 　 　 . 用 公式 ( 9 ) 和 ( 10 ) 计算 p ( y ｜ x ) . 
 　 　 步骤 5 . 
 　 　 . 计算 由 p ( y ｜ x ) 确定 的 模型 与 经验 概率分布 模型 ( y ｜ x ) 的 距离 D ( p ‖ ) ; 
 　 　 . 用 D ( p ‖ ) 与 上次 的 D ′ ( p ‖ ) 比较 ; 若 D - D ′ ＜ ε , 则 过程 结束 ; 否则 , T = T - Δ T , 转 步骤 3 . 
 3.4   阈值 T 的 确定 
 　 　 ( 1 )   T 初值 的 确定 
 　 　 从 算法 可以 看出 , 在 经过 式 ( 17 ) 的 运算 后 , 已 将 互信息 的 分布 变为 正态分布 . 从 概率论 可知 : 正态分布 在 区间 ［ - 3 , + 3 ］ 内 , 其 整个 概率 覆盖度 可达 99% 左右 . 因此 , T 可以 在 ［ - 3 , + 3 ］ 内 进行 取值 . 因为 开始 时要 选出 表征 意义 大 的 特征 , 所以 应 赋予 T 一个 较大 的 初值 . 
 　 　 ( 2 )   T 阈值 的 变化 
 　 　 初值 确定 后 , 以后 每次 以 一个 步长 Δ T 减少 , 这 就 意味着 每次 根据 T 选中 的 特征 不是 一个 , 而是 具有 同等 表达 程度 的 一个 候选 特征 子集 , 且 选出 的 子 集中 包含 上 一次 选出 的 特征 集合 . 因此 , 在 进行 下 一次 的 参数估计 时 , 对于 以前 的 特征 其 初值 可以 从 上次 确定 的 值 开始 , 这样 做 可以 节省 大量 运算 时间 . 特征选择 过程 最终 得到 的 特征 集合 是 它 所 确定 的 模型 的 D ( p ‖ ) 较小且 具有 较 一般 表征 意义 的 集合 . 
 3.5   两个 原子 特征选择 算法 的 计算 量 比较 
 　 　 ( 1 )   Della   Pietra 的 特征选择 算法 的 计算 量分析 
 该 特征选择 算法 每次 确定 一个 特征 时 的 计算 量 由 两 部分 组成 , 即 调用 IIS 对 每 一 候选 特征 进行 参数估计 和 计算 模型 的 对数 似然 . 总 的 计算 量 可 表示 为 
 C1 = n * ( IIS1 + L1 ) + ( n - 1 ) * ( IIS2 + L2 ) +...+ ( n - k ) * ( IISk + Lk ) . （ 18 ） 
 其中 n 为 候选 特征 集合 中 候选 特征 的 个数 , k 为 最终 特征 集合 中 的 特征 个数 , IISi 和 Li 分别 表示 在 选 第 i 个 特征 时 参数估计 的 计算 量 和 对数 似然 的 计算 量 . 
 　 　 设 IIS  min 为 在 k 次 特征选择 过程 中 , 参数估计 过程 所 需 的 最少 时间 , 则 
 C1 ≥ n * ( IIS  min + L1 ) + ( n - 1 ) * ( IIS  min + L2 ) +...+ ( n - k ) * ( IISmin + Lmin ) . 
 由 公式 ( 12 ) 可知 , L1 = L2 = ... = Lk , 则 
 C1 ≥ n * ( IISmin + L1 ) + ( n - 1 ) * ( IISmin + L1 ) +...+ ( n - k ) * ( IISmin + L1 ) . 
 =   . （ 19 ） 
 　 　 ( 2 )   本文 提出 的 特征选择 算法 的 计算 量分析 
 　 　 本文 提出 的 特征选择 算法 每次 入选 的 特征 有 多个 , 整个 过程 的 计算 量 由 3 部分 组成 , 即 互信息 的 计算 量 、 参数估计 的 计算 量 及 Kullback - Leibler 距离 的 计算 量 . 总 的 计算 量 为 
 C2 = O ( m * n ) + ( IISa1 + Da2 ) + ( IISa2 + Da2 ) +...+ ( IISai + Dai ) . （ 20 ） 
 其中 O ( m * n ) 为 互信息 的 计算 量 , ai 为 第 i 次 选中 的 特征 个数 , IISai 和 Dai 分别 表示 在 第 i 次 特征选择 时 参数估计 的 计算 量 和 Kullback - Leibler 距离 的 计算 量 . 
 　 　 设 IISmax 为 在 i 次 特征选择 中 参数估计 时间 的 最 大量 , 则 
 C2 ≤ O ( m * n ) + ( IISmax + Da1 ) + ( IISmax + Da2 ) +...+ ( IISmax + Dai ) . 
 由 公式 ( 13 ) 可知 , Da1 = Da2 = ... = Dai , 则 
 C2 ≤ O ( m * n ) + i . IISmax + i . Da1 . ( 21 ) 
 　 　 由 算法 可知 , m 《 n , i ＜ k 及 k 《 n , IISmin 与 IISmax 的 计算 复杂度 属于 同一 数量级 , 而 对数 似然 的 计算 复杂度 与 Kullback - Leibler 距离 的 计算 量 大致相同 , 所以 , 本文 提出 的 特征选择 算法 所 需 的 运算量 小于 Della   Pietra 等 人 提出 的 特征选择 算法 所 需 的 运算量 . 
 4   改进 最大 熵 模型 的 应用 及 实验 结果 
 4.1   基于 最大 熵 原理 的 义类 排歧 
 　 　 作者 将 上面 描述 的 建立 最大 熵 模型 的 方法 应用 于 解决 汉语 文本 中 的 义类 排歧 问题 . 
 　 　 模型 输入 :   已知 多义词 w 的 由 N 个 样本 组成 的 样本空间 : ( x1 , y1 ) , ( x2 , y2 ) , ... , ( xn , yn ) , ( xi , yi ) 表示 当 上下文 信息 为 xi 时 , w 的 义类 为 yi . xi 为 yi 的 上下文 环境 . 模型 的 目标 是 利用 最大 熵 原理 建立 学习 模型 p ( y ｜ x ) , 其 含义 为 在 上下文 为 x 时 输出 义类 为 y 的 概率 . 
 　 　 模型 输出 :   特征 集及 对应 参数 集 , 即 〈 S , λ 〉 ; 其中 S = { f1 , f2 , ... , fn } 且 λ = { λ 1 , λ 2 , ... , λ n } . 
 4.2   实验 过程 
 　 　 ( 1 )   语料库 和 义类 词典 
 在 实验 中 , 将 2   000 万字 已经 进行 了 词 切分 和 词性 标注 的 《 人民日报 》 语料库 作为 系统 的 数据 来源 , 以 在 语料库 中常 出现 的 词性 为 动词 或 名词 的 高频 多义词 作为 义类 排歧 的 对象 . 在 对 多义词 进行 语义 标注 时 , 采用 的 是 《 同义词 词林 》 中 的 义类 代码 , 代码 由大 、 中 、 小 三级 组成 , 如 “ 建 ” 有 两个 义类 “ Hc05 ” 和 “ Hd01 ” , 它们 分别 代表 两种 不同 的 意义 . 
 　 　 ( 2 )   样本 数据 
 根据 最大 熵 模型 , 义类 排歧 模型 的 样本空间 为 从 语料库 中 抽出 包含 某个 多义词 的 词 及其 周围 的 上下文 环境 , 然后 对 每个 样本 进行 人工 排歧 , 形成 样本空间 . 在 样本 数据 的 准备 过程 中 , 我们 做 了 两 方面 的 工作 . ( 1 ) 确定 义类 排歧 的 对象 为 同一 词性 内 的 义类 歧义 词 ; ( 2 ) 对 多义词 周围 所取 的 上下文 的 长度 的 原则 定为 : 以 句子 为 单位 , 在 一句 中选 该词 周围 前后 各 7 个 词 . 以 “ 打 ” 、 “ 建 ” 和 “ 获 ” 这 3 个 具有 义类 歧义 的 词 作为 实验 对象 , 它们 在 语料库 中 出现 的 次数 及 候选 特征 的 个数 见表 1 . 
 　 　 ( 3 )   特征选择 和 参数估计 
 按 本文 提出 的 特征选择 算法 最终 产生 的 特征 个数 和 T 的 值 见表 2 . 
 表 1   　 　 　 　 　 　 　 　 　 　 　 　 表 2 
 
 义类 歧义 词 样本 个数 候选 特征 个数 义类 歧义 词 最后 T 的 值 特征 个数 
 打 1   6421   145 打 1.117377 
 建 1   9283   029 建 1.216402 
 获 2   6823   766 获 1.343285 
 
 该 模型 产生 的 有关 “ 打 ” 的 部分 特征 见表 3 . 
 表 3 
 
 y = “ Fa10 ” 且 “ 井 ” ∈ x 
 y = “ Fa10 ” 且 “ 深 ” ∈ x 
 y = “ Fa10 ” 且 “ 扩孔 ” ∈ x 
 y = “ Hi44 ” 且 “ 死 ” ∈ x 
 y = “ Hi44 ” 且 “ 士兵 ” ∈ x 
 y = “ Hi44 ” 且 “ 致残 ” ∈ x 
 
 　 　 ( 4 )   用 最大 熵 模型 进行 义类 排歧 的 过程 及 结果 
 　 　 具体 过程 为 : 
 　 　 .   找出 含有 指定 多义词 w 的 上下文 （ x , w ） , 其中 x 为 多义词 w 的 上下文 环境 ; 
 　 　 .   根据 模型 学习 到 的 关于 w 的 参数 集 〈 S , λ 〉 , 用 公式 （ 9 ） 和 （ 10 ） 计算 w 的 各个 义类 在 x 下 的 条件 概率 p ( yi ｜ x ) , 其中 yi ∈ Y . 
 　 　 .   取 条件 概率 较大 者 对应 的 义类 为 所 选义类 . 
 在 此 , 我们 分别 采用 封闭 测试 和 开放 测试 两种 方法 对模型 进行 测试 , 测试 正确率 定义 为 
   . 
 得到 的 测试 结果 见表 4 . 从表中 数据 可以 看出 , 用 本文 提出 的 特征选择 算法 建立 的 最大 熵 模型 可以 保证 有 较 高 的 排歧 准确率 . 
 表 4 
 
 　 封闭 测试 开放 测试 
 样本 个数 正确率 ( % ) 样本 个数 正确率 ( % ) 
 打 10089.55083 . 7 
 建 10093.25090 . 6 
 获 10091.85089 . 1 
 
 5   结束语 
 　 　 本文 提出 一种 改进 的 最大 熵 方法 , 该 方法 利用 互信息 的 概念 , 使用 Z - 测试方法 进行 特征选择 , 并 以 建立 模型 与 经验 模型 的 Kullback - Leibler 距离 作为 过程 的 结束 条件 , 因此 , 可以 保证 模型 的 准确性 . 将 模型 用于 汉语 的 义类 排歧 中 , 取得 了 较 高 的 排歧 正确率 . 本文 提出 的 方法 还 可 用于 词性 标注 、 句子 边界 识别 等 问题 . 
 致谢 　 本文 的 研究 得到 国家自然科学基金 资助 ， 此 项目编号 为 69433010 . 
 　 　 本文 研究 得到 国家自然科学基金 重点项目 资助 . 作者 李 涓 子 , 女 , 1964 年生 , 博士生 , 主要 研究 领域 为 计算机 语言学 . 黄 昌宁 , 1937 年生 , 教授 ， 博士生 导师 , 主要 研究 领域 为 计算 语言学 , 人工智能 . 
 　 　 本文 通讯联系 人 : 李 涓 子 ， 北京 100084 , 清华大学 计算机科学 与 技术 系 
 作者 单位 ： 清华大学 计算机科学 与 技术 系   北京 　 100084 
 　 　 　 　 　 清华大学 智能 技术 与 系统 国家 重点 实验室   北京   100084 
 　 　 　 　 　 E - mail :   ljz @ s1000e . cs . tsinghua . edu . cn 
 参考文献 
 ［ 1 ］ Ronnald   Rosenfeld .   A   maximum   entropy   to   adaptive   statistical   language   learning .   Computer   Speech   and   Language ,   1996 , 10 ( 3 ) : 187 ～ 228   
 ［ 2 ］ Andrei   Mikheev   et   al .   Collocation   Lattices   and   maximum   entropy   models .   In :   Zhou   Joe   ed .   Proceedings   of   the   5th   Workshop   on   Very   Large   Corpora .   Beijing :   Association   for   Computational   Lingnistics ,   1997 .   216 ～ 230 
 ［ 3 ］ Berger   A   L ,   Della   Pietra   S   et   al .   A   maximum   entropy   approach   to   natural   language   processing .   Computational   Linguistics ,   1996 , 22 ( 1 ) : 40 ～ 72 
 ［ 4 ］ Della   Pietra   S ,   Della   Pietra   V   et   al .   Inducing   features   of   random   fields .   IEEE   Transactions   on   Pattern   Analysis   and   Machine   Intelligent ,   1997 , 19 ( 4 ) , 380 ～ 393 
 ［ 5 ］ Church   K ,   Hanks   P .   Word   association   norms ,   mutual   information ,   and   lexicography .   Computational   Linguistics ,   1990 , 16 ( 1 ) , 22 ～ 29 
 ［ 6 ］ Frank   Smadja .   Retrieving   collocation   from   text :   Xtract .   Computational   Linguistics ,   1993 , 19 ( 1 ) : 143 ～ 175 
 （ 1998 - 03 - 12 收稿 ） 
