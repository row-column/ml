自动化 学报 
 ACTA   AUTOMATICA   SINICA 
 1997 年   第 23 卷   第 1 期   Vol.23   No.1   1997 
 
 
 
 对 多层 前向 神经网络 研究 的 几点 看法 
 阎 平凡 
 摘 　 要 　 从 不同 的 领域 对 多层 前向 网络 的 作用 本质 作 了 分析 ， 对 泛化 能力 、 模型 选择 、 有限 样本量 等 主要 问题 做 了 定性 讨论 ； 对 当前 前向 网络 研究 中 的 一些 问题 提出 了 看法 . 
 关键词 　 神经网络 ， 多层 感知器 ， 统计 建模 . 
 SOME   VIEWS   ON   THE   RESEARCH   OF   MULTILAYER 
 FEEDFORWARD   NEURAL   NETWORKS 
 YAN   PINGFAN 
 ( Dept . of   Automation , Tsinghua   University , Beijing   100084 ) 
 Abstract 　 The   essential   function   of   multilayered   feedforward   nerwork   was   analysed   from   the   point   of   view   of   various   discipline . Generalization   ability , model   selection   and   the   problem   of   limited   sample   size   were   discussed   qualitatively . Some   problems   in   the   research   of   multilayer   feedforward   network   are   pointed   out . 
 Key   word 　 Neural   network , multilayer   perception , statistical   modeling . 
 1 　 引言 
 　 　 文献 ［ 1 ］ 对 当前 国内 人工神经网络 ( ANN ) 研究 的 现状 作 了 分析 ， 作者 对 该文 指出 的 一些 问题 也 有 同感 . 有关 联想 记忆 网络 ， 文 ［ 2 ］ 已 发表 了 一些 很 好 的 看法 ， 这里 着重 讨论 关于 多层 前向 网络 ( 简记 为 MFN ) 研究 中 的 问题 . 国内 在 MFN 方面 的 研究 工作 很多 ， 其中 不少 是 很 好 的 ， 但 也 存在 一些 问题 ， 一是 有些 理论 研究 不够 深入 ， 甚至 没有 抓住 关键问题 ； 二是 有些 应用 没有 说 清楚 为什么 ANN 是 较 合理 的 选择 ， 使人 感到 其 应用 有 一定 盲目性 . 为此 ， 提出 以下 应 注意 的 问题 . 
 　 　 1 ) 从 多方面 深入 理解 MFN 作用 的 本质 ； 
 　 　 2 ) 对 MFN 的 主要 要求 是 什么 ； 
 　 　 3 ) 用 好 MFN 的 关键问题 及 主要 困难 ； 
 　 　 4 ) 对 一些 笼统 的 提法 或 似是而非 的 问题 ， 应 加以分析 . 
 2 　 对 MFN 作用 的 理解 
 　 　 要 理解 MFN 的 作用 不能 只 局限于 对   ANN   的 研究 ， 应 搞清楚 其 与 相关 领域 的 关系 和 差异 . 
 　 　 2.1 　 函数 逼近 
 　 　 用 一些 简单 函数 的 复合 去 逼近 一个 未知 函数   f , 工程 上 常用 的 有 两种 类型 . 一是 谱 方法 ， 二是 有限元法 . 谱 方法 把   f   分解成 不同 频率 的 谐波 成份 ， 基 函数 的 正交 性 显而易见 . 这种 分解 在 频域 的 分辨率 几乎 是 无限 的 ( 单频 的 谐波 在 频域 是 一个   δ 函数 ) ， 有利于 频域 分析 . 但 各基 函数 在 时 ( 空 ) 域 是 全局性 的 ， 时 ( 空 ) 域 分辨率 很 低 . 有限元法 则 是 在 空 ( 时 ) 域 把   f   分成 一些 小 区域 ， 每一 小区 可用 简单 的 函数 表示 ， 其空 ( 时 ) 域 分辨率 高而 频域 分辨率 低 . 
 　 　 从 函数 逼近 角度 可 把 MFN 看作 由 许多 简单 函数 组成 的 复合 函数 ， 或者 看作 函数 的 综合 过程 . 文献 ［ 3 ］ 给出 一个 广义 网络 逼近 定理 . 
 　 　 以下 各类 网络 在 由   d 维 欧氏 空间 紧集 上 的 连续函数 构成 的 空间 上 ， 在 下述 意义 下 是 稠密 的 ， 即 对 任一 上述 函数   f *   都 存在 一个 收敛 于   f *   的 网络函数 序列 fn . 
 　 　 所 列 网络 包括 了 不同 结构 和 不同 节点 函数 的 网络 . 可以 说 任何 一种 复合 函数 都 对应 一种   MFN ， 反之 任一   MFN 也 都 可 写成 复合 函数 的 形式 ， 可 把 网络 看作 逼近 算法 的 图 ( Graph ) 示 形式 . MFN 表示 的 复合 函数 一般 不能 硬性 归属于 前面 说 的 两种 极端 情况 . 基于 记忆 的 映射 网络 如 ： CMAC ［ 4 ］ ， 资源 可 重置 网络 ［ 5 ］ 和 RBF 网络 应 属于 有限元 类 ， 多数 以 指数函数 为 节点 函数 的 网络 ， 则 看作 谱 分解 更 合理 . 
 　 　 许多 工程 问题 ， 如 动态 系统 建模 ， 信号 的 滤波 及 变换 等 ， 都 是 一个 函数 到 另 一 函数 的 映射 ［ 6 ］ ， 文 ［ 7 ］ 对 MFN 的 逼近 能力 作 了 扩展 ， 证明 它 不仅 可 逼近   Rn   上 的 函数 ， 且 可 逼近 函数 簇 ( 泛函 ) ， 文 ［ 8 ］ 给出 了 更 一般 的 多层 ( 网络 ) 泛函 的 概念 ， 可 把 MFN 看作 无限 维 函数 空间 上 的 算子 . 
 　 　 2.2 　 回归 与 分类 
 　 　 由 随机样本 组 ( xi ， yi )   i = 1 , … , n   寻找   y   与   x 间 关系 的 最佳 ( 在 均 方 误差 意义 下 ) 函数   y = f ( x ) , 是 统计学 中 的 一个 古老 分支 — — 回归 分析 的 研究 内容 ， 可以 说 任一 参数 或非 参数 回归 模型 都 对应 一个 MFN ， 任一 MFN 也 可 写出 其 对应 的 回归 模型 . 文 ［ 9 ］ 在 从 统计 观点 研究 ANN 的 学习 时 ， 把 希望 输出 与 观测 输入 的 关系 写成   d = g ( x ) + ε ， g ( x ) = E ［ d ｜ x ］ . 这 就是 回归 模型 . 文 ［ 10 ］ 从 非线性 数据分析 的 观点 讨论 了 MFN   的 作用 ， 证明 当 用作 分类器 时 ， 是 按 后验 概率 划分 的 ， 对此 ， 文 ［ 11 ］ 有 更 详细 的 讨论 . 
 　 　 回归 分析 中有 一 类型 ， 其 形式 与 MFN 更 接近 ， 即 投影 寻踪 回归 ( Projction   Purswit   Regression , PPR ) ［ 12 ］ ， 其 输入   x   与 输入   y   间 关系 可 表示 为 
 
 
 其 大致 过程 是 ， 第一步 ， 先 把 高维 的 输入   x   投影 到 低 维空间   u ， 第二步 ， 由   u   的 非线性 函数 σ 的 线性组合 逼近   y ， 要 学习 的 参数 有 三个 ： β ik , 平滑 函数   σ 的 形式 和 输入 层 数值   wk ， 文 ［ 13 ， 14 ］ 对 PPR 与 MFN 的 学习 作 了 比较 ， 文 ［ 15 ］ 从 PPR 的 观点 讨论 了 Cascade - Correlation 学习 ， 文 ［ 16 ］ 介绍 了 PPR 网络 的 实现 方法 . 
 　 　 3 ) 正规化 理论 ( Regularization   Theory ) 
 　 　 T . Poggio 等 在 文 ［ 17 ］ 中 把 监督 学习 与 逼近 联系 起来 ， 并用 正规化 方法 处理 它们 ， 说明 了 正规化 方法 可 等价 于 一个 只含 一个 隐层 的 MFN ， 称之为 正规化 网络 . 进一步 又 提出 广义 正规化 网络 ， 它 包含 了 很 广 一类 逼近 网络 ， 文 ［ 18 ］ 是 对 正规化 理论 与 ANN 的 全面 总结 . MFN 可 看作 某一 正规化 方法 的 实现 ， 由于 MFN 学习 本身 就是 一个 反 问题 ［ 1 ， 19 ］ ， 它 与 正规化 理论 的 联系 是 显而易见 的 . 
 3 　 对 MFN 的 基本 要求 和 主要 困难 
 　 　 在 有限 数据 ( 样本 ) 上 学习 后 有 推广 ( 泛化 ) 能力 ， 是 对 MFN 的 基本 要求 ( 有关 泛化 问题 作者 已有 讨论 ［ 20 ］ ) . 除此 ， 网络 的 容错性 、 学习 速度 等 有时 也 很 重要 ， 相对 其它 性能 而言 ， 泛化 能力 是 主要 的 . 泛化 能力 与 网络结构 ( 规模 ) 、 给定 样本 数量 与 问题 本身 的 复杂程度 有关 . 问题 的 复杂程度 无法控制 ， 实际 可 存在 两类 问题 ： 
 　 　 1 ) 给定 网络结构 ， 为 达到 好 的 推广 能力 需要 多少 训练样本 . 
 　 　 2 ) 训练样本 量 已定 ， 确定 合理 的 网络结构 以 保证 好 的 推广 能力 . 
 　 　 第二类 问题 实际上 遇到 的 最 多 ， 即 在 样本量 已 定时 ， 如何 根据 对 问题 的 先验 知识 ( 如果 有 的话 ) 来 设计 网络结构 ， 下 一步 是 用 BP 法 学习 参数 ( 权 和 阈值 ) . 前者 是 结构 学习 ， 后者 是 参数 学习 ， 目前 大部分 研究 是 针对 参数 学习 的 ， 应 看到 更 有 决定 意义 的 是 结构设计 ， 目前 对此 问题 尚 没有 理想 的 方法 ， 文 ［ 21 ］ 对此 作 了 初步 探讨 ， 下 一节 将 从 统计 建模 观点 讨论一下 基本 原则 . 
 4 　 统计 建模 与 最 简单 原则 
 　 　 建模 是 一种 对 未知 世界 的 逼近 方法 ， 对模型 的 三个 基本 要求 是 ： Flexibility ( 适用性 ) ， Dimensionality ( 计算 复杂性 ) 和   Interpretability ( 透明性 ) . Flexibility 是 模型 对 不同 问题 提供 准确 逼近 的 能力 ， 不 准确 ( 模型 与 真实 规律 的 偏离 ) 表现 为 估计 结果 的 偏置 ( Bias ) . Dimensionality 是 据 有限 样本 确定 模型 中 的 参数 时 产生 的 不稳定性 ， 表现 为 估计 结果 的 方差 ( Variance ) ， 所谓 “ 维数 灾难 ” 是 指为 把 方差 限制 到 要求 的 范围 内 所 需 数据量 随维数 ( 自变量 数 ) 的 迅速 增长 ( 如 指数 增长 ) . Flexibility   和   Dimensionality 是 一对 矛盾 ， 对此 可 简单 说明 如下 ［ 22 ］ ： 
 　 　 给定 数据 ( xi ， yi ) 后 ， 在 均 方 误差 意义 下   y   对   x   的 回归   E ［ y ｜ x ］ = 是 最好 的 模型 . 因为 
 
 
 　 　 可用 后 一项 ， 即 ( x ) 与   E   ［ y ｜ x ］ 的 接近 程度 作为 ( x ) 好坏 的 度量 ， 经过 推导 可 得 
 
 
 
 　 　 　 　 　 　 　 　 　 　 　 　 偏置 
 　 　 　 　 　 　 　 　 　 　 　 方差 
 　 　 结合   MFN 看 ， 若 网络 过于 简单 ， 则 不足以 反映 未知 规律   y = f ( x )   的 复杂性 ， 因而 无法 准确 逼近   f ( x ) ( 偏置 大 ) . 反之 若 结构 过于 复杂 ， 则 要 由 给定 数据 学习 的 未知 参数 太 多 ， 使 学习 结果 不 稳定 ( 方差 大 ) . 
 　 　 14 世纪 的 法国 修道士 William   Ockham 提出 过 一个 最 简单 ( 最 节省 ) 原则 ： “ 与 已知 事实 符合 ( 一致 ) 的 理论 中 的 最 简单 者 就是 最好 的 理论 ” ， 后 人称 此 原则 是 “ Occam ' s   Razor ” . 文 ［ 23 ］ 从 Bazes   观点 对此 作 了 一个 定量 说明 . 这一 思想 与 爱因斯坦 的 一句 名言 “ Any   theory   must   be   as   simple   as   possible , but   not   simpler " 的 精神 是 一致 的 . 对 建模 问题 说 就是 与 给定 数据 ( 例子 ， 或 训练样本 ) 一致 的 最 简单 者 是 最好 的 选择 . 所以 通常 的 建模 准则 总 包含 两项 ， 一是 模型 输出 与 给定 数据 一致 ( 用 对数 似然 函数 表示 ， 当 数据 为 高斯分布 时 就是 均方 误差 ) ， 二是 对模型 复杂性 ( 参数 多少 ) 的 约束 项 ( 此项 的 作用 可 理解 为 适当 引入 偏置 以 减少 方差 ) . 如 信息 判据 ( AIC ， NIC ) ［ 24 ］ ， 正规化 ［ 25 ］ ， 最小 描述 长度 ( MDL ) ［ 26 ， 27 ］ 都 如此 . 模型 有 泛化 能力 是 基于 它 表示 的 规律 具有 某种程度 的 平滑 性 ( 简单 性 ) ， 正规化 理论 ， 回归 模型 也 都 是 以 不同 形式 对   f ( x ) 的 平滑 性 加以 约束 . 
 5 　 几点 看法 
 　 　 对于 为什么 采用 ANN 以及 用 好 ANN 的 关键问题 ， 有 一些 过于 笼统 甚至 似是而非 的 提法 ， 应当 分析 . 例如 ： 
 　 　 1 ) “ ANN 的 构造 和 作用 机制 更 接近 人脑 ， 所以 对于 模式识别 ， 对 环境 自 适应 等 问题 用 ANN 比 其它 方法 好 ” 。 应当 看到 现有 的 ANN 模型 只是 人脑 的 一种 表面 的 ( 非本质 的 ) 粗略 近似 ， 其 规模 和 机制 的 复杂程度 离 人脑 还有 很大 差距 ， 现有 ANN   仍 是 一种 工程 上 的 人造 模型 . 用 现有 的   ANN   模型 去 解决 上述 一类 问题 并 不 注定 比 传统 方法 好 ， 传统 方法 中 的 一些 基本 困难 ( 如 模型 选择 、 变量 选择 、 有限 样本 问题 ) 也 没有 消失 . 
 　 　 2 ) “ ANN   本质 上 具有 学习 和 自 适应能力 ， 只要 有 例子 ， ANN   就 能 通过 学习 解决问题 ” . 并非 只有   ANN   才 有 学习 能力 ， 例如 有 较长 历史 的 模式识别 与 可 训练 机器 理论 ［ 28 ， 29 ］ 也 是 通过 学习 来 解决问题 的 ， 也 并非 只要 有 例子 就 一定 能 学到 其中 隐含 的 规律 ， 哪 一类 问题 是 可以 学习 的 ， 对 那些 可以 学习 的 问题 其 学习 的 复杂性 如何 ， 正是 计算 学习 理论 研究 的 问题 ［ 30 — 32 ］ ， 一些 研究者 不 理解 上述 问题 的 重要性 ， 以为 学习 速度 是 主要矛盾 . 
 　 　 3 ) “ ANN   可以 模拟 任何 非线性 关系 ， 且 不 需 有关 系统 的 模型 知识 ” . 并非 只有   ANN   具有 逼近 任何 非线性 规律 的 能力 ， 同时 有 能力 的 方法 不 一定 就是 好 方法 ， 还要 看 它 的 计算 复杂性 是否 可以 接受 ( 有效性 ) . 非 参数 回归 可以 模拟 任何 非线性 规律 ， 很早 对 它 的 一致性 ( 即 随着 样本数   n   的 增加 误差 一致 下降 ， 且   n → ∞   时   e → o ) 已 给出 了 理论 证明 ， 但 至今 非 参数 回归 用 的 不 多 ( 有 一些 被 称为 非 参数 回归 的 方法 实际上 是 一种 半 参数 法 ) ， 这是 由于 所 需 样本数 随 问题 规模 急剧 增长 ( Dimensionality 不好 ) ， 文 ［ 33 ］ 证明 非 参数 回归 的 收敛 速度 正比 于   n - r , 其中   r = ( p - m ) / ( 2p + d ) , p 代表 未知 规律   f   的 平滑 程度 ， m   是 要 估计 的   f   的 导数 次数 ， d   为 输入 变量 数 ， 粗略 看 可以 认为 逼近 误差 按   n - ( p / d ) 下降 . 
 　 　 对   MFN 尚 没有 明确 的 规律 ， 有人 对 逻辑 型 网络 ( 奇偶校验 网络 ) 作 了 试验 ， 把 逻辑 函数 看作 谓词 ， 输入 变量 维数 是 谓词 的 阶数 ， 结果 发现 学习 时间 随 输入 维数 指数 上升 ［ 34 ］ . 
 　 　 4 ) “ 传统 的 模式识别 和 回归 分析 要 事先 抽取 特征 ， ANN 可 自动 压缩 变量 维数 ” ， MFN   的 隐层 可 理解 为 对 输入 变量 做 了 变换 ， 但 这 代替 不了 开始 的 特征 ( 变量 ) 选择 ， 特别 是 当 样本量 有限 时 提高 网络 性能 的 一个 重要 措施 是 压缩 原始 变量 的 维数 ， 这一 工作 可用 ANN ［ 35 ］ 也 可以 用 其它 方法 ， 有时 还要 依靠 专家 知识 . 
 　 　 此外 还有 以下 问题 值得注意 ： 
 　 　 1 ) 有限 样本量 问题 　 样本量 有限 是 统计 模式识别 和 回归 分析 的 主要 困难 ， 如果 样本量 不 受限制 ( 也 有 足够 的 计算资源 来 处理 这些 样本 ) ， 则 不管 模型 多 复杂 也 不会 出现 过 拟合 现象 ， 统计 建模 中 的 Dimensionality 问题 也 不 存在 了 . 最近 文 ［ 36 ］ 对此 问题 作 了 分析 ， 结合 ANN 的 讨论 可见 文 ［ 37 ， 38 ］ . 没有 其它 先验 知识 时 ， 解决问题 的 信息 全靠 从 训练样本 得到 . 能 达到 的 最好 效果 决定 于 样本 的 数量 和 质量 ( 如果 认为 样本 来源 可靠 则 只 决定 于 样本 的 数量 ) . 希望 所 选 网络 尽可能 把 隐含 在 样本 中 有关 问题 的 信息 都 提取 出来 ， 这 就 要求 网络 与 样本量 “ 匹配 ” ［ 38 ］ . 
 　 　 2 ) 变量 选择 问题 　 前面 已 指出 ， 为 保证 估计 结果 的 稳定性 ， 所 需 样本量 大致 按 问题 规模 ( 输入 变量 维数 ) 指数 上升 ， 可见 当 样本量 有限 时 ， 保证 推广 能力 的 最 有效 方法 是 降低 输入 向量 的 维数 ， 只 保留 那些 重要 的 变量 . 特征选择 的 具体方法 在 模式识别 和 回归 分析 中 都 有 ， 这里 不再 讨论 . 
 　 　 3 ) 先验 知识 的 运用 　 如果 能 利用 先验 知识 ， 则 有 可能 限制 模型 的 复杂性 ， 对 解决问题 有 很大 帮助 ， 特别 是 样本量 少 而 问题 又 较 复杂 时 ， 应 设法 尽量 利用 先验 知识 . 对 此文 ［ 39 ］ 作 了 讨论 ， 许多 人 主张 把 传统 人工智能 技术 与   ANN 结合 起来 ， 在 一定 程度 上 也 是 由于 传统 AI 更 易于 利用 人类 的 先验 知识 以 降低 ANN 学习 的 负担 . 
 6 　 讨论 
 　 　 提出 以上 看法 不是 想 说明 ANN   的 许多 能力 别的 方法 也 能 做到 ， 从而 限制   ANN   的 应用 与 研究 ， 也 不能 要求 事先 把 为什么 用   ANN   完全 搞清楚 才能 用 它 ， 因为 未 经过 实践 是 不能 搞清楚 的 ， 只是 为了 更 深入 有效 的 研究   ANN   及其 应用 提供 一些 看法 ， 供 大家 深入 思考 . MFN   和 一些 传统 方法 的 等效 性 说明 原来 方法 的 一些 基本 问题 仍然 存在 ， 它们 的 研究 结果 对 研究   ANN   是 很 有用 的 ， 出现 上述 问题 的 一个 原因 是 在   ANN   的 发展 初期 ， 国外 一些 文章 较 多 强调 了   ANN   与 传统 方法 的 不同 ， 而 对 使用 中 的 一些 困难 ， 如 结构设计 ( 建模 问题 ) 、 样本量 、 变量 选择 等 很少 讨论 . 有 一些 应用 文章 ( 不少 是 权威性 的 工作 ) 对 上述 问题 的 处理 不够 合理 ， 例如 有些 应用 中 ， 模型 中 的 未知 参数 ( 权 系数 个数 ) 甚至 比 样本数 还 多 . 对 这种 情况 最近 国外 也 有所 讨论 ［ 40 ］ ， 应 引起 足够 注意 . 当然   MFN   确有 自己 的 特点 ， 作者 认为 主要 是 ： 
 　 　 1 ) 构造 上 更 接近 生物 神经系统 . 信息 的 表示 与 处理 是 分布 在 联接 权上 ， 尽管 离 真实 脑 模型 还有 很大 距离 ， 其 功能 也 没有 达到 所 期望 的 那样 强 ， 但 总是 朝 这 方向 前进 了 一步 . 
 　 　 2 ) 有 很大 的 灵活性 ， 同一 形式 的   MFN ， 可 用作 函数 逼近 、 回归 、 正规化 等等 ， 而且 更 形象化 . 在 某种意义 上 也 可以 灵活 地 引入 不同 性质 的 约束 ， 也 可 方便 地 引入 信息反馈 与 上下文 信息 ， 加 一些 局部 反馈 又 可 模拟 时间 序列 或 动态 系统 等等 . 促进 了 上述 各 方法 的 融合 . 
 　 　 3 ) 本质 上 的 并行处理 及 结构 上 的 规则性 ， 对 硬件 实现 有利 . 
 资金 项目 ： 国家 攀登 计划 认知科学 ( 神经网络 ) 重大 关键 项目 资助 
 作者 单位 ： 清华大学 自动化系 　 北京 　 100084 
 参考文献 
 ［ 1 ］ 张承福 . 对 当前 神经网络 研究 的 几点 看法 . 力学 进展 ， 1994 ， 26 ( 2 ) ： 181 — 186 . 
 ［ 2 ］ 张承福 等 . 联想 记忆 神经网络 中 的 若干 问题 . 自动化 学报 ， 1994 ， 20 ( 5 ) ： 513 — 521 . 
 ［ 3 ］ Barron   A   R . Statistical   properties   of   artificial   neural   networks . Proc.28 th   Conference   on   decision   and   control , 1989 , 280 - 285 . 
 ［ 4 ］ Albus . A   new   approach   to   manipulator   control , the   cerebellar   model   articulation   controller ( CMAC ) . J . on   Dynamical   Systems   Measure   and   control , 1975 , 220 - 227 . 
 ［ 5 ］ Platt   J . A   resource - allocating   network   for   function   interpolation . Neural   computation , 1991 , 3 : 213 - 225 . 
 ［ 6 ］ Barrett   J   F . The   use   of   functional   in   the   analysis   of   nonlinear   physical   system . J . Electro   and   control , 1963 , 15 : 567 - 615 . 
 ［ 7 ］ Chen   Tianping   et   al . Approximation   of   continuous   functions   by   NN   with   application   to   dynamic   systems . IEEE   Trans . NN , 1993 , 4 ( 6 ) : 910 - 918 . 
 ［ 8 ］ Modha   D   S . et   al . Multilayer   functionals ,   in   Mathamatical   Approch   to   NN , Taylor   J   G   ed . Amsterdam   Elsevier   Science   Publishers , 1993.235 - 260 . 
 ［ 9 ］ White   H . Artificial   neural   networks ,   approximation   and   learning   theory . Cambirdge , MA . Blackwell . 
 ［ 10 ］ Asoh   H . Nonlinear   data   analysis   and   MLP . Proc . of   IJCNN - 90 , 1990 , Ⅱ 411 - Ⅱ 415 . 
 ［ 11 ］ Gish   H . Apobabilistic   approach   to   the   understanding   and   training   of   NN   classifier . Proc . ICASSP ' 90 , 1990 , 1361 - 1364 . 
 ［ 12 ］ 成 平等 . 投影 寻踪 — — 一类 新 的 统计 方法 . 应用 概率 统计 ， 1986 ， 3 ： 267 — 276 . 
 ［ 13 ］ Maechler   M , et   al . Projection   pursuit   learning   networks   for   regression . Proc . Tools   for   AI , IEEE   Press , 1990 , 350 - 358 . 
 ［ 14 ］ Hwang   Jeng - Nang , et   al . Regression   modeling   in   backpropagation   and   projection   pursuit   learning . IEEE   Trans . NN , 1994 , 5 ( 3 ) : 342 - 357 . 
 ［ 15 ］ Hwang   Jeng - Nang , et   al . The   cascade - correlation   learning ,   A   projection   pursuit   learning   perspective . IEEE   Trans . NN , , 1996 , 7 ( 2 ) : 278 - 288 . 
 ［ 16 ］ Zhao   Ying , et   al . Implementing   projection   pursuit   learning , IEEE   Trans . NN ,   362 - 392 . 
 ［ 17 ］ Poggio   T , et   al . Networks   for   approximation   and   learning . Proc . IEEE , 1990 , 78 ( 9 ) : 1481 - 1496 . 
 ［ 18 ］ Girosi   F , et   al . Regularization   theory   and   neural   network   architectures . Neural   computation , 1995 , 7 : 219 - 296 . 
 ［ 19 ］ Ogawa   H . Neural   network   theory   as   inverse   problem . J . EICE , Japan , 1990 , 73 : 690 - 759 . 
 ［ 20 ］ 阎 平凡 . 多层 前馈 网络 的 推广 和 学习 问题 . 中国 神经网络   93   年 学术会议 论文集 ， 1993 年 ， 上册 ， 68 — 77 . 
 ［ 21 ］ 阎 平凡 . 现在 神经网络 解决问题 的 困难 与 结构 自 适应 问题 . 中国 神经网络   94 年 学术 大会 报告 ， 1994 ， 10 月 ， 武汉 . 
 ［ 22 ］ Gemman   S . Neural   networks   and   the   Bias / variance   dilemma . Neural   computation , 1992 , 4 : 1 - 57 . 
 ［ 23 ］ Garett   A   J . Ockham ' s   Razor ,   in   maximum   entropy   and   bayes   method , Grandy   W   T , et   al . Ed . Kluwer   Academic   Publisher , 1991.357 - 364 . 
 ［ 24 ］ Nobora , et   al . NIC - Determing   the   number   of   hidden   unit   for   ANN . IEEE   Trans . NN , 1994 , 5 ( 6 ) : 865 - 872 . 
 ［ 25 ］ Wu   lizhong , et   al . A   smoothing   regularizor   for   feedfroward   and   recurrent   neural   networkds . IEEE   Trans . NN.1996 , 8 : 461 - 489 . 
 ［ 26 ］ 阎 平凡 . 最小 描述 长度 与 多层 前馈 网络 设计 中 的 一些 问题 . 模式识别 与 人工智能 ， 1993 ， 6 ( 2 ) ： 143 - 148 . 
 ［ 27 ］ Rohwer   R , et   al . Minimum   description   length , regularization   and   multimodel   data . IEEE   Trans . NN.1996 , 8 : 595 - 609 . 
 ［ 28 ］ 边肇祺 等 . 模式识别 . 北京 ： 清华大学出版社 ， 1988 . 
 ［ 29 ］ Sklansky   J . 著 ， 阎 平凡 等 译 . 模式 分类器 和 可 训练 机器 . 北京 ： 科学出版社 ， 1987 
 ［ 30 ］ 张鸿宾 . 计算 学习 理论 与其 应用 ( 1 ) ( 2 ) . 计算机科学 ， 1992 ， 19 ： 18 — 22 . 
 ［ 31 ］ Anthony   M , et   al . Computational   learning   theory .   Cambridge ,   England :   Cambridge   University   Press , 1992 . 
 ［ 32 ］ Anthony   M , et   al . Computational   learning   theory   for   ANN . in   Mathematical   approcach   to   NN , J   G   Taylor   Ed.1993 , 25 - 62 . 
 ［ 33 ］ Stone   C   J . Optimal   global   rate   of   convergence   for   nonparametric   regression . ANN . Stat , 1982 , 10 ( 4 ) : 1040 - 1053 . 
 ［ 34 ］ Tesauro   G , et   al . Scaling   relationships   in   BP   learning . Complex   system , 1988 , 2 : 39 - 44 . 
 ［ 35 ］ Mao   Jianchang , et   al . ANN   for   feature   extraction   and   multivariate   data   projection . IEEE   Trans . NN , 1995 , 6 ( 2 ) : 296 - 317 . 
 ［ 36 ］ Randy   S   J , et   al . Small   sample   size   effects   in   statistical   pattern   recognition , recommendation   for   practice . IEEE   Trans . PAMI - 13 , 1991 , 252 - 264 . 
 ［ 37 ］ 张鸿宾 . 训练 多层 网络 的 样本数 问题 . 自动化 学报 ， 1993 ， 19 ： 71 - 77 . 
 ［ 38 ］ 阎 平凡 . 人工神经网络 的 容量 ， 学习 与 计算机 复杂性 问题 . 电子学 报 ， 1995 ，   23 ( 4 ) ： 63 - 67 . 
 ［ 39 ］ Abu - Mostafa   Y   S . Hints . Neural   computation , 1995 , 7 : 639 - 671 . 
 ［ 40 ］ Duin   R . Superlearning   and   neural   network   magic . Pattern   recognition   letters , 1994 , 15 : 215 - 217 . 
 　 　 阎 平凡 　 简介 见 本刊 第 21 卷 第 1 期 . 
 收稿 日期 　 1995 - 08 - 07 
