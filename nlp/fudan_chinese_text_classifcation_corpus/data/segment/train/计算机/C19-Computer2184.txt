计算机 应用 研究 
 APPLICATION   RESEARCH   OF   COMPUTERS 
 2000 　 Vol.17 　 No.2 　 P.34 - 37 
 
 
 
 神经网络 规则 抽取 研究 
 孙晨 　 周志华 　 陈兆乾 
 摘   要   尽管 神经网络 已经 在 很 广泛 的 领域 得到 应用 ， 但 由于 训练 好 的 神经网络 中 的 知识 不 易于 理解 ， 神经网络 被 视为 一个 典型 的 摵 谙 鋽 结构 。 从 神经网络 中 抽取 规则 来 表示 其中 隐含 的 知识 是 解决 这个 问题 的 一个 有效 的 手段 。 将 对 一些 具有 代表性 的 神经网络 规则 抽取 算法 进行 综述 分析 ， 并 提出 一些 未来 的 研究 重点 。 
 关键词   神经网络   规则 抽取   机器 学习 
 1   引言 
 　 　 神经网络 技术 自从 80 年代 复苏 以来 获得 了 较大 的 发展 ， 由于 其 具有 强有力 的 非线性 处理 能力 ， 可以 在 数据量 较大 、 领域 信息 不 完备 以及 存在 噪音 数据 的 情况 下 取得 传统 符号 学习 方法 所 难以达到 的 效果 。 但是 ， 由于 训练 好 的 神经网络 模型 是 典型 的 摵 谙 鋽 结构 ， 获取 的 知识 不易 被 人 理解 ， 使 其 在 数据挖掘 、 知识 精化 等 方面 的 应用 中 受到 了 很大 的 限制 。 为了 充分发挥 神经网络 技术 的 特长 ， 获取 神经网络 中 的 知识 已经 成为 该 领域 中 的 一个 非常 重要 的 研究课题 。 
 　 　 获取 神经网络 模型 中 的 知识 主要 有 两大 研究 分支 ： 一是 研究 如何 利用 神经网络 来 精化 规则 ； 二是 研究 从 神经网络 中 抽取 规则 。 本文 主要 讨论 后者 ， 在 分析 现有 的 典型 规则 抽取 算法 基础 上 ， 对 神经网络 规则 抽取 技术 未来 的 研究 发展 提出 一些 看法 。 
 2   规则 抽取 研究 的 重要性 
 　 　 神经网络 技术 从 四十年代 问世 以来 ， 已经 成功 地 应用 在 很多 领域 ， 但是 其 能力 仍 受到 怀疑 。 主要 原因 在于 神经网络 通过 训练 学习 到 的 知识 以 数值 的 形式 存储 在 网络 中 ， 由于 其 知识 表示 的 分布式 特性 ， 使用者 难以 对 神经网络 的 能力 有 清晰 的 了解 ， 也 无法 获得 明确 的 推理 过程 的 解释 。 由于 对 知识 表示 的 可 理解 性是 学习 算法 优劣 的 重要 评价 标准 之一 [ 1 ] ， 因此 规则 抽取 对 神经网络 学习 的 研究 有 重要 的 意义 。 从 神经网络 中 抽取 规则 的 目的 是 将 神经网络 中 隐含 的 知识 以 一种 易于 理解 的 方式 明确 地 表达出来 。 一方面 ， 以 规则 的 形式 表示 神经网络 获取 的 知识 可以 方便 使用者 和 设计者 理解 神经网络 的 推理 过程 ； 另一方面 ， 通过 抽取 出来 的 规则 ， 用户 可以 发现 输入 数据 中 一些 以前 被 忽略 的 重要 关系 ， 从而 帮助 用户 进行 决策 处理 。 此外 ， 除了 可以 作为 训练 好 的 神经网络 的 知识 解释器 ， 神经网络 规则 抽取 技术 作为 一种 适用 于 各种 不同 领域 的 规则学习 的 工具 ， 其 有效性 和 正确性 也 已经 得到 证实 。 
 　 　 正 因为 神经网络 规则 抽取 具有 上述 重要 意义 ， 这方面 的 工作 受到 了 越来越 多 的 关注 。 IEEE   Transaction   on   Neural   Networks 在 1998 年底 专门 出版 了 一期 专刊 ， Tickle 等 在 首篇 文章 中 明确提出 从 神经网络 中 抽取 规则 是 当前 神经网络 界 急需解决 的 问题 [ 2 ] ， 这 说明 了 对 规则 抽取 的 研究 目前 已经 成为 一个 重要 的 研究 方向 。 
 3   规则 抽取 算法 
 　 　 神经网络 规则 抽取 的 研究 最早 开始 于 80 年代 末 。 Gallant 在 1988 年 [ 3 ] 描述 了 一个 可以 用 IF - THEN 规则 解释 推理 结论 的 神经网络 专家系统 ， 自此以后 ， 很多 研究者 都 进行 了 规则 抽取 方面 的 研究 工作 ， 并 取得 了 大量 的 成果 。 根据 抽取 规则 算法 设计 思想 的 不同 可以 分成 基于 结构 分析 的 算法 和 基于 性能 分析 的 算法 两大类 。 根据 抽取 规则 算法 适用 的 神经网络 结构 不同 ， 可以 分成 针对 特殊 神经网络 结构 的 算法 和 通用型 规则 抽取 算法 。 根据 抽取 的 规则 形式 不同 又 可 分为 抽取 最 普通 的 IF - THEN 规则 的 算法 、 抽取 模糊 规则 的 算法 、 抽取 决策树 规则 算法 以及 抽取 区间 规则 的 算法 等 。 在 本节 中 ， 我们 从 规则 抽取 算法 的 设计 思想 出发 ， 分别 对 基于 结构 分析 的 和 基于 性能 分析 的 典型 算法 进行 介绍 和 分析 。 
 3.1   基于 结构 分析 的 规则 抽取 算法 
 　 　 基于 结构 分析 的 神经网络 规则 抽取 算法 把 规则 抽取 看成 一个 搜索 过程 。 其 基本 思想 是 把 已 训练 好 的 神经网络 结构 映射 成 对应 的 规则 。 图 1 就 表示 一个 最 简单 的 规则 抽取 算法 。 
 
 图 1 　 最 简单 的 规则 抽取 
 　 　 由于 搜索 过程 的 计算 复杂度 和 输入 分量 之间 是 指数 级 的 关系 ， 当 输入 分量 很多 时 ， 会 出现 组合 爆炸 问题 。 因此 ， 此类 算法 一般 采用 剪枝 聚类 等 方法 减少 神经网络 中 的 连接 以 降低 计算 复杂度 。 比较 典型 的 算法 有 以下 几种 ： 
 　 　 ( 1 ) Gallant 方法 [ 3 ] 
 　 　 随着 神经网络 研究 的 兴起 ， 于 80 年代 末 ，   Gallant 首先 提出 了 一个 新型 的 神经网络 专家系统 ， 并 描述 了 一个 简单 的 规则 抽取 算法 用于 解释 该 专家系统 所 做 的 推理 。 
 　 　 该 算法 抽取 单个 规则 来 解释 神经网络 如何 为 某个 给定 事例 ( Case ) 得出结论 。 因此 ， 其 基本 思想 就 是从 当前 已知 的 信息 集中 选择 一个 能 有效 地 产生 该 结论 的 最小 信息 集合 。 也就是说 不管 其它 未知 的 输入 分量 的 取值 为 多少 ， 只要 满足 该 最小 信息 集合 的 取值 要求 就 可以 得出 该 结论 。 
 　 　 严格 地说 ， 该 算法 并 不是 一个 真正 意义 上 的 规则 抽取 算法 。 其 主要 目的 不是 进行 规则 抽取 ， 而是 希望 对 一个 已知 结论 的 推理 给出 一个 合理 的 解释 ， 因此 算法 简单 ， 只 适用 于 小型 的 神经网络 。 
 　 　 ( 2 )   Subset 算法 [ 4 ] 
 　 　 由 Fu 提出 的 Subset   算法 是 最早 的 神经网络 规则 抽取 算法 之一 。 基于 搜索 的 规则 抽取 算法 大多 是 在 它 的 基础 上 发展 起来 的 。 
 　 　 Subset 算法 采用 简单 的 广度 优先 算法 抽取 普通 的 IF - THEN 规则 。 Subset 是 由 神经网络 中 所有 连接 组成 的 集合 的 幂集 的 一个 子集 。 算法 从 由 单个 连接 组成 的 初始 Subset 集合 开始 测试 ， 检测 其 每个 成员 能否 有效 地 保证 超过 偏移 值 ， 如果 可以 的话 则 用 规则 表述 出来 。 不断扩大 Subset 的 大小 并用 同样 的 方法 测试 产生 规则 ， 直至 所有 可能 的 Subset 都 被 搜索 过 。 最后 删除 冗余 的 规则 。 
 　 　 Subset 算法 没有 采用 降低 计算 复杂性 的 技术 ， 因此 它 只 适合 处理 小型 的 问题 。 Saito 和 Nakano [ 5 ] 提出 的 KT 算法 ， 用 限定 规则 前 件数 目的 方法 对 Subset 算法 进行 了 改进 ， 并 将 属性 分为 “ 正 属性 ” ( Pos - atts ) 和 “ 负 属性 ” ( Neg - atts ) ， 对 其 分别 进行 搜索 。 这样 ， 虽然 降低 了 搜索 的 复杂性 但 也 限制 了 抽取 规则 的 能力 ， 无法 对原 神经网络 进行 精确 地 描述 。 
 　 　 ( 3 )   MOFN 算法 [ 4 ] 
 　 　 MOFN 算法 是 基于 Towell   和 Shavlik 提出 的 KBANN ( Knowledge   based   networks ) 神经网络 的 一种 规则 抽取 算法 ( 如图 2 所示 ) 。 
 
 图 2 　 抽取 MOFN 规则 
 　 　 其 抽取 的 规则 形式 为 M - of - N 的 规则 ： 
 if   ( M   of   N   antecedents   are   true )   then   ........... 
 　 　 MOFN 算法 技术 的 关键在于 引入 等价 类 ( Equivalence   class ) 的 概念 。 算法 首先 使用 标准 的 聚类 算法 将 神经网络 中权值 接近 的 连接 合并 创建 等价 类 ， 并 将 每个 等价 类 的 权值 设成 该组 的 平均值 ， 然后 去掉 那些 对 结果 影响 不大 的 等价 类 ， 在 不 调整 权值 的 前提 下 对 神经网络 重新 进行 训练 ， 最后 直接 根据 网络结构 和 权值 抽取 出 规则 ， 并 将 规则 简化 。 
 　 　 由于 使用 一般 的 神经网络 算法 训练 好 的 神经网络 中 的 连接 权 大多 分散 地 分布 在 权值 空间 中 ， 难以 对 其 进行 处理 ， 因此 MOFN 算法 仅 适用 于 KBANN 神经网络 。 为了 扩展 其 适用范围 ， Craven 在 1993 年 提出 用 柔性 权 共享 ( Soft   weight - sharing ) 的 方法 训练 神经网络 [ 1 ] ， 在 训练 中 鼓励 权值 聚类 ， 然后 再 对 用 该 方法 训练 好 的 神经网络 使用 MOFN 算法 ， 可以 有效 地 抽取 MOFN 规则 。 
 　 　 MOFN 算法 抽取 出来 的 规则 集 比较简单 易懂 ， 而且 这种 规则 形式 也 减少 了 产生 的 规则 数 。 另外 ， 由于 采用 了 等价 类 的 方法 对 连接 进行 了 聚类 ， 也 使得 最后 抽取 规则 的 时间 花费 大大减少 。 
 　 　 MOFN 算法 的 主要 缺陷 在于 ： 首先 ， 该 算法 要求 神经网络 中 的 每个 神经元 的 激活 值为 双 极值 模式 ， 这 就 对 激活 函数 的 选择 给出 了 某些 限定 ， 从而 限制 了 神经网络 的 功能 。 其次 ， 该 算法 要求 输入 属性 的 取值 为 离散 值 ， 这 就 使得 神经网络 学习 系统 在 处理 连续 属性 方面 的 优势 难以 得到 发挥 ； 第三 ， 该 算法 要求 每个 神经元 表示 一个 唯一 的 概念 ， 这 与 神经网络 中 通常 的 分布式 知识 表示 有 较大 的 冲突 ， 极大 地 限制 了 其 适用范围 。 
 　 　 ( 4 )   RX ( Rule - extraction ) 算法 [ 6 ] 
 　 　 该 算法 是 Setiono 提出 的 一个 适用 于 标准 的 三层 前馈 网络 的 通用型 规则 抽取 算法 。 该 算法 抽取 的 是 普通 的 IF - THEN 规则 。 
 　 　 该 算法 在 最 基本 的 神经网络 规则 抽取 算法 上作 了 两 方面 的 改进 ： ( 1 ) 将 隐含 层 神经元 的 激活 值 通过 聚类 离散 化 ， 也就是说 用 一组 离散 值来 表示 每个 隐含 层 神经元 的 激活 值 ； ( 2 ) 采用 对 隐层 神经元 再 划分 的 技术 。 
 　 　 该 方法 对于 每个 输出 层 神经元 ， 直接 根据 与其 相连接 的 隐层 神经元 的 离散 化 的 激活 值 构造 相应 的 规则 。 对于 隐层 神经元 ， 首先 考虑 与其 相连接 的 输入 神经元 的 个数 ， 如果 与 之 相连 的 输入 神经元 的 数目 不 超过 标准 ， 则 直接 对应 产生 描述 规则 。 否则 ， 将 生成 一个 子 网络 ， 其中 输出 结点 的 个数 为 该 隐层 神经元 的 离散 激活 值 的 数目 ， 输入 结点 的 个数 为 与 该 隐层 神经元 相连 的 输入 神经元 的 个数 。 对子 神经网络 进行 训练 后 再 调用 本 算法 抽取 规则 。 显然 ， RX 算法 以 递归 的 处理 方式 进行 规则 抽取 。 
 　 　 另外 ， 张朝辉 [ 12 ] 等 人 提出 了 一种 类似 于 RX 算法 的 用 神经网络 发现 分类 规则 的 算法 ， 其 特点 是 采用 了 遗传算法 ( GA ) 对 神经网络 结构 进行 剪枝 。 
 　 　 在 大部分 基于 搜索 的 神经网络 规则 抽取 算法 中 为了 减少 搜索 的 时间 而 不得不 限制 规则 前件 的 个数 ， 该 算法 采取 对 隐含 层 神经元 进行 再 构造 的 方法 ， 虽然 增加 了 算法 的 训练 时间 ， 但 对 神经网络 的 描述 相对 更为 精确 。 
 　 　 该 算法 适用 于 标准 的 三层 前馈 神经网络 。 为了 能 有效 地 产生 规则 ， 算法 要求 对 神经网络 先 进行 剪枝 以 去掉 对 结论 影响 不大 的 连接 。 由于 在 规则 抽取 过程 中 可能 递归 地 生成 多个 子 网络 ， 因此 该 算法 的 时间 开销 很大 。 由于 对 激活 值 进行 了 离散 化 ， 与 以前 的 算法 相比 RX 算法 对 激活 值 的 范围 减少 了 限制 ， 但 该 算法 仍 要求 输入 属性 为 离散 属性 ， 要 处理 连续 属性 仍 需要 将 其 先 离散 化 [ 7 ] 。 
 3.2   基于 性能 分析 的 规则 抽取 算法 
 　 　 与 基于 结构 分析 的 算法 不同 ， 基于 性能 分析 的 神经网络 规则 抽取 算法 不 把 规则 抽取 过程 看作 一个 对 网络结构 进行 搜索 的 过程 ， 而是 把 神经网络 看成 一个 整体 来 处理 ， 在 规则 抽取 过程 中 不 考虑 具体 的 神经网络 结构 。 
 　 　 ( 1 )   RF ( Rule   from   facts ) 算法 和 RN ( Rule   from   networks ) 算法 [ 8 ] 
 　 　 Saito 和 Nakano 提出 这 两种 算法 用于 抽取 DNF ( Disjuctive   Normal   Form ) 形式 的 规则 。 RF 算法 是 使用 启发式 搜索 ， 对 搜索 空间 剪枝 后 从 事实 中 直接 抽取 规则 。 RN 算法 则 从 训练 好 的 神经网络 中 抽取 规则 。 RN 算法 首先 根据 一个 真 事实 ( Positive   fact ) 通过 调整 输入 属性 的 取值 范围 ， 极大 化 一条 规则 ， 然后 根据 伪 事实 ( Negative   fact ) 来 缩小 该 规则 ， 直至 该 规则 不 包含 任何 反例 为止 。 
 　 　 该 算法 是 最早 的 神经网络 规则 抽取 算法 之一 。 它 把 整个 网络 看成 一个 整体 ， 不 考虑 网络 的 具体 结构 ， 主要 通过 事实 来 抽取 规则 。 
 　 　 ( 2 )   基于 学习 的 规则 抽取 算法 
 　 　 基于 学习 的 神经网络 规则 抽取 算法 将 规则 抽取 视为 一个 归纳 学习 问题 。 Craven   和 Shavlik   根据 该 思想 提出 了 TREPAN 等 算法 来 抽取 多种形式 的 规则 [ 9 , 10 ] 。 根据 Craven   的 定义 ， 规则 抽取 是 给定 一个 训练 好 的 神经网络 以及 用于 对 其 进行 训练 的 数据 ， 产生 一个 可 理解 的 概念 描述 ， 该 描述 将 以 与 网络 相同 的 方式 对 示例 进行 划分 。 
 　 　 其 关键技术 在于 把 规则 抽取 过程 视作 一个 由 数据 和 提问 驱动 的 学习 过程 。 算法 的 基本 思想 是 提供 了 两个 调用 来 回答 提问 ： 一个 是 实例 调用 ( Example   Oracle ) ， 它 用于 随机 地 产生 实例 ， 并 根据 这个 实例 产生 若干条 假设 规则 。 简单 地说 ， 对于 一个 实例 ， 直接 将 其 对应 成 一条 最 特殊 的 规则 ， 这 也 就是 最 基本 的 第一条 假设 规则 ， 在 此基础 上 ， 任意 删除 一个 或 多个 属性 就 可以 产生 其它 的 假设 规则 ， 删除 的 前件 越 多 ， 规则 的 泛化 能力 越强 ， 覆盖 的 实例 的 分类 错误 性 也 越 大 ， 这 就 需要 对 规则 进行 测试 。 另 一个 是子 调用 ( Subset   Oracle ) ， 它 的 设计 随 学习 模型 的 不同 而 不同 ， 其 主要 功能 就是 根据 已经 训练 好 的 学习 模型 测试 给出 的 假设 规则 是否 正确 ， 每次 对 根据 实例 数据库 产生 的 一组 假设 规则 进行 测试 ， 产生 一组 正确 的 规则 集 ， 最后 将 该 规则 集中 前 件 数目 最少 的 那条 规则 放入 规则 库 。 
 　 　 该类 算法 最大 的 特点 在于 它 是 一个 通用型 的 规则 抽取 算法 ， 不 受 特定 学习 模型 的 限制 ， 可以 从 各种 神经网络 以及 符号 学习 系统 中 抽取 出所 需 的 规则 。 在 输入 属性 分量 较 多 的 情况 下 其 算法 的 计算 复杂度 比 其它 算法 要 低 。 该类 算法 的 主要 缺陷 在于 假设 规则 的 产生 方式 限制 其仅 适用 于 处理 离散 属性 ， 而 不 适合 连续 属性 的 处理 。 如果 要 处理 连续 属性 ， 则 需要 在 规则 抽取 之前 将 所有 连续 属性 进行 离散 化 处理 。 
 　 　 ( 3 )   基于 VI 分析 ( Validity - Interval   Analysis ) 的 规则 抽取 算法 [ 11 ] 
 　 　 Thrun 提出 了 一种 基于 有效 区间 分析 ( Validity - Interval   Analysis ) 的 规则 抽取 算法 。 该 算法 将 整个 神经网络 结构 看成 一个 黑盒 ， 只 考虑 输入 对 输出 产生 的 影响 ， 而 不 考虑 隐含 层 结点 。 因此 算法 抽取 的 规则 如 公式 ( 1 ) 。 
 
 THEN   此组 xi 表示 的 概念 为 真 　 　 ( 1 ) 
 其中 xi 表示 某个 输入 或 输出 属性 ， 表示 对 该 属性 的 取值 空间 的 一个 划分 。 算法 将 从 训练 实例 中 产生 的 最 特殊 的 规则 作为 种子 规则 ， 用 VI 分析 技术 对 该 规则 进行 测试 ， 根据 结果 扩大 对 取值 空间 的 划分 直至 不能 扩大 为止 。 不断 循环 产生 新 的 种子 规则 并用 上述 方法 改进 规则 ， 直至 到达 某个 结束 标准 。 
 　 　 该 算法 不 受 神经网络 结构 的 限制 ， 产生 的 规则 直接 从 输入 分量 映射 到 输出 分量 ， 不 受 中间 神经元 多少 的 影响 ， 适用 于 各种 多层 神经网络 ， 且 对 输入 属性 的 取值 没有 限制 。 抽取 出来 的 规则 精度 比较 高 ， 但是 以 区间 的 形式 表示 规则 前 件 使得 规则 的 可 理解 性 很 低 。 而且 该 算法 的 计算 开销 十分 昂贵 ， 在 实际 应用 中是 不 可行 的 ， 因此 该 算法 的 主要用途 是 进行 规则 测试 ， 而 不是 用于 规则 抽取 领域 。 
 　 　 ( 4 )   Benitez 等 提出 的 算法 
 　 　 Benitez 、 Castro 和 Requena [ 11 ] 提出 标准 的 三层 前馈 神经网络 学习 系统 和 基于 模糊 规则 的 学习 系统 是 等价 系统 。 文章 证明 对于 任意 使用 布尔 函数 作为 中间层 神经元 激活 函数 的 三层 前馈 神经网络 均 存在 一个 对应 的 FAS ( fuzzy   Additive   System ) 。 神经网络 中 的 每 对 ( 隐含 神经元 j ， 输出 神经元 k ) 都 可以 直接 对应 以下 的 一条 模糊 规则 如 公式 ( 2 ) 。 
 　 　 ( 2 ) 
 其中 A 代表 R 上 的 模糊集 ， 其 隶属 度 函数 为 神经网络 的 激活 函数 ， xi 代表 第 i 个 输入 神经元 的 输入 值 ， wij 代表 第 i 个 输入 神经元 和 第 j 个 中间 神经元 之间 的 权值 ， τ j 为 第 j 个 中间 神经元 的 偏移 值 ， yk 代表 第 k 个 输出 神经元 的 输出 值 ， β jk 代表 第 j 个 中间 神经元 和 第 k 个 输出 神经元 之间 的 权值 。 根据 三层 前馈 神经网络 的 基本 算法 可以 简单 证明 上式 的 正确性 。 虽然 用该 方法 可以 简单 地 抽取 规则 ， 但 规则 的 可 理解 性 很 差 ， 在 实际 应用 中 用途 不 大 。 
 4   进一步 的 研究 
 　 　 作为 一种 获取 神经网络 结构 中 隐含 的 知识 的 手段 ， 神经网络 规则 抽取 算法 无疑 获得 了 较大 的 成功 ， 它 送给 人们 一把 打开 知识 丰富 的 摵 诤 袛 的 钥匙 。 但是 现有 的 神经网络 规则 抽取 算法 在 应用 中 仍 有 很多 不足之处 。 对 这些 不足之处 的 改进 已经 成为 新 的 研究 目标 。 进一步 的 研究 主要 集中 在 以下 方面 ： 
 　 　 ( 1 )   连续 属性 的 处理 
 　 　 大部分 规则 抽取 算法 受限于 神经网络 结构 和 应用 范围 ， 主要 原因 是 只能 处理 离散 的 输入 属性 ， 而 对 连续 输入 属性 无能为力 ， 产生 这个 问题 的 原因 在于 简单 的 规则 本身 对 连续 属性 的 表达能力 不够 ， 可以 考虑 适当 地 引入 其它 的 规则 表达形式 ， 比如 使用 比较简单 的 模糊 规则 来 处理 连续 属性 ， 采用 回归 树 ( Regression   trees ) 作为 抽取 的 规则 的 表达形式 [ 4 ] 。 也 可以 采用 比较 好 的 聚类 算法 ， 在 规则 抽取 过程 中 根据 需要 对 连续 属性 离散 化 ， 而 不 在 训练 神经网络 时 进行 离散 化 工作 。 
 　 　 ( 2 )   算法 效率 的 提高 
 　 　 规则 抽取 算法 的 计算 复杂度 可能 是 该 技术 进一步 扩宽 应用 的 一个 很 重要 的 限制 因素 。 因此 如何 提高 算法 的 效率 ， 降低 计算 复杂度 也 是 今后 工作 的 重点 。 其 解决 方法 可以 从 两 方面 进行 考虑 ： 一是 可以 采用 各种 剪枝 算法 降低 神经网络 结构 的 复杂性 ； 二是 可以 通过 减少 搜索 学习 空间 ， 采用 各种 启发式 搜索 策略 来 降低 计算 复杂度 。 
 　 　 ( 3 )   规则 表示 形式 的 研究 
 　 　 现有 的 算法 在 抽取 出来 的 规则 数目 以及 规则 表示 的 复杂性 方面 也 有待 提高 。 可以 考虑 其它 形式 的 规则 表示 方式 ， 比如 采用 带 优先 度 的 规则 表示 方式 ， 降低 规则 的 数目 和 规则 表示 的 复杂性 ； 另外 ， 也 可以 考虑 对 抽取 的 规则 集 采用 剪枝 算法 [ 1 ] ， 提高 规则 的 可 理解 性 和 规则 的 泛化 能力 。 
 　 　 ( 4 )   从 神经网络 集成 中 抽取 规则 
 　 　 目前 ， 神经网络 集成 ( Ensemble ) [ 13 ] 逐渐 成为 神经网络 领域 的 一个 研究 热点 。 该 技术 来源于 机器 学习 界 目前 极 热门 的 Boosting 方法 [ 14 ] ， 虽然 神经网络 集成 能 提高 预测 精度 ， 但 其 可 理解 性比 单一 神经网络 更 低 ， 因此 研究 如何 从 神经网络 集成 中 抽取 易于 理解 的 规则 或 规则 集 日益 成为 规则 抽取 领域 的 一个 重要 研究 方向 。 
 5   小结 
 　 　 缺乏 对 隐含 知识 的 解释 能力 一直 是 阻碍 神经网络 技术 被 广泛应用 的 重要 因素 之一 。 从 神经网络 中 抽取 规则 的 研究 工作 ， 除了 可以 用 易于 理解 的 方式 解释 神经网络 学习 到 的 知识 外 ， 对于 知识 发现 的 研究 ， 专家系统 中 的 知识 获取 工作 的 研究 ， 以及 研究 如何 融合 神经网络 技术 和 符号 学习 技术 的 研究 都 很 有 参考价值 。 
 　 　 现有 的 各种 神经网络 规则 抽取 算法 都 还 存在 着 不同 的 缺陷 ， 例如 算法 的 计算 复杂性 较 高 ， 规则 的 可 理解 性 较差 等 。 我们 相信 ， 随着 规则 抽取 研究 的 不断深入 将会 使 神经网络 技术 得到 更为 广泛 的 应用 。 
 本 课题 获得 国家自然科学基金 资助 
 孙晨 （ 南京大学 计算机软件 新 技术 国家 重点 实验室   南京   210093 ） 
 周志华 （ 南京大学 计算机软件 新 技术 国家 重点 实验室   南京   210093 ） 
 陈兆乾 （ 南京大学 计算机软件 新 技术 国家 重点 实验室   南京   210093 ） 
 参考文献 
 1 ， Craven   M ,   Shavlik   J .   Learning   Symbolic   Rules   Using   Artificial   Neural   Networks .   In :   Proceedings   of   the   Tenth   International   Conference   on   Machine   Learning ,   Amherst ,   MA ,   Morgan   Kaufmann ,   1993 ,   73 ～ 80 
 2 ， Alan   B . Tickle , Robert   Andrews ,   The   Truth   Will   Come   to   Light : Directions   and   Challenges   in   Extracting   the   Knowledge   Embedded   Within   Trained   Artificial   Neural   Networks .   IEEE   TRANSACTIONS   ON   NEURAL   NETWORKS , Vol   9 , NO   6   NOVEMBER   1998   1057 ～ 1068 
 3 ， Stephen   I . Gallant ,   Connectionist   Expert   Systems   Communication   of   the   ACM .   Vol   31 , NO   2   1988   152 ～ 169 
 4 ， Towell   G . G   Shablik , J . W , ( 1993 ) ,   Extracting   Refined   Rules   From   Knowledge - Based   Neural   Networks   Machine   Learning   Vol.13   , 1 ,   1993 
 5 ， Fu , L . M   ( 1991 )   Rule   Learning   by   Searching   on   Adapted   Nets .   In   Porceeding   of   the   Ninth   National   Conference   on   Artificial   Intelligence , pp   590 ～ 595 ,   Anaheim , CA : AAAI   Press 
 6 ， Setiono   R .   Extracting   Rules   from   Neural   Networks   by   Pruning   and   Hidden - Unit   Splitting .   Neural   Computation ,   1997 ,   Vol.9 ,   No.1 ,   205 ～ 225 
 7 ， Craven   M ,   Shavlik   J .   Extracting   Tree - Structured   Representations   of   Trained   Networks .   In :   Touretzky   D ,   Mozer   M ,   Hasselmo   M ,   eds . ,   Advances   in   Neural   Information   Processing   Systems   ( Volume   8 ) ,   The   MIT   Press ,   Cambridge ,   MA ,   1996 ,   24 ～ 30 
 8 ， Saito   K ,   Nakano   R .   Rule   Extraction   from   Facts   and   Neural   Networks .   In :   Proceedings   of   the   International   Neural   Network   Conference   ( INNC - 90 ) ,   Paris ,   France ,   Kluwer   Academic   Publishers ,   Netherlands ,   1990 ,   379 ～ 382 
 9 ， Craven   M ,   Shavlik   J .   Using   Sampling   and   Queries   to   Extract   Rules   from   Trained   Neural   Networks .   In :   Proceedings   of   the   Eleventh   International   Conference   on   Machine   Learning ,   New   Brunswick ,   NJ ,   Morgan   Kaufmann ,   1994 ,   37 ～ 45 
 10 ， Thrun   S   Extracting   Rules   from   Artificial   Neural   Networks   with   Distributed   Representations .   In :   Tesauro   G ,   Touretzky   D ,   Leen   T ,   eds . ,   Advances   in   Neural   Information   Processing   Systems   ( Volume   7 ) ,   The   MIT   Press ,   Cambridge ,   MA ,   1995 
 11 ， Benitez   J ,   Castro   J ,   Requena   I .   Are   Artificial   Neural   Networks   Black   Boxes ?   IEEE   Transactions   on   Neural   Networks ,   1997 ,   Vol.8 ,   No.5 , . 1156 ～ 1164 . 
 12 ， 张朝辉 等 .   利用 神经网络 发现 分类 规则 .   计算机 学报 ,   1999 ,   Vol .   22 ,   No.1 ,   108 ～ 112 
 13 ， Igelnik   B , Pao   Y ,   The   Ensemble   Approach   to   Neural - Network   Learning   and   Generalization   IEEE   Transactions   on   Neural   Networks   , 1999 , 10 ( 1 ) : 19 ～ 30 
 14 ， Schapire   R .   The   Strength   of   Weak   Learnability .   Machine   Learning ,   1990 ,   5 :   197 ～ 227 
 收稿 日期 ： 1999 年 9 月 6 日 
