计算机 研究 与 发展 
 JOURNAL   OF   COMPUTER   RESEARCH   AND   DEVELOPMENT 
 1999 年   第 36 卷   第 5 期   Vol.36   No.5   1999 
 
 
 
 B 样条 神经网络 的 构造 理论 
 於 东军 　 王士同 
 摘 　 要 　 文中 首先 讨论 了 B 样条 基 函数 的 特性 ， 在 此基础 上 采用 构造性 的 方法 从 理论 上 证明 了 B 样条 神经网络 能够 以 任意 精度 逼近 任意 定义 在 致密 区间 上 的 连续 实 函数 . 最后 给出 了 构造性 算法 ， 使用 此 算法 ， 能 在 满足 误差 要求 的 条件 下 ， 构造 出 几乎 最小 的 B 样条 基 函数 . 
 关键词 　 B 样条 ， 神经网络 ， 构造 理论 
 中图法 分类号 　 TP18 
 CONSTRUCTIVE   THEORY   FOR   B - SPLINE   NEURAL   NETWORK 
 YU   Dong - Jun   and   WANG   Shi - Tong 
 ( Department   of   Computer   Science , East   China   Shipbuilding   Institute , Zhenjiang 　 212003 ) 
 Abstract 　 The   characteristics   of   B - spline   basis   function   is   first   discussed ,   based   on   which   B - spline   neural   network   is   proved   to   be   a   universal   approximator .   And   then   a   constructive   algorithm   is   presented .   It   is   proved   that   this   algorithm   can   be   used   to   build   a   B - spline   neural   network   with   minimum   hidden   units   to   approximate   any   continuous   function   defined   on   compact   set   to   a   prescribed   accuracy . 
 Key   words 　 B - spline ,   neural   network ,   constructive   theory 
 1 　 引 　 　 言 
 　 　 对于 一个 给定 的 非线性 函数 ， 我们 都 知道 用 神经网络 可以 以 任意 的 精度 来 逼近 它 ， 原因 就是 神经网络 是 全局 逼近 器 ［ 1 ］ . 最常 使用 的 神经网络 就是 3 层网 . 对于 3 层 神经网络 而言 ， 输入 至 输出 的 映射 实际上 是 通过 基 函数 的 加权 组合 来 实现 的 ， 当基 函数 取 不 同时 ， 网络 有 不同 的 性质 ， 并且 计算 复杂程度 不同 . 常用 的 基 函数 有 Sigmoid 、 高斯型 等等 ， 当基 函数 取 B 样条 时 ， 称此 神经网络 为 B 样条 神经网络 . 
 　 　 对于 如图 1 所示 的 3 层 神经网络 ， 输入 至 输出 的 映射 是 通过 基 函数 α i ( ) 的 加权 组合 来 实现 的 ， 即 y = ， 其中 = ( x1 , x2 , … , xn ) T . 将基 函数 取为 B 样条 函数 时 ， 称此 3 层 神经网络 为 B 样条 神经网络 . 由于 B 样条 函数 本身 具有 极好 的 性质 ［ 2 ］ （ 正 定性 ， 归一性 ， 致密 性 ） ， 因此 B 样条 神经网络 得到 了 极为 广泛 的 应用 . 
 
 
 图 1 　 B 样条 神经网络 的 拓扑 结构 
 　 　 虽然 B 样条 神经网络 有着 很多 优点 ， 但是 当 面对 一个 实际 应用 问题 时 ， 隐 节点 数目 的 选取 却是 个 问题 ， 至今 未有 理论 来 指导 . 本文 就是 旨在 解决 这个 问题 ， 给出 找到 几乎 最小 隐 节点 数目 的 算法 . 
 2 　 B 样条 基 函数 简介 
 　 　 B 样条 函数 是 工程 上 常用 的 一种 插值 函数 ， 由于 它 符合 工程 实际 需要 ， 因此 应用 范围 很广 . 
 　 　 假定 输入 向量 为 ： = ( x1 , x2 , … , xn ) T ， xi ∈ Ui ， Ui 为 有限 区间 ， 可以 定义 为 ： 
 Ui = { xi | xmini ≤ xi ≤ xmaxi } 
 　 　 对 第 i 维 输入 区间 进行 分割 ： 
 xmini < ρ i , 1 < ρ i , 2 < … < ρ i , mi < xmaxi 
 ρ i , j 称为 xi 的 第 j 个 内 插点 . 
 … < ρ i , - 1 < ρ i , 0 = xmini ;   xmaxi = ρ i , mi + 1 < ρ i , mi + 2 < … 
 称为 外 插点 . 
 　 　 （ 1 ）   单 变量 基 函数 
 　 　 设 输入 域 为 U = ［ xmin , xmax ］ ， 在 此 区间 内有 m 个 内 插点 ， 并 在 区间 之外 的 两边 分别 定义 k - 1 个外 插点 . k 为 B 样条 函数 的 阶数 . 这样 可定义 单 变量 基 函数 为 ： 
 α j ( x ) = Nkj ( x ) 
 其中 ： 
 
 　 　 图 2 表示 了 k = 3 时 的 B 样条 基 函数 . 
 
 
 图 2 　 单 变量 3 阶 B 样条 基 函数 
 这样 定义 的 B 样条 函数 有 3 条 重要 性质 ［ 2 ］ ： 
 　 　 ①   正 定性 ： Nkj ( x ) > 0 ， 对于 x ∈ ［ ρ j - k , ρ j ) , 1 ≤ j ≤ m + k . 
 　 　 ②   紧密性 ： Nkj ( x ) = 0 ， 对于 x ［ ρ j - k , ρ j ) ， 1 ≤ j ≤ m + k . 
 　 　 ③   归一性 ： ， 对于 任意 的 x ∈ U . 
 　 　 我们 把 集合 α = { Nkj ( x ) , j = 1 , 2 , … , m + k } 称为 k 阶 B 样条 基 函数 集 . 
 　 　 ( 2 )   多 变量 基 函数 
 　 　 输入 向量 为 = ( x1 , x2 , … , xn ) T ∈ Rn ， 定义 基 函数 ， Nkii , ji ( xi ) 表示 xi ( i = 1 , 2 , … , n ) 的 第 ji ( ji = 1 , 2 , … , mi + ki ) 个单 变量 基 函数 ， 且 单 变量 xi 的 基 函数 阶数 为 ki . 则 多 变量 基 函数 的 总数 为 ， 且 α j1j2 … jn ( ) 同样 满足 正 定性 ， 紧密性 ， 归一性 . 
 　 　 ①   正 定性 ： α j1j2 … jn ( ) > 0 , 对于  　 i , xi ∈ ［ ρ i , ji - mi , ρ i , ji ) , 1 ≤ ji ≤ mi + ki . 
 　 　 ②   紧密性 ： α j1j2 … jn ( ) = 0 , 对于  　 i , xi ∈ ［ ρ i , ji - mi , ρ i , ji ) , 1 ≤ ji ≤ mi + ki . 
 　 　 ③   归一性 ： ， 对于 任意 的   = ( x1 , x2 , … , xn ) T ∈ U1 × U2 × … × Un . 
 　 　 其 证明 见 文献 ［ 2 ］ . 
 3 　 B 样条 神经网络 的 构造 理论 
 　 　 在 本节 中 ， 我们 将 给出 一个 定理 ， 该 定理 表明 ： 可以 通过 构造 的 方法 得到 一个 B 样条 神经网络 ， 此 网络 能以 给定 的 精度 逼近 任意 给定 的 定义 在 致密 区间 上 的 多 变量 连续 实 函数 . 
 　 　 首先 ， 我们 引入 几个 定义 ［ 4 ］ . 设 UR 为 定义域 ， A ( x ) 为 定义 在 U 上 的 函数 . 存在 x * ∈ U ， 使得 对 任意 的 x ∈ U , 有 A ( x ) ≤ A ( x * ) ， 称 A ( x ) 为 峰值 函数 ， 并称 x * 为 峰值 点 . 
 　 　 定义 1 .   若 集合 α = { Ai ( x ) , i = 1 , 2 , … , m } 中 的 每个 Ai ( x ) 均 为 峰值 函数 ， 且 对于 任意 的 x ∈ U ， 有 成立 . 称 α 为 峰值 基 函数 集 . 
 　 　 显然 单 变量 的 B 样条 基 函数 集是 满足 定义 1 的 . 
 　 　 定义 2 .   对于 给定 的 函数 f : U → R 以及 一个 峰值 基 函数 集 α = { Ai ( x ) , i = 1 , 2 , … , m } ， 我们 定义 函数 : U → R 为 ， 其中 x * i 为 Ai ( x ) 的 峰值 点 . 称为 在 α 上 对 f 的 插值 . 
 　 　 这样 ， 当 Ai ( x ) 取 B 样条 函数 时 ， 称为 在 α 上 对 f 的 B 样条 插值 . 这时 就 对应 于 一个 B 样条 神经网络 ， 见图 3 . 
 
 
 图 3 　 单 输入 的 B 样条 神经网络 
 　 　 定义 3 .   设 α i ( xi ) = { Aij ( xi ) , j = 1 , 2 , … , mi } 为 定义 在 UiR 上 的 峰值 基 函数 集 ， i = 1 , 2 , … , n ， 那么 定义 = { A1i1 ( x1 ) A2i2 ( x2 ) … Anin ( xn ) , 1 ≤ ij ≤ mi , 1 ≤ j ≤ n , xi ∈ Ui } , 当有 …   成立 ， 则 称 其 为 定义 在 U1 × U2 × … × Un 上 的 乘积 峰值 基 函数 集 . 
 　 　 由 第 2 节 的 说明 ， 多 变量 B 样条 基 函数 符合 定义 3 ， 我们 称之为 B 样条 乘积 峰值 基 函数 集 . 
 　 　 现在 ， 可以 给出 定理 1 . 
 　 　 定理 1 .   设 f : → R 为 定义 在 致密 区间 Rn 上 的 连续 实 函数 ， 则 对于 ε > 0 ， 存在 上 的 一个 B 样条 乘积 峰值 基 函数 集 Θ = { A1i1 ( x1 ) A2i2 ( x2 ) … Anin ( xn ) } ， 使得 成立 ， 其中 ( ) 为 在 Θ 上 对 f 的 B 样条 插值 . 
 　 　 证明 .   为了 标记 上 的 简便 ， 我们 假定 输入 的 维数 为 2 ， 且 = ［ 0 , 1 ］ 2 ， 各维上 的 B 样条 基 函数 的 阶数 为 3 . 
 　 　 由于 f 在 上 是 连续 的 ， 且 是 致密 的 ， 则 f 必定 一致 连续 . 因此 对于 ε > 0 ， 必定 存在 δ > 0 ， 使得 当时 ， 有 成立 ， 其中 ( x1 , y1 ) , ( x2 , y2 ) ∈ . 
 　 　 我们 先沿 x 轴 将 区间 [ 0 , 1 ] 均匀 划分 N 个 等 份 （ 内 插点 ， 并 在 区间 两边 各找 相应 的 两个 外 插点 （ 因为 阶数 为 3 ） ， 当 N 足够 大时 ， 令 . 这样 就 可以 沿 x 轴 定义 B 样条 基 函数 集 α = { Aj ( x ) , j = 1 , 2 , … , N + 2 } ， 其中 Aj ( x ) = N3j ( x ) ， 见图 4 所示 . 
 
 
 图 4 　 沿 x 轴 定义 B 样条 基 函数 
 　 　 　 同理 ， 可以 沿 y 轴 定义 B 样条 基 函数 集 β = { Bj ( y ) , j = 1 , 2 , … , N + 2 } ， 且 Bj ( y ) = Aj ( y ) , j = 1 , 2 , … , N + 2 . 然后 可以 构造 乘积 B 样条 基 函数 
 Θ = { Cij ( x , y ) | Cij ( x , y ) = Ai ( x ) Bj ( x ) , i , j = 1 , 2 , … , N + 2 } 
 　 　 依据 定义 2 ， 并 注意 到 Ai ( x ) 的 峰值 点为 ， 则 在 Θ 上 对 f 的 B 样条 插值 可以 写为 ： ( x , y ) = . 
 　 　 对于 任意 给定 的 一点 ( x , y ) ∈ ， 我们 总 可以 将 其定 位于 ： 
 
 其中 0 ≤ i , j ≤ N - 1 . 
 　 　 又 因 Ai ( x ) , Bj ( y ) 均 为 3 阶 B 样条 函数 ， 即 每 一维 上 至多 有 3 个基 函数 不为 零 ， 这样 上 式 可以 重写 为 ： 
 
 
 
 
 
 
 
 
 
 其中 ， 利用 B 样条 函数 的 归 一性 有 
 a1 + a2 + … + a9 = ［ Ai + 1 ( x ) + Ai + 2 ( x ) + Ai + 3 ( x ) ］ ． ［ Bi + 1 ( x ) + Bi + 2 ( x ) + Bi + 3 ( x ) ］ = 1 ． 1 = 1 . 
 　 　 不失 一般性 ， 不妨 设 分别 为 上式 中 的 最大 最小值 ， 就 有 下式 成立 ： 
 且 　 　 　 　 　 
 结合 上式 就 有 ： 
 
 即 
 
 
 由于 点和点 之间 的 距离 ≤ δ ， 
 所以 
 
 即 
 
 由于 点 ( x , y ) 与 点 之间 的 距离 亦 ≤ δ ， 就 有 ： 
 
 综合 上面 两 式 ， 就 有 ： 
 
 　 　 结合点 ( x , y ) 的 任意性 ， 就 证明 了 定理 1 . 
 　 　 定理 1 从 理论 上 能够 保证 ， 对于 任意 定义 在 致密 区间 上 的 连续 实 函数 ， 存在 B 样条 基 函数 ， 用 这些 基 函数 实现 的 B 样条 神经网络 能以 任意 给定 的 精度 逼近 原函数 ， 但 并 没有 给出 如何 求得 基 函数 的 方法 ， 这样 在 实际 应用 很 容易 导致 基 函数 过多 . 本文 下面 给出 一个 算法 ， 使用 该 算法 ， 在 满足 允许 误差 的 条件 下 找出 几乎 最小 的 基 函数 集 . 
 4 　 构造 算法 
 　 　 同样 ， 为了 简明 起 见 ， 这里 的 算法 描述 的 是 二维 情形 . 
 　 　 设 f : 为 连续 实 函数 ， 给定 一个 允许 误差 ε ， 我们 可以 找到 一个 δ ， 使得 当时 ， 有 . 基于 δ ， 我们 设置 　 ， 这里 ( x ≥ 0 ) 表示 不 大于 x 的 最大 整数 . 基于 N ， 可 将 划分 为 N2 个 网格 . 
 　 　 算法 1 .   初始化 ： 基于 上述 网格 划分 ， 我们 初始化 所谓 的 极值 集合 S0 ： 
 　 　 FOR   0 < i , j < N 
 　 　 　 { 
 　 　 　 　 若 
 　 　 　 　 或   
 　 　 　 　 成立 ， 则 
 　 　 　 　 
 　 　 　 　 　 } 
 　 　 我们 用 t 表示 迭代 次数 ， Φ t ( x , y ) 表示 在 第 t 次 迭代 时 的 误差 函数 . 初始 时 ， 设置 t = 0 ， Φ 0 ( x , y ) = f ( x , y ) . 
 　 　 步骤 1 .   t = t + 1 , 并 引入 
 
 
 假定 Stx ， Sty 分别 包含 有 p , q 个 元素 ， 那么 总 可以 将 这些 元素 排序 为 ： 
 　 　 　 　 　 　 　 　 Stx 中 ： 0 ≤ x1 < x2 < … < xp ≤ 1 , ( 1 ≤ p ≤ N - 1 ) 
 　 　 　 　 　 　 　 　 Sty 中 ： 0 ≤ y1 < y2 < … < yq ≤ 1 , ( 1 ≤ q ≤ N - 1 ) 
 　 　 对于 Stx ， 我们 在 区间 [ 0 ， 1 ] 两边 分别 定义 两个 适当 的 外 插点 x - 2 , x - 1 和 xp + 2 , xp + 3 ( 例如 可取 , ， ) ， 0 作为 x0 ， 1 作为 xp + 1 . 使用 这些 插点 ， 就 可以 沿着 x 轴 定义 3 阶 B 样条 基 函数 ： 
 α t = { Ati ( x ) | Ati ( x ) = N3i ( x ) , 1 ≤ i ≤ p + 3 } 
 上式 中 N3i ( x ) 是 按 下式 递归 定义 的 ： 
 
 其中 1 ≤ i ≤ p + 3 . 
 同理 ， 可以 沿着 y 轴 定义 3 阶 B 样条 基 函数 ： 
 β t = { Btj ( y ) | Btj ( y ) = N3j ( y ) , 1 ≤ j ≤ q + 3 } 
 上式 中 的 N3j ( y ) 同样 是 递归 定义 的 ： 
 
 其中 1 ≤ j ≤ q + 3 . 
 　 　 步骤 2 .   利用 α t , β t , 为 Φ t ( x , y ) 构造 插值 t ： 
 
 x  i , y  j 分别 为 Ati ( x ) , Btj ( y ) 的 峰值 点 . 
 设置 Φ t + 1 ( x , y ) = Φ t ( x , y ) - t ( x , y ) ， 并 按 下述 方法 更新 St ： 
 FOR   0 ≤ i , j ≤ N ， 　 　 　 　 　 　 　 ( 1 ) 
 　 　 AND   
 　 　 　 　 　 　 　 　 　 　 　 　 　 
 　 　 　 　 　 　 　 　 　 　 　 　 ( 2 ) 
 　 　 OR 
 　 　 　 　 　 　 　 　 　 　 　 ( 3 ) 
 　 　 　 　 　   　 
 　 　 　 　 　 　   St ∪ Et + 1 → St + 1 ; 
 　 　 　 　 　   　 } 
 　 　 步骤 3 .   IF   Et + 1 为 空集 THEN   
 　 　 ( x , y ) = 0 ( x , y ) + 1 ( x , y ) + … + t ( x , y ) 
 结束 算法 . 
 ELSE   转 步骤 1 . 
 　 　 下面 我们 就是 要 来 证明 ， 按照 上述 算法 构造 出来 的 B 样条 神经网络 满足 给定 的 误差 要求 ε ， 也 就是 仍然 要 满足 定理 1 的 结论 . 
 　 　 定理 2 .   按照 上述 算法 构造 出来 的 B 样条 神经网络 仍然 满足 定理 1 的 结论 . 
 　 　 证明 .   由 算法 的 步骤 3 可以 知道 ， 如果 存在 点 满足 式 ( 1 ) ， 必定 满足 式 ( 2 ) 或式 ( 3 ) ， 这样 算法 才能 继续 . 否则 ， Et + 1 将 为 空集 ， 这 也 就 意味着 对于 任意 的 点 ∈ ， 均 满足 ： 
 
 　 　 又 由于 
 　 　 这 也就是说 下 式 成立 ： 
 
 　 　 对于 任意 给定 的 点 ( x , y ) ∈ ， 我们 总 可以 确定 其 范围 ： 
 
 　 　 这样 ， 当 N 充分 大时 ， 必定 有 ： 
 
 　 　 　 　 　 　 　 　 　 　 　 　 　 证毕 . 
 5 　 结 　 　 论 
 　 　 本文 采用 了 构造性 的 方法 证明 了 B 样条 神经网络 仍然 是 全局 逼近 器 ， 并且 给出 构造 几乎 最小 隐 节点 数目 的 算法 ， 这 就 从 理论 上 为 B 样条 神经网络 的 使用 提供 了 依据 ， 具有 明显 的 实际 应用 意义 . 
 本 课题 得到 国家自然科学基金 资助 ( 项目编号   69673001 ) . 
 作者简介 ： 於 东军 ， 男 ， 1974 年 10 月生 ， 硕士 研究生 ， 研究 方向 为 神经网络 、 人工智能 . 王士同 ， 男 ， 1964 年 10 月生 ， 教授 ， 英国 Bristol 大学 博士后 ， 主要 研究 方向 为 人工智能 、 神经网络 、 模糊数学 . 
 作者 单位 ： 华东 船舶工业 学院 计算机科学 系 　 镇江 　 212003 
 参考文献 
 　 1 　 张立明 . 人工神经网络 的 模型 及其 应用 . 上海 ： 复旦大学 出版社 ， 1993 
 ( Zhang   Liming .   Models   and   Applications   of   Artificial   Neural   Networks ( in   Chinese ) .   Shanghai : Fudan   University   Press , 1993 ) 
 　 2 　 孙增圻 ， 张再兴 ， 邓志东 . 智能 控制 理论 与 技术 . 北京 ： 清华大学出版社 ， 1991 
 ( Sun   Zengqi , Zhang   Zaixing , Deng   Zhidong . Intelligent   control   theory   and   its   applications . Peking : Tsinghua   University   press , 1997 ) 
 　 3 　 Lin   C   T .   Fuzzy   Neural   Networks .   England :   Prentice - Hall   Press ,   1997 
 　 4 　 Wang   Peizhuang   et   al . Constructive   theory   for   fuzzy   systems . Fuzzy   Sets   and   Systems , 1997 , 88 ( 2 ) : 180 ～ 185 
 　 5 　 胡守仁 等 . 神经网络 导论 . 长沙 ： 国防科技大学 出版社 ， 1993 
 ( Hu   Shouren   et   al .   Introduction   to   Neural   Networks ( in   Chinese ) .   Changsha : National   University   of   Defence   Technology   Press ,   1993 ) 
 原稿 收到 日期 ： 1998 - 07 - 16 
 修改稿 收到 日期 ： 1998 - 12 - 04 
