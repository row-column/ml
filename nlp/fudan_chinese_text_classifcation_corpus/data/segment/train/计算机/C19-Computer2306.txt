计算机 研究 与 发展 
 JOURNAL   OF   COMPUTER   RESEARCH   AND   DEVELOPMENT 
 1999 年   第 36 卷   第 5 期   Vol.36   No.5   1999 
 
 
 
 一种 短语 结构 规则 的 自动 获取 方法 
 朱靖波 　 张玥杰 　 姚 天顺 
 摘 　 要 　 文中 提出 一种 新 的 知识 获取 方法 ， 即 从 完全 没有 任何 标注 的 生 语料库 中 ， 采用 NA 假设 自动 构造 带标 训练 数据 ， 利用 基于 多 特征 的 相似 评估 技术 自动 获取 名词 短语 结构 规则 . 该 方法 具有 两个 特点 ： ①   由于 从 没有 任何 标注 的 生 语料库 中 自动 获取 带标 训练 数据 ， 促使 带标 数据 规模 可以 很大 ， 且 容易 构造 不同 领域 的 带标 语料库 ； ②   所 获取 的 短语 结构 规则 具有 概率 属性 ， 可 用于 分类 检索 等 应用 中 的 名词 短语 抽取 . 为 论证 方法 有效性 ， 采用 美国 Berlitz 公司 的 汽车配件 真实 语料 进行 测试 ， 前 50 个 名词 短语 结构 规则 的 准确率 高达 80% . 
 关键词 　 名词 短语 结构 规则 ， 距离 函数 ， 基于 多 特征 的 相似 评估 
 中图法 分类号 　 TP391 
 A   NEW   APPROACH   TO   PHRASE   STRUCTURE   RULE   ACQUISITION 
 ZHU   Jing - Bo ,   ZHANG   Yue - Jie ,   and   YAO   Tian - Shun 
 ( Institute   of   Computer   Science ,   School   of   Information   Science   and   Engineering ,   Northeastern   University ,   Shenyang 　 110006 ) 
 Abstract 　 Here   presented   is   a   new   approach   to   NP   phrase   structure   rule   acquisition   based   on   multi - feature   similarity   estimation   from   corpora   without   bracketed   and   nonterminal   labels .   By   computing   the   distance   between   a   rule   and   all   feature   rules   based   on   their   local   contextual   information ,   the   system   could   sort   all   rules   by   their   distances .   The   smaller   the   distance ,   the   larger   the   similarity .   Experiments   using   Berlitz   corpus   show   that   the   approach   presented   achieves   a   relatively   high   accuracy :   80%   in   the   first   50   rules .   This   result   demonstrates   that   training   data   acquisition   based   on   NA   assumption   is   effective   for   rule   acquisition   and   parsing . 
 Key   words 　 noun   phrase   structure   rule ,   distance   function ,   multi - feature - based   similarity   estimation 
 1 　 引 　 　 言 
 　 　 目前 许多 自然语言 处理 系统 采用 基于 文法 的 句子 分析 技术 ， 但是 手工 构造 文法 的 难度 大 ， 十分复杂 . 国内外 学者 提出 了 许多 基于 语料库 统计 的 文法 自动 获取 方法 . 这些 自动 获取 方法 可以 根据 它们 使用 的 语料库 特点 来 分类 ， 例如 训练 语料库 中 是否 具有 短语 边界 标注 ， 或者 非 终结 符号 标注 等等 . 国外 有 一些 具有 边界 全 标注 和 非 终结 符号 标记 的 英文 分析 语料库 ， 很多 研究 人员 利用 这些 语料库 来 构造 概率 文法 ［ 1 ～ 4 ］ . 同时 从 具有 非 终结符 标注 的 语料库 统计 文法 的 概率 ， 应用 于 评估 句子 的 分析 结果 的 概率 . 
 　 　 实际上 目前 很多 基于 统计 的 分析 技术 取得 了 很 好 的 实验 结果 ， 很大 程度 依赖于 语料库 构造 的 代价 . 为了 减轻 人工 标注 语料库 的 瓶颈 问题 ， 如何 从 更少 标注 代价 的 语料库 中 获取 更 多 、 更 深层次 的 知识 的 获取 技术 的 研究 越来越 有 意义 . 例如 利用 没有 非 终结符 标注 或者 部分 标注 的 语料库 进行 统计分析 . 国外 有 很多 语料库 只有 边界 标注 ， 没有 非 终结符 标注 ， 如 EDR 语料库 ［ 5 ］ ， ATIS 口语 语料库 ［ 6 ］ 等等 . Baker ［ 7 ］ 很 早就 利用 Inside - Outside 算法 ， 从 没有 非 终结符 标注 的 语料库 中 获取 概率 上下文 无关 文法 用于 语音 识别 ［ 8 ， 9 ］ . 但是 获取 的 概率 文法 严格 限制 为 CNF （ Chomsky   normal - form ） ， 训练 语料库 规模 不 大 ， 获取 概率 文法 的 计算 复杂度 特别 大 . Thanaruk   Theeramunkong 和 Manabu   Okumura ［ 10 ］ 从 没有 非 终结符 标注 ， 具有 边界 标注 的 语料库 中 ， 利用 聚类分析 技术 获取 概率 上下文 有关 文法 ， 进行 句法分析 . 实验 结果表明 对于 短 句子 取得 了 很 好 的 效果 . 
 　 　 针对 上述 一些 问题 ， 文中 提出 了 一种 新 的 知识 获取 方法 ， 从 完全 没有 任何 标注 的 生 语料库 中 ， 采用 NA 假设 自动 构造 带标 训练 数据 ， 利用 基于 多 特征 的 相似 评估 技术 自动 获取 名词 短语 结构 规则 . 这种 方法 具有 两个 特点 ， 首先 由于 带标 训练 数据 是从 没有 任何 标注 的 生 语料库 中 自动 获取 的 ， 这样 促使 带标 训练 数据 的 规模 可以 很大 ， 并且 很 容易 构造 不同 领域 的 带标 语料库 . 其次 ， 该 方法 获取 的 短语 结构 规则 具有 概率 属性 ， 同时 利用 局部 上下文 （ 前后 两个 词性 ） 计算 规则 的 概率分布 . 因此 短语 结构 规则 及其 概率分布 可以 用于 从 文本 中 抽取 名词 短语 用于 分类 检索 应用 等等 . 为了 论证 该 方法 的 有效性 ， 本文 采用 美国 Berlitz 公司 的 汽车配件 真实 语料 进行 测试 ， 最后 给出 实验 结果 . 
 2 　 距离 函数 
 　 　 本节 将 讨论 一些 用于 测试 不同 概率分布 相似 程度 的 函数 的 计算 特性 ， 我们 称 这些 函数 为 距离 函数 . 通过 距离 函数 计算 不同 概率分布 之间 的 距离 ， 距离 越小 ， 相似 程度 越大 . 本节 主要 讨论 两种 距离 函数 ： 基于 Kullback - Leibler ［ 11 ］ 距离 的 距离 函数 和 基于 几何 形式 的 距离 函数 . 本文 的 实验 结果 将 验证 各种 距离 函数 的 有效性 . 
 2.1 　 概率分布 评估 
 　 　 对象 o 的 条件 概率分布 Po 使用 最大 似然 估计 方法 （ maximum   likelihood   estimate ， MLE ） 来 计算 . MLE 方法 使用 训练 数据 来 评估 下面 的 条件 概率 Po = PMLE ( c | o ) ： 
 　 　 　 　 　 　 　 　 　 　 　 　 　 　 　 ( 1 ) 
 　 　 其中 R ( o , c ) 表示 对象 o 在 局部 上下文 c 条件 下 在 训练 语料 中 出现 的 次数 . 同理 R ( o ) 表示 对象 o 在 训练 语料 中 出现 的 次数 . 我们 设定 当 R ( o , c ) = 0 ， 则 PMLE ( c | o ) = 0 . 也就是说 ， 由于 受到 样本 语料 的 规模 限制 ， 一旦 出现 某一 数据 对 ( x , y ) 没有 出现 在 训练样本 语料 中 ， 我们 可能 得出 该 数据 对 的 条件 概率 PMLE ( c | o ) 为 0 . 实验 发现 ， 许多 数据 对 即使 出现 在 训练样本 语料 中 ， 计算 得出 的 结果 也 是 属于 低 概率 事件 ， 这 就是 我们 稀疏 数据 问题 . 严重 的 稀疏 数据 问题 会 导致 这种 评估 方式 成为 没有 太 多 价值 . 
 　 　 为了 解决 稀疏 数据 问题 ， 后来 许多 人 提出 了 一些 MLE 的 改进 方法 . 基本 思想 将 MLE 作为 最初 的 评估 ， 然后 将 数据 对 的 条件 概率 之 和 小于 1 ， 留出 一部分 概率 赋给 未 出现 在 样本 语料 中 的 可能 数据 对 . 这个 技术 我们 称之为 平滑 （ smoothing ） 技术 . 
 　 　 Jelinek 和 Mercer ［ 12 ］ 提出 了 著名 的 内插法 平滑 技术 . 我们 采用 简化 的 内插法 平滑 公式 ： 
 　 　 　 　 　 　 　 　 　 　 ( 2 ) 
 　 　 其中 R ( o , c ) 表示 ( o , c ) 在 训练 语料 中 出现 的 次数 ， R ( o ) 表示 对象 o 在 训练 语料 中 出现 的 次数 . | CT | 表示 局部 上下文 的 数目 . 本文 考虑 的 局部 上下文 为 对象 的 前 一个 词 的 词性 和 后 一个 词 的 词性 ， 因此 可以 得出 ： | CT | = 13 种 词性 × 13 种 词性 = 169 种 局部 上下文 . 在 本文 的 实验 中 ， 设置 插值 参数 λ 为 0.99 ， 取得 了 很 好 结果 . 
 2.2 　 基于 Kullback - Leibler 距离 的 距离 函数 
 　 　 首先 定义 o1 和 o2 为 两个 不同 的 对象 ， CT 为 局部 上下文 ， c ∈ CT 表示 某一 具体 局部 上下文 . Po1 和 Po2 分别 表示 对象 o1 和 o2 的 概率分布 . Kullback - Leibler 距离 公式 定义 如下 ： 
 　 　 　 　 　 　 　 　 ( 3 ) 
 　 　 我们 可以 利用 D ( Po1 ‖ Po2 ) + D ( Po2 ‖ Po1 ) 来 计算 两个 不同 概率分布 Po1 和 Po2 的 距离 ， 距离 函数 ξ ( Po1 , Po2 ) 如下 ： 
 　 　 　 　 ( 4 ) 
 2.3 　 基于 几何 形式 的 距离 函数 
 　 　 如果 我们 把 ( Po1 ,   Po2 , … ， Pon ) 看作 一个 向量 空间 ， 就 可以 通过 几何 方式 计算 不同 概率分布 的 距离 ， 从而 评估 它们 的 相似 程度 . 
 　 　 几何 形式 的 距离 函数 有 两种 形式 ： 
 　 　 ( 1 )   L1 形式 ， 计算公式 如下 ： 
 　 　 　 　 　 　 　 　 　 　 ( 5 ) 
 　 　 ( 2 )   L2 形式 ， 计算公式 如下 ： 
 　 　 　 ( 6 ) 
 　 　 很 明显 ， 当且 仅 当 对于 所有 c ∈ CT ， Po1 = Po2 ， L1 ( Po1 , Po2 ) = L2 ( Po1 , Po2 ) = 0 . 
 3 　 训练 数据 的 自动 构造 
 3.1 　 Nonambiguity - Ambiguity 假设 
 　 　 为了 方便 描述 这个 Nonambiguity - Ambiguity 假设 ， 下面 简称 NA 假设 . 为了 自动 抽取 带有 词性 标注 的 训练 数据 ， 本文 定义 语料库 中 具有 如下 特征 之一 的 词汇 为 无 歧义 语言 现象 ， 反之 为 具有 歧义 的 语言 现象 . 4 个 特征描述 如下 ： 
 　 　 ( 1 )   词典 中 只 具有 单一 词性 的 词汇 ， 如 英文 代词 词汇 “ we ” ； 
 　 　 ( 2 )   可以 通过 形态 分析 确定 唯一 词性 的 词汇 ， 如 英文 动词 词汇 “ bought ” ； 
 　 　 ( 3 )   可以 通过 前缀 和 后缀 分析 确定 唯一 词性 的 词汇 ， 如 英文 形容词 词汇 “ well - founded ” ； 
 　 　 ( 4 )   非 英文 词汇 ， 如 数字 、 标点符号 等 . 
 　 　 定义 1 . Ω 为 语言 现象 集合 ， Ω A 为 具有 歧义 的 语言 现象 集合 ， Ω N 为 无 歧义 的 语言 现象 集合 . 很 明显 ： Ω A ∪ Ω N   = Ω . 
 　 　 NA 假设 ： 利用 Ω N 构造 带标 训练 语料库 ， 训练 N - Gram 概率模型 的 参数 ， 完成 对 Ω A 的 消歧 处理 是 可行 的 . 
 　 　 例如 针对 两个 英文 词汇 “ I ” 和 “ report ” ， 我们 知道 ， 英文 词汇 “ I ” 只有 一种 词性 代词 ( R ) ， 所以 词汇 “ I ” ∈ Ω N ， 但是 英文 词汇 “ report ” 具有 两种 词性 ： 名词 ( N ) 和 动词 ( V ) ， 所以 词汇 “ report ” ∈ Ω A . 
 　 　 根据 NA 假设 ， 我们 可以 将 单个 词性 和 通过 简单 处理 可以 唯一 确定 词性 的 语言 现象 构成 带标 训练 语料 ， 训练 N 元 语法 模型 的 参数 . 
 3.2 　 带标 训练 数据 的 自动 构造 过程 
 　 　 我们 采用 的 源 语料 是 美国 Berlitz 公司 的 汽车配件 真实 语料 ， 未 经过 任何 词性 人工 标注 . 为了 从源 语料库 中 抽取 出 训练 数据 ， 我们 构造 了 两个 词库 ： 基本 词库 ： 具有 大约 4 万多 基本词汇 ， 每个 基本词汇 具有 词性 全集 和 专业词库 ： 具有 大约 两万多 汽车配件 专业 词汇 ， 每个 专业 词汇 具有 词性 全集 . 英文 词汇 词性 定义 ［ 13 ］ 如下 ： 
 表 1 　 英文 词汇 词性 
 
 名词 N 动词 V 形容词 A 
 副词 D 介词 P 助动词 X 
 连词 C 感叹词 E 代词 R 
 数词 M 冠词 Z 标点符号 W 
 其他 类型 O 　 　 　 　 
 
 
 　 　 为了 实现 对 英文 词汇 的 形态 分析 ， 我们 构造 了 一个 基于 规则 的 形态 分析器 . 训练 数据 的 自动 构造 过程 分为 4 个 步骤 ： 
 　 　 步骤 1 .   对非 英文 词汇 的 识别 ， 包括 数字 类型 、 标点符号 类型 和 其它 类型 ； 
 　 　 步骤 2 .   直接 查 词库 过程 ， 将 词库 中 所有 词性 作为 标注 结果 ， 查不到 标注 为 NULL ； 
 　 　 步骤 3 .   利用 通用 缩略语 和 不规则 动词 表 ， 前缀 和 后缀 规则 对 英文 词汇 进行 形态 分析 ； 
 　 　 步骤 4 .   带标 训练 数据 的 生成 . 
 　 　 下面 以 一个 句子 来 说明 处理过程 ： 
 　 　 They   reported   the   enemy   to   be   10   miles   away . 
 　 　 步骤 1 识别 结果 ： 
 　 　 They   reported   the   enemy   to   be   10 ( M )   miles   away . ( W ) 
 　 　 步骤 2 识别 结果 ： 
 　 　 They ( R )   reported ( NULL )   the ( Z )   enemy ( N )   to ( P )   be ( V , X )   10 ( M )   miles ( NULL )   away ( D ) . ( W ) 
 　 　 步骤 3 识别 结果 ： 
 　 　 They ( R )   reported ( V )   the ( Z )   enemy ( N )   to ( P )   be ( V , X )   10 ( M )   miles ( N )   away ( D ) . ( W ) 
 　 　 步骤 4 识别 结果 ：   
 表 2 　 训练 数据 自动 抽取 结果 
 
 序号 词汇 训练 数据 带标 训练 数据 
 1They   reported   the   enemy   to 　 　 R   V   Z   N   P 
 2be 　 　 NULL 
 310   miles   away . 　 　 M   N   D   W 
 
 
 3.3 　 带标 训练 数据 的 自动 抽取 结果 
 　 　 我们 采用 的 源 语料 是 美国 Berlitz 公司 的 汽车配件 真实 语料 ， 覆盖 了 汽车配件 的 使用 和 维修 内容 ， 没有 任何 标注 ， 包含 大约 4 万个 英文 句子 . 带标 数据 的 抽取 处理程序 在 P266 的 微机 ， PWindows98 环境 下 ， 处理 平均速度 大约 为 15 句 / 秒 . 带标 训练 数据 抽取 结果 统计 如下 ：   
 表 3 　 带标 训练 数据 自动 抽取 结果 统计 
 
 源 语料库 规模 大小 Ω 训练 语料库 规模 大小 Ω N 具有 歧义 的 数据 规模 大小 Ω A 
 674116402722271394 
 
 
 　 　 为了 验证 基于 NA 假设 的 训练 数据 自动 抽取 方法 有效性 ， 我们 曾经 将 自动 获取 的 带标 训练 语料库 用于 词性 自动 标注 应用 中 ， 对 两万 词次 关于 汽车配件 的 真实 语料 进行 开放性 测试 ， 准确率 达到 93% 以上 . 下文 将 利用 自动 抽取 的 带标 训练 数据 进行 训练 名词 短语 结构 规则 的 自动 获取 模型 的 参数 .   
 4 　 名词 短语 结构 规则 的 自动 获取 过程 
 　 　 最近 几年 ， 国外 有 许多 学者 利用 带 短语 边界 和 非 终结符 标注 的 语料库 进行 获取 上下文 无关 文法 ； 还有 一些 学者 利用 只有 短语 边界 标注 ， 没有 非 终结符 标注 的 语料库 进行 获取 概率 文法 . 本文 提出 的 概率 文法 获取 方法 采用 的 语料库 没有 短语 边界 、 非 终结符 和 词性 标注 ， 采用 基于 多 特征 的 相似 度 评估 技术 ， 进行 自动 获取 名词 短语 结构 规则 . 实际上 ， 本文 获取 的 名词 短语 结构 规则 是 具有 概率 属性 ， 同时 利用 了 局部 上下文 信息 （ 前后 两个 词性 ） 构造 了 名词 短语 结构 规则 的 概率分布 . 国外 有些 文献 讨论 了 局部 上下文 的 有效性 问题 ［ 10 ］ . 本文 提出 的 文法 自动 获取 过程 主要 分为 5 个 步骤 ： 
 　 　 Step1 . 首先 构造 名词 短语 结构特征 产生 式 ， 同时 计算 它们 的 概率分布 ； 
 　 　 Step2 . 构造 名词 短语 结构 产生 式 候选 集合 ， 设置 产生 式 的 右边 最大 长度 为 5 ； 
 　 　 Step3 . 计算 候选 集合 中 所有 产生 式 的 概率分布 ； 
 　 　 Step4 . 计算 候选 集合 中 所有 产生 式 的 概率分布 与 所有 特征 产生 式 的 概率分布 的 距离 ； 
 　 　 Step5 . 根据 平均 距离 对 候选 集合 中 所有 产生 式 进行 从小到大 进行 排序 . 
 4.1 　 特征 的 选取 和 构造 
 　 　 名词 短语 结构 的 特征 很多 ， 本文 的 实验 中 主要 选取 了 其中 3 个 特征 F ： ①   形容词 加 名词 可以 组成 名词 短语 ； ②   数词 加 名词 可以 组成 名词 短语 ； ③   冠词 加 名词 可以 组成 名词 短语 . 我们 可以 采用 产生 式 形式 来 描述 3 个 特征 F ， 即 ： ①   F1 ： NPA + N ； ②   F2 ： NP   M + N ； ③   F3 ： NPZ + N . 根据 第 2.1 节 论述 的 公式 ( 2 ) 分别 计算 3 个 特征 的 概率分布 ， 即 ： 
 　 　 ( 1 )   构造 Ω ( F1 ) = ( P ( c1 | F1 ) , P ( c2 | F1 ) , …   … , P ( cn | F1 ) ) ， 其中 ci ∈ CT ； 
 　 　 ( 2 )   构造 Ω ( F2 ) = ( P ( c1 | F2 ) , P ( c2 | F2 ) , …   … , P ( cn | F2 ) ) ， 其中 ci ∈ CT ； 
 　 　 ( 3 )   构造 Ω ( F3 ) = ( P ( c1 | F3 ) , P ( c2 | F3 ) , …   … , P ( cn | F3 ) ) ， 其中 ci ∈ CT ； 
 　 　 最后 构造 特征 产生 式 的 概率分布 矩阵 Ω ( F ) = ( Ω ( F1 ) ,   Ω ( F2 ) ,   Ω ( F3 ) ) . 
 4.2 　 候选 集合 的 构造 
 　 　 首先 我们 假设 任何 短语 结构 都 可以 组成 名词 短语 ， 本文 实验 中 设置 了 名词 短语 的 最大 长度 为 5 个 词汇 . 根据 这个 假设 ， 我们 很 容易 构造 名词 短语 结构 规则 的 候选 集合 φ ， 构造 过程 分为 5 步 操作 ： 
 　 　 ( 1 )   任取 1 个 词性 E ， 构造 产生 式 Pro ： NPE ， 增加 Pro ∈ φ ；   
 　 　 ( 2 )   任取 2 个 词性 E1 和 E2 ， 构造 产生 式 Pro ： NPE1   E2 ， 增加 Pro ∈ φ ； 
 　 　 ( 3 )   任取 3 个 词性 E1 ， E2 和 E3 ， 构造 产生 式 Pro ： NPE1   E2   E3 ， 增加 Pro ∈ φ ；   
 　 　 ( 4 )   任取 4 个 词性 E1 ， E2 ， E3 和 E4 ， 构造 产生 式 Pro ： NPE1   E2   E3   E4 ， 增加 Pro ∈ φ ； 
 　 　 ( 5 )   任取 5 个 词性 E1 ， E2 ， E3 ， E4 和 E5 ， 构造 产生 式 Pro ： NPE1   E2   E3   E4   E5 ， 增加 Pro ∈ φ ； 
 　 　 从 理论 上 说 ， 根据 这种 方法 生成 的 名词 短语 结构 的 数目 为 402233 （ 计算公式 为 ： 135 + 134 + 133 + 132 + 13 = 402233 ） . 实际上 在 这 庞大 数目 的 产生 式 集合 中 大量 是 不 合法 的 产生 式 . 为了 减少 不必要 的 运算量 ， 实验 中 采用 了 下列 两条 原则 进行 初步 筛选 . 筛选 原则 如下 ： 
 　 　 原则 1 . 该 产生 式 结构 在 语料库 中 至少 出现 一次 ； 
 　 　 原则 2 . 标点符号 、 连词 、 介词 、 助动词 、 感叹词 、 副词 不能 在 一个 名词 短语 本身 第一个 和 最后 一个 词汇 的 位置 出现 . 
 　 　 我们 把 经过 初步 筛选 的 产生 式 作为 候选 集合 ， 在 本文 的 实验 中 ， 候选 集合 中 产生 式 的 数目 降低 到 432 个 . 然后 根据 第 2.1 节 论述 的 公式 ( 2 ) 计算 候选 集合 中 所有 产生 式 的 概率分布 . 计算方法 如下 ： 
 　 　 任取 一个 产生 式 Proi ∈ Φ ， 构造 Ω ( Proi ) = ( P ( c1 | Proi ) , … , P ( cn | Proi ) ) ， 其中 ci ∈ CT . 
 4.3 　 基于 多 特征 的 相似性 评估 
 　 　 我们 采用 第 2 节 介绍 的 距离 函数 来 评估 候选 集合 中 产生 式 与 特征 产生 式 的 概率分布 的 相似 度 ， 距离 越小 ， 相似 度越 大 . 本文 方法 中 利用 与 所有 特征 产生 式 概率分布 之间 的 平均 距离 来 评估 相似 度 . 根据 第 2 节 介绍 的 3 个 距离 函数 式 ( 4 ) ～ ( 6 ) ， 可以 推导 出 3 个 平均 距离 DIS ( Pro , F ) 的 计算公式 . 
 　 　 采用 ξ 距离 函数 式 ( 4 ) ， 计算公式 1 ： 
 　 　 　 　 　 　 　 　 　 　 　 ( 7 ) 
 采用 L1 距离 函数 式 ( 5 ) ， 计算公式 2 ： 
 　 　 　 　 　 　 　 　 　 　 ( 8 ) 
 采用 L2 距离 函数 式 ( 6 ) ， 计算公式 3 ： 
 　 　 　 　 　 　 　 　 　 　 　 ( 9 ) 
 其中 ， 候选 产生 式 Pro ∈ Φ ， 特征 产生 式 Fi ∈ F ， 本文 实验 中 采用 了 3 个 特征 产生 式 ， 即 ： n = 3 . 
 5 　 实验 结果 
 　 　 为了 验证 该 方法 有效性 ， 我们 作 了 如下 实验 . 源 语料库 是 美国 Berlitz 公司 的 汽车配件 真实 语料 ， 覆盖 了 汽车配件 的 使用 和 维修 内容 ， 没有 任何 关于 短语 边界 、 非 终结符 和 词性 的 标注 . 在 第 3 节中 详细 论述 带标 训练 数据 的 自动 抽取 过程 ， 自动 构造 的 训练 语料库 规模 大小 为 402722 词次 . 该 实验 完成 分别 利用 3 个 平均 距离 计算公式 ( 7 ) ～ ( 9 ) 自动 获取 名词 短语 结构 产生 式 规则 . 下面 图中 描述 利用 不同 距离 计算公式 计算 的 平均 距离 与 自动 生成 的 产生 式 数目 之间 的 关系 . 
