计算机 研究 与 发展 
 JOURNAL   OF   COMPUTER   RESEARCH   AND   DEVELOPMENT 
 1999 　 Vol.36 　 No.7 　 P.788 - 793 
 
 
 
 一种 基于 信息 增益 与 费用 评价 函数 的 特征选择 准则 
 王亚东 　 郭茂祖 　 钱国良 
 摘 　 要 ： 特征选择 问题 是 机器 学习 和 模式识别 中 的 一个 重要 问题 . 然而 ， 在 实际 应用 中 ， 由于 没有 将 特征选择 与 特征提取 过程 统一 考虑 ， 只 注重 特征 本身 的 分类 性能 ， 没有 考虑 特征提取 的 费用 问题 ， 导致 识别系统 的 效率 较 低 . 文中 从 实际 应用 角度 ， 提出 一种 新 的 特征选择 准则 ， 将 特征 的 分类 性能 与 特征 的 提取 费用 统一 考虑 ， 利用 信息 增益 与 特征提取 费用 综合 评价 函数 作为 特征选择 准则 ， 并 给出 了 启发式 算法 ECFS . 将 该 算法 应用 于 实际 领域 的 学习 问题 并 与 决策树 算法 ID3 和 BP 神经网络 进行 了 比较 . 实验 结果表明 ， ECFS 在 保证 识别 精度 的 同时 ， 大大减少 了 特征提取 的 时间 消耗 ， 提高 了 识别 速度 . 
 关键词 ： 信息 增益 ， 费用 ， 特征选择 ， 决策树 
 分类号 ： TP18 
 A   FEATURE   SELECTION   CRITERION   BASED   ON   INFORMATION   
 GAIN   AND   COST   EVALUATION   FUNCTION 
 WANG   Ya - Dong 
 ( Department   of   Computer   Science   and   Engineering ,   Harbin   Institute   of   Technology ,   Harbin   150001 ) 
 GUO   Mao - Zu 
 ( Department   of   Computer   Science   and   Engineering ,   Harbin   Institute   of   Technology ,   Harbin   150001 ) 
 QIAN   Guo - Liang 
 ( Department   of   Computer   Science   and   Engineering ,   Harbin   Institute   of   Technology ,   Harbin   150001 ) 
 Abstract ： Feature   selection   is   an   important   problem   in   the   fields   of   machine   learning   and   pattern   recognition .   However ,   in   real - world   domains ,   the   fact   that   feature   selection   and   feature   extraction   are   not   considered   together   in   existing   heuristic   algorithms   leads   to   the   lower   efficiency   of   application   system .   In   this   paper ,   a   new   feature   selection   criterion   is   presented   which   considers   feature   selection   and   feature   extraction   together .   A   heuristic   algorithm   based   on   information   gain   and   cost   of   feature   extraction   evaluation   function ,   ECFS   is   also   given .   It   is   applied   to   the   learning   problem   in   real - world   domain   and   is   compared   with   ID3   and   BP   algorithms .   The   experimental   results   show   that   under   the   condition   of   ensuring   the   recognition   rate ,   ECFS   can   reduce   a   lot   of   cost   of   feature   extraction   and   improve   recognition   speed   greatly . 
 Key   words ： information   gain ,   cost ,   feature   selection ,   decision   tree ▲ 
 1 　 引言 
 　 　 特征选择 问题 是 机器 学习 和 模式识别 所 面临 的 一个 重要 问题 ［ 1 ］ . 特征选择 是 指 从 已知 一组 特征 集中 按照 某一 准则 选择 出有 很 好 的 区分 性 的 特征 子集 ， 或 按照 某一 准则 对 特征 的 分类 性能 进行 排序 ， 用于 分类器 的 优化 设计 ［ 2 ］ . 目前 的 特征选择 方法 主要 是 传统 的 模式识别 方法 ， 如类 内 、 类间 距离 度量 法 ［ 2 ］ ， 神经网络 ［ 3 ， 4 ］ 以及 一些 机器 学习 方法 等 ［ 5 ， 6 ］ . 这些 特征选择 准则 主要 集中 讨论 特征 的 可 区分 性 问题 ， 即 根据 某一 准则 选择 分类 性能 较 好 的 特征 来 进行 分类 ， 并 没有 考虑 到 特征提取 的 费用 问题 . 然而 ， 在 实际 应用 中 ， 一个 特征 的 优劣 仅用 其 分类 性能 来 衡量 是 不够 的 ， 特征提取 的 费用 也 是 一项 非常 重要 的 考虑 因素 . 因为 一般 情况 下 特征提取 费用 高 的 特征 获得 的 信息量 通常 要 多于 费用 低 的 特征 . 但是 ， 有时 费用 低 的 几个 特征 获得 的 信息量 总和 可能 多于 一个 费用 高 的 特征 获得 的 信息量 ， 并且 其 费用 开销 之 和 小于 费用 高 的 特征 . 因此 ， 确定 特征选择 准则 时 考虑 每个 特征提取 的 费用 是 十分必要 的 . 
 　 　 本文 从 实际 应用 角度 ， 提出 一种 基于 信息 增益 和 特征提取 费用 综合 评价 函数 的 特征选择 准则 . 将 特征 的 分类 性能 与 特征 的 提取 费用 统一 考虑 ， 在 识别 过程 中 特征 的 选择 与 提取 同时 进行 的 方法 ， 并 给出 了 启发式 算法 ECFS . 将 该 算法 应用 于 手写 汉字 识别系统 的 特征选择 问题 ， 并 与 决策树 学习 算法 ID3 和 BP 神经网络 算法 进行 了 比较 . 实验 结果表明 ， ECFS 在 保证 识别 精度 的 同时 ， 大大减少 了 特征提取 的 时间 消耗 ， 提高 了 识别 速度 . 
 2 　 传统 特征选择 准则 
 2.1 　 距离 准则 
 　 　 给定 一组 特征向量 集合 F = { f1 ,   f2 , … ,   fn } ， 从中 选择 具有 分类 能力 较 好 的 特征 子集 ， 或 对 特征 的 分类 能力 排序 ， 满足 类间 相似 度 尽量 小而类 内 相似 度 尽量 大 的 特性 . 依据 距离 准则 ［ 2 ］ ， 应该 使 不同 类别 之间 的 特征 均值 向量 之间 的 距离 最大 ， 而 同一 类别 内 的 特征 均值 向量 之间 的 方差 和 最小 . 假设 各个 特征向量 之间 是 统计 独立 的 ， 根据 训练样本 ， 对 F 中 的 n 个 特征向量 逐个 独立 地 分析 ， 从中 找出 m 个 最好 的 作为 分类 特征 . 例如 ， 给定 两类 训练样本 Wi 和 Wj ， 其 特征 均值 向量 分别 为 mi 和 mj ， 第 k 个 特征 分量 为 mik 和 mjk ， 相应 的 均值 方差 为 . 这里 ， 特征选择 的 准则 函数 为 . 显然 ， Gk   的 值 越 大 ， 表示 第 k 个 特征 区分 两类 Wi 和 Wj 的 性能 越好 . 将 特征向量 f1 ,   f2 ,   … ,   fn ， 按照 Gk 的 大小 排序 ， 选出 能够 完全 区分 出 训练样本 的 特征 子集 { fi ,   fj ,   … ,   fm } ， 1 ≤ i ＜ m ≤ n . 
 2.2 　 神经网络 准则 
 　 　 神经网络 是 进行 特征选择 的 一种 有效 手段 . 在 文献 ［ 3 ］ 中 给出 了 一种 “ 弱 刺激 萎缩 ” 学习 算法 用于 手写 汉字 识别 的 特征选择 . 其 基本 模型 和 算法 构造 如下 ： 
 
 　 　 　 　 
 图 1 　 神经网络 基本 模型 
 　 　 （ 1 ）   调整 输出 节点 和 输入 节点 间 的 连接 权重 的 Perceptron 收敛 算法 ； 
 　 　 （ 2 ）   完成 对 训练样本 集中 线性 可 分 样本 和 线性 不可 分 样本 自动 分类 的 “ 仿生 注意 ” 机制 ； 
 　 　 （ 3 ）   减少 输出 节点 和 输入 节点 间 不必要 的 联结 的 “ 弱 刺激 缩维 ” 法 . 
 　 　 由于 在 输入 向量 ( 抽取 的 手写 汉字 特征 集 ) 中 可能 包含 了 很多 对于 识别 过程 并 无 很大 的 影响 的 冗余 特征 ， 这些 特征 却 占用 了 大量 的 网络资源 . 采用 这种 学习 算法 能够 使得 神经元 可以 找到 输入 节点 中 兴奋 度 最高 、 数目 最少 的 一些 节点 . 这些 节点 所 对应 的 输入 特征 应当 认为 是 识别 所 需要 的 主要 特征 ， 从而 达到 特征选择 的 目的 . 
 2.3 　 决策树 准则 
 　 　 决策树 学习 算法 采用 分治 策略 ， 在 递归 构造 过程 中 ， 在 树 的 结点 上 利用 启发式 方法 进行 特征选择 . 其中 最为 著名 的 是 Quinlan 的 以 信息熵 作为 启发 函数 的 决策树 归纳 学习 算法 ID3 ［ 5 ］ ， 被 广泛应用 于 模式识别 、 专家系统 等 领域 . 
 　 　 假设 在 决策树 的 第 Ti 个 结点 上 ， 含有 的 训练样本 正 例数 和 反例 数 分别 为 T + i ， T - i , 则 该 结点 所 具有 的 信息熵 定义 如下 ： 
 
 　 　 如果 以 特征 Xk 作为 当前 结点 的 分枝 属性 ， Xk 具有 m 个值 { V1 ,   V2 , … ,   Vm } ， 它 将 Ti 分为 m 个子 结点 ， 则 有 Ti + 1 = { T ( i + 1 ) 1 ， T ( i + 1 ) 2 ， … ， T ( i + 1 ) m } . 假设 第 T ( i + 1 ) k 结点 上 含有 个 正例 和 个 反例 ， 则 子 结点 T ( i + 1 ) k 所 具有 的 信息熵 是 I ( T ( i + 1 ) k ) ， 那么 以 特征 Xk 为 分枝 属性 得到 的 子 结点 集 Ti + 1 所 具有 的 信息熵 是 ： . 因此 ， 在 第 Ti 结点 上以 特征 Xk 为 分枝 属性 的 信息 增益 是 ： Gain ( Xk ) = I ( Ti ) - I ( Ti + 1 ) . ID3 选择 使 Gain ( Xk ) 最大 的 特征 Xk 作为 分枝 属性 （ 信息 增益 最大 原则 ） . 整个 决策树 就是 通过 递归 地 利用 上述 启发式 算法 选择 不同 的 特征 作为 树 的 结点 属性 建立 起来 . 
 　 　 上述 特征选择 准则 方法 主要 集中 讨论 特征 的 可 区分 性 问题 ， 即 根据 某一 准则 选择 分类 性能 较 好 的 特征 来 进行 分类 ， 并 没有 考虑 到 特征提取 的 费用 问题 . 然而 ， 在 实际 应用 中 ， 一个 特征 的 优劣 仅用 其 分类 性能 来 衡量 是 不够 的 ， 特征提取 的 费用 也 是 一项 非常 重要 的 考虑 因素 . 正如 医生 看病 一样 ， 手术 开刀 可能 是 最 可靠 的 确认 病情 方法 ， 然而 费用 也 是 非常 昂贵 的 ， 有些 情况 完全 可以 通过 其它 几种 简单 的 途径 来 确诊 . 这 在 专家系统 、 故障诊断 和 一些 模式识别 领域 的 实际 应用 中 尤其 重要 ［ 7 ］ . 因此 ， 进行 特征选择 时 考虑 每个 特征提取 的 费用 是 十分必要 的 . 
 3 　 基于 信息 增益 与 费用 评价 函数 的 特征选择 准则 
 3.1 　 特征 抽取 费用 
 　 　 特征 抽取 是 获取 特征 用于 分类 的 一种 手段 或 算法 ［ 2 ］ . 如何 衡量 特征 抽取 的 费用 问题 对于 不同 的 学习 领域 和 实际 问题 是 不同 的 . 给定 一组 分类 特征 集合 F = { F1 , F2 , F3 , … ,   Fn } ， 获取 特征 Fi 的 费用 表示 为 Cost ( Fi ) . 根据 实际 问题 的 不同 ， Cost ( Fi ) 可以 用 时间 、 金钱 、 能量 等 不同 尺度 来 衡量 . 对于 大多数 模式识别 问题 ， 如 数字 识别 、 汉字 识别 ［ 8 ］ 等 ， Cost ( Fi ) 的 度量 大多 表现 为 特征 获取 的 时间 或 时间 复杂度 . 而 在 专家系统 、 故障诊断 等 领域 中 ［ 7 ］ ， 则 更 多 地 考虑 以 金钱 费用 作为 Cost ( Fi ) 的 度量 单位 . 
 　 　 为了 进一步 描述 特征提取 与 费用 之间 的 关系 ， 我们 首先 对 抽取 的 特征 集 和 对 特征 抽取 的 算法 集 进行 编号 ： F1 ,   F2 , … ,   Fn 和 A1 ,   A2 , … ,   Am ， ( 其中 ， m ≤ n ) . 在 这里 ， 算法 是 一个 较为 抽象 的 概念 ， 可以 看作 是 一个 程序 ， 也 可以 是 一种 操作步骤 . 对于 每 一个 算法 ， 给出 相应 的 费用函数 ， 从而 得到 费用函数 集合 C = { C1 ,   C2 , … ,   Cm } . 然后 建立 一个 特征 抽取 控制器 （ 图 2 ） ， 用于 建立 抽取 的 特征 集 与 特征 抽取 算法 集 的 对应 关系 表 . 其中 每 一个 特征 的 抽取 过程 所 对应 的 算法 都 用 一个 费用函数 进行 估价 . 特征 抽取 控制器 工作 原理 ： 根据 给定 的 不同 的 特征 编号 入口 参数 ， 激活 相应 的 算法 来 进行 特征 抽取 ， 其 费用 由 相应 的 费用函数 来 确定 . 这样 ， 对于 每 一个 特征 Fi ， 可以 利用 特征 抽取 控制器 ， 调用 相应 的 算法 Aj 来 获得 ， 并 给出 其 特征提取 的 费用 Cj ， 即 C ( Fi ) = Cj . 
 
 　 　 　 　 
 图 2 　 特征 抽取 控制器 
 3.2 　 基于 信息 增益 与 费用 评价 函数 的 特征选择 启发式 算法 ECFS 
 　 　 决策树 是 一种 非常 有效 的 归纳 学习 工具 ， 具有 训练 （ 学习 ） 速度 、 识别 速度 快 的 优点 ， 特别 适合 于 处理 大规模 数据 分类 问题 . 在 决策树 的 结点 形成 过程 中 ， 采用 的 特征选择 准则 不同 ， 形成 不同 风格 的 决策树 . 其中 最为 著名 的 是 Quinlan 的 ID3 ［ 5 ］ 及其 改进 算法 ［ 6 ］ . 然而 ， 由于 以往 的 决策树 学习 算法 在 特征选择 准则 中 ， 没有 将 特征 的 选择 与 提取 过程 统一 考虑 ， 只 注重 特征 的 分类 性能 ， 没有 考虑 到 特征提取 的 费用 问题 . 因此 ， 在 实际 应用 中 存在 效率 较 低 的 问题 . 因为 特征提取 的 费用 也 是 整个 系统 构造 过程 中 的 一项 重要 考虑 因素 . 所以 ， 进行 特征选择 时 考虑 每个 特征提取 的 费用 是 十分必要 的 . 
 　 　 本文 以 决策树 为 工具 ， 将 特征选择 与 特征提取 费用 统一 考虑 ， 建立 基于 信息 增益 与 费用 评价 函数 的 特征选择 准则 ： 
 　 　 （ 1 ）   信息 增益 大 的 特征 的 优先级 高于 信息 增益 小 的 特征 的 优先级 . 
 　 　 （ 2 ）   特征提取 费用 小 的 特征 被 选进 的 优先级 高于 特征提取 费用 大 的 特征 . 
 其中 ， 第 ( 1 ) 条 准则 是 生成 的 决策树 平均 高度 极小 化 的 优化 准则 ， 使得 在 每 一非叶 结点 进行 测试 时 ， 能 获得 关于 被 测试 例子 最大 的 类别 信息 . 从而 保证 该 非叶 结点 到达 各 后代 叶 结点 的 平均 路径 最短 . 使 生成 的 决策树 平均 深度 较 小 ， 从而 有 较 快 的 分类 速度 . 第 ( 2 ) 条 准则 通过 选择 那些 特征提取 费用 小 的 特征 ， 减少 整个 系统 运作 的 费用 ， 从而 达到 提高 系统 效率 的 目的 . 
 　 　 基于 上述 两个 准则 建立 特征选择 准则 的 启发式 评价 函数 ： F ( A , E )   =   Gain ( A ) / ( 1 + α × C ( A ) ) . 其中 Gain ( A )   表示 选择 特征 A 带来 的 信息 增益 ， 其 实际意义 与 本文 第二 部分 决策树 准则 中 的 概念 相同 ； C ( A ) 是 抽取 特征 A 的 费用 ， 根据 不同 情况 ， 可以 用 时间 或 金钱 等 度量 单位 表示 ； α 为 调控 因子 ， 根据 实际 情况 来 确定 其 取值 ， 以 调节 信息 增益 与 特征提取 费用 之间 的 平衡 关系 ， 其 值域 为 ( 0 , 1 ) 区间 上 的 实数 . 
 　 　 从 F ( A , E ) 的 定义 可以 看出 ， F ( A , E ) 表示 对于 训练 例子 集 E ， 单位 费用 内 获得 的 信息量 ； 其 取值 越大 ， 表明 选择 的 特征 越好 . 当 不 考虑 特征提取 的 费用 时 ， 即 C ( A ) = 0 或 α 非常 小时 ， F ( A , E ) 就 相当于 传统 的 信息 增益 启发 函数 ， 也 即 ID3 的 信息 增益 准则 是 本文 特征选择 准则 的 一个 特例 . 下面 我们 给出 建立 在 这种 评价 准则 上 的 特征选择 启发式 算法 ECFS ， 算法 具体 描述 如下 ： 
 　 　 算法 1 .   给定 训练 例子 集 E ， 例子 的 特征 集合 A = { A1 ,   A2 , … ,   An } ， F ( Ai , E ) 是 上面 定义 的 评价 函数 ： 
 　 　 PROCEDURE   ECFS ( E ) 
 　 　 BEGIN 
 　 　 　 　 IF 　 E 中 只 含有 正例 或 反例 ， 则 返回 叶 结点 ， 并 标记 类别 ； 
 　 　 　 　 ELSE   
 　 　 　 　 　 　 选择 一个 特征 Ak ， Ak ∈ A ， 使得 F ( Ak , E ) 最大 ； 
 　 　 　 　 　 　 For 　 Ak 的 每个 取值 Vj ， 递归 构造 子树 ECFS ( Ej ) ； Ej 中 只 包含 Ak 取值 为 Vj 的 例子 ； 
 　 　 　 　 　 　 返回 非叶 结点 及 相应 的 特征 Ak ； 
 　 　 　 　 ENDIF 
 　 　 END 
 3.3 　 特征 抽取 控制器 引入 ECFS 
 　 　 为了 进一步 将 特征选择 与 特征提取 过程 紧密结合 起来 ， 我们 在 特征选择 的 训练 过程中将 选择 的 特征 编号 引入 ECFS ， 然后 在 识别 过程 中 利用 特征 抽取 控制器 减少 特征提取 的 数量 ， 从而 达到 提高 系统 效率 的 目的 . 具体方法 如下 ： 
 　 　 ( 1 ) 训练 过程 . 首先 选择 训练样本 作为 训练 例子 集合 ， 然后 利用 特征提取 算法 抽取 每个 样本 的 所有 特征 ， 构造 一个 具有 n 维 的 特征向量 空间 （ 训练 例子 特征 集 ） . 然后 利用 启发式 算法 ECFS ， 建立 特征选择 的 决策树 ， 在 决策树 的 每个 非 叶子 结点 上 ， 引进 一个 新 的 结点 信息 ： 该 结点 上要 用 选择 的 特征 编号 ； 
 ( 2 ) 识别 过程 . 在 用 决策树 进行 识别 的 时候 ， 给定 一个 新 的 待 识别 样本 ， 根据 树 结点 的 特征 编号 信息 ， 通过 特征 抽取 控制器 ， 激活 相应 的 算法 来 抽取 与 之 对应 的 特征 . 这样 ， 通过 特征 抽取 控制器 ， ECFS 可以 做到 边 分类 边 进行 特征 的 选择 和 抽取 ， 当 需要 某个 特征 进行 分类 的 时候 ， 再 进行 该 特征 的 抽取 . 而 不必 抽取 全部 特征 ， 从而 减少 了 特征 抽取 的 数量 ， 提高 了 系统 效率 （ 因为 系统 的 许多 费用 大都 消耗 在 特征提取 上 ） . 
 3.4 　 一个 特征选择 的 例子 
 　 　 下面 我们 构造 一个 简单 的 特征选择 例子 来 说明 ECFS 的 运作 机制 （ 见表 1 ） . 
 表 1 　 一个 特征选择 的 例子 
 
 例子 x1x2 类 
 e110P 
 e201N 
 e310P 
 
 特征 费用 
 x15 
 x28 
 
 
 　 　 在 表 1 的 例子 中 ， 共有 4 个 例子 ， 分为 两类 . 用 两个 特征 x1 , x2 进行 分类 ， x1 和 x2 的 特征提取 费用 分别 为 5 和 8 . 如果 采用 决策树 算法 ID3 ［ 5 ］ ， 利用 x1 或 x2 都 可以 将 训练 例子 集 完全 区分 开 . 其 信息 增益 均 为 2 . 结果 为 { x1 = 1 → e ∈ P ;   x1 = 0 → e ∈ N ; } 或 { x2 = 0 → e ∈ P ;   x2 = 1 → e ∈ N ; } . 也就是说 ， ID3 只 考察 特征 的 分类 性能 ， 无法 区分 x1 和 x2 的 实际 应用 效率 . 而 如果 利用 ECFS ， 则 只 选择 特征 x1 作为 分类 特征 . 因为 ， 假设 α 取值 为 0.5 ， 则 F ( x1 , E ) = 0.57 ；   F ( x2 , E ) = 0.4 ； 显然 ， F ( x1 , E ) ＞   F ( x2 , E ) ， 故 ECFS 只 选择 特征 x1 作为 分类 特征 ， 结果 为 { x1 = 1 → e ∈ P ;   x1 = 0 → e ∈ N ; } . 这样 ， 在 识别 一个 新 的 例子 时 ， 只 需要 获取 特征 x1 的 取值 ， 就 可以 通过 ECFS 判别 其 所属 类别 ， 从而 节省 了 费用 . 通过 这个 简单 的 例子 可以 看出 ， 将 特征选择 和 特征提取 统一 处理 的 ECFS 算法 的 优点 . 
 　 　 利用 特征选择 准则 启发式 算法 ECFS 和 特征 抽取 控制器 ， 将 特征 的 分类 性能 与 特征提取 的 费用 统一 考虑 ， 相互 平衡 ， 相互 联系 ， 使得 整个 系统 形成 一个 有机 的 整体 .   
 4 　 实验 结果 
 　 　 本文 将 ECFS 算法 应用 于 实际 应用领域 的 手写 汉字 识别系统 ， 并 与 传统 的 决策树 学习 算法 ID3 和 神经网络 BP 算法 进行 比较 . 我们 选用 1000 个 常用 汉字 ， 由 40 个 学生 来 提供 手写 汉字 样本 . 每个 汉字 包含 20 个 不同 的 样本 ， 将 每个 字 的 前 12 个 样本 作为 训练样本 集 （ 共 12000 个 例子 ） ， 将 其余 8 个 样本 作为 识别 集 （ 共 8000 个 例子 ） . 抽取 了 横竖 笔划 特征 、 周边 特征 、 结构 划分 特征 和 特征 点 作为 分类 特征 . 抽取 的 特征 集 构成 了 一个 12 维 的 特征 空间 ： A = { A1 ,   A2 ,   A3 , … ,   A12 }   ( 见表 2 ) . 
 表 2 　 抽取 的 手写 汉字 特征 集 
 
 A1 笔划 个数   ( 1 - z ) A7 下 周边 特征 ( 0 - f ) 
 A2 长横 的 位置 ( 0 - 2 ) A8 结构 划分 特征 ( 0 - 2 ) 
 A3 长 竖 的 位置   ( 0 - 2 ) A9 左上区 的 特征 点 个数 ( 0 - 4 ) 
 A4 左 周边 特征 ( 0 - f ) A10 右下区 的 特征 点 个数 ( 0 - 4 ) 
 A5 右 周边 特征 ( 0 - f ) A11 左下区 的 特征 点 个数 ( 0 - 4 ) 
 A6 上 周边 特征 ( 0 - f ) A12 右 上区 的 特征 点 个数 ( 0 - 4 ) 
 
 
 　 　 这里 ， 本文 采用 时间 来 定义 特征 抽取 算法 的 费用 . 现将 启发 函数 F ( A , E ) 在 手写 汉字 识别 中 的 应用 意义 说明 如下 ： F ( A , E ) = Gain ( A ) / ( 1 + α × C ( A ) ) ， 其中 Gain ( A )   是 选择 特征 A 带来 的 信息 增益 ， C ( A ) 表示 抽取 特征 A 的 实际 时间 消耗 ， α 为 调控 因子 ， 在 本文 试验 中 取值 为 0.45 ； F ( A , E ) 表示 对于 训练 例子 集 E ， 单位 时间 内 获得 的 信息量 . 在 486 微机 的 实验 结果 比较 如表 3 所示 .   
 表 3 　 ECFS 与 ID3 、 BP 算法 的 比较 结果 
 
 　 BP 算法 ID3ECFS 
 训练 时间 260   h2   h2   h 
 识别 速度 14   字 / s12   字 / s21   字 / s 
 识别 精度 77.85% 76.32% 75.24% 
 
 
 　 　 从表 3 的 试验 结果 可以 发现 ， 利用 ECFS 算法 构造 的 特征选择 决策树 ， 在 识别 过程 中 特征提取 与 识别 交替 进行 的 方法 （ 只 提取 识别 所 需要 的 特征 ） 是 非常 有效 的 . ECFS 继承 了 ID3 算法 学习 、 识别 速度 快 的 优点 ， 在 识别率 上 与 ID3 相差不多 的 同时 ， 识别 速度 有 很大 的 提高 ， 提高 了 系统 效率 . 
 　 　 另外 ， 我们 还 发现 ， ECFS 训练 后 的 决策树 平均 树高 只有 8 （ 最大 高度 为 12 ） . 也就是说 ， 在 识别 的 过程 中 ， 对于 每个 要 识别 的 汉字 ， 所 需 特征 平均 只有 8 个 即可 , 减少 了 特征提取 的 数量 , 提高 了 识别 速度 . 同时 ， ECFS 对于 不同 的 汉字 ， 所 使用 的 特征 有 较大 不同 . 其中 ， 对于 复杂 的 汉字 ， 周边 特征 使用 的 频率 很 高 （ 87% ） ； 而 对于 一般 字 ， 结构特征 和 分区 特征 点 的 使用 频率 较 高 （ 82% ） ； 在 简单 汉字 的 识别 上 ， 笔划 特征 使用 相对 要 多一些 （ 78% ） . 可以 看出 ， ECFS 在 识别 不同 复杂度 的 汉字 时 ， 能够 有效 地 选择 不同 的 特征 .   
 5 　 结论 
 　 　 本文 从 实际 应用 角度 ， 将 特征 的 分类 性能 与 特征 的 提取 费用 统一 考虑 ， 提出 一种 基于 信息 增益 和 特征提取 费用 综合 评价 函数 的 特征选择 准则 ， 在 识别 过程 中 特征 的 选择 与 提取 同时 进行 的 方法 ， 并 给出 了 启发式 算法 ECFS . 将 该 算法 应用 于 手写 汉字 识别系统 的 特征选择 问题 . 实验 结果表明 ， ECFS 在 保证 识别 精度 的 同时 ， 大大减少 了 特征提取 的 时间 消耗 ， 提高 了 识别 速度 . ■ 
 基金项目 ： 本 课题 得到 国家自然科学基金 、 国际 合作项目 彩色 匹配 、 哈工大 校管 基金 资助 . 
 作者简介 ： 王亚东 ， 男 ， 1964 年生 ， 副教授 ， 主要 研究 领域 为 专家系统 、 彩色 匹配 . 
 　 　 　 　 　 郭茂祖 ， 男 ， 1966 年生 ， 副教授 ， 博士 ， 主要 研究 领域 为 机器 学习 、 彩色 匹配 、 非 　 　 　 　 　 　 数值 并行算法 . 
 　 　 　 　 　 钱国良 ， 男 ， 1971 年生 ， 博士生 ， 主要 研究 领域 为 机器 学习 、 彩色 匹配 、 模式识别 . 
 作者 单位 ： 王亚东 ( 哈尔滨工业大学 计算机科学 与 工程系 　 哈尔滨 　 150001 ) 
 　 　 　 　 　 郭茂祖 ( 哈尔滨工业大学 计算机科学 与 工程系 　 哈尔滨 　 150001 ) 
 　 　 　 　 　 钱国良 ( 哈尔滨工业大学 计算机科学 与 工程系 　 哈尔滨 　 150001 ) 
 参考文献 ： 
 ［ 1 ］ 陈彬 ， 洪家 荣 ， 王亚东 . 最优 特征 子集 选择 问题 ， 计算机 学报 ， 1997 ,   20 ( 2 ) :   133 ～ 138 
 　 　 　 ( Chen   Bin ,   Hong   Jiarong ,   Wang   Yadong .   The   problem   of   finding   optimal   subset   of   features .   Chinese   Journal   of   Computers ( in   Chinese ) ,   1997 ,   20 ( 2 ) :   133 ～ 138 ) 
 ［ 2 ］ 蔡 元龙 .   模式识别 .   西安 :   西安电子科技大学 出版社 ， 1992 ,   104 ～ 120 
 　 　 　 ( Cai   Yuanlong .   Pattern   Recognition ( in   Chinese ) .   Xi ' an :   Xi ' an   University   of   Electronic   Science   and   Technology   Press .   1992 ,   104 ～ 120 ) 
 ［ 3 ］ 刘迎建 ， 戴汝为 ， 张立 清 . 基于 神经网络 的 手写 汉字 特征选择 ， 模式识别 与 人工智能 ， 1992 ,   5 ( 1 ) :   37 ～ 43 
 　 　 　 ( Liu   Yingjian ,   Dai   Ruwei ,   Zhang   Liqing .   Feature   selection   of   handwritten   Chinese   character   based   on   neural   network .   Pattern   Recognition   and   Artificial   Intelligence ( in   Chinese ) .   1992 ,   5 ( 1 ) :   37 ～ 43 ) 
 ［ 4 ］ 权光 日 ， 崔明根 ， 张 朝晖 ， 洪家 荣 . 基于 Hopfield - Tank 模型 的 变 参数 神经网络 方法 ， 电子学 报 ，   1996 ,   24 ( 8 ) :   78 ～ 81 
 　 　 　 ( Quan   Guangri ,   Cui   Minggen ,   Zhang   Zhaohui ,   Hong   Jiarong .   A   variant   parameter   network   method   based   on   Hopfield - Tank   model .   Acta   Electronica   Sinica .   1996 ,   24 ( 8 ) :   78 ～ 81 ) 
 ［ 5 ］ Quinlan   J   R .   Induction   of   decision   trees .   Machine   Learning ,   1986 ,   1 ( 1 ) :   81 ～ 106 
 ［ 6 ］ 洪家 荣 ， 丁明峰 ， 李星原 ， 王丽薇 . 一种 新 的 决策树 归纳 学习 算法 ， 计算机 学报 ， 1995 ,   18 ( 6 ) :   18 ～ 21 
 　 　 　 ( Hong   Jiarong ,   Ding   Mingfeng ,   Li   Xingyuan ,   Wang   Liwei .   A   new   algorithm   of   decision   tree   induction .   Chinese   Journal   of   Computers ( in   Chinese ) ,   1995 ,   18 ( 6 ) :   18 ～ 21 ) 
 ［ 7 ］ Pazzani   M ,   Merz   C ,   Murphy   P   et   al .   Reducing   misclassification   costs .   In :   Proceedings   of   the   11th   International   Conference   of   Machine   Learning ,   New   Brunswick :   Morgan   Kaufmann ,   1994 ,   217 ～ 225 
 ［ 8 ］ 钱国良 . 基于 机器 学习 的 手写 汉字 识别 的 研究 与 实现 ［ 硕士论文 ］ ， 哈尔滨工业大学 ， 哈尔滨 ， 1995 
 　 　 　 ( Qian   Guoliang .   Research   and   implementation   of   handwritten   Chinese   character   recognition   based   on   machine   learning ［ master   dissertation ］ ( in   Chinese ) .   Harbin   Institute   of   Technology ,   Harbin   1995 ) 
 收稿 日期 ： 1998 - 10 - 06 
 修稿 日期 ： 1999 - 04 - 06 
