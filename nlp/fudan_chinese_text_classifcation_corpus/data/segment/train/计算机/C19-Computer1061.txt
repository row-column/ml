自动化 学报 
 ACTA   AUTOMATICA   SINICA 
 1998 年   第 24 卷   第 5 期     Vol.24     No.5   1998 
 
 
 
 基于 Q 学习 算法 和 BP 神经网络 的 
 倒立 摆 控制 1 ) 
 蒋国飞 　 　 吴沧浦 
 摘 　 要 　 Q 学习 是 Watkins ［ 1 ］ 提出 的 求解 信息 不 完全 马尔可夫 决策问题 的 一种 强化 学习 方法 . 将 Q 学习 算法 和 BP 神经网络 有效 结合 ， 实现 了 状态 未 离散 化 的 倒立 摆 的 无 模型 学习 控制 . 仿真 表明 ： 该 方法 不仅 能 成功 解决 确定 和 随机 倒立 摆 模型 的 平衡 控制 ， 而且 和 Anderson ［ 2 ］ 的 AHC   ( Adaptive   Heuristic   Critic ) 等 方法 相比 ， 具有 更好 的 学习效果 . 
 关键词 　 Q 学习 ， BP 网络 ， 学习 控制 ， 倒立 摆 系统 ， 高斯 噪声 . 
 LEARNING   TO   CONTROL   AN   INVERTED   PENDULUM   USING 
 Q - LEARNING   AND   NEURAL   NETWORKS 
 JIANG   GUOFEI 　 WU   CANGPU 
 ( Department   of   Automatic   Control ,   Beijing   Institute   of   Technology ,   Beijing   100081 ) 
 Abstract 　 Q - learning   is   a   reinforcement   learning   method   to   solve   Markovian   decision   problems   with   incomplete   information .   This   paper   presents   a   novel   method   to   control   an   inverted   pendulum   with   unquantized   states   by   using   Q - learning   and   neural   networks .   Simulation   results   are   included   to   show   that   the   new   method   can   not   only   balance   the   determined   or   stochastic   inverted   pendulums   successfully   but   also   lead   to   a   better   effect   of   learning   when   compared   with   Anderson ' s   AHC   method . 
 Key   words 　 Q - Learning ,   BP   neural   network ,   learning   control ,   inverted   pendulum ,   Gaussian   noise . 
 1 　 引言 
 　 　 在 各种 非线性 系统 中 ， 倒立 摆是 一个 十分 典型 的 例子 . 用 强化 学习 方法 来 实现 倒立 摆 的 平衡 控制 ， 迄今 已经 取得 了 不少 成果 . 1983 年 Barto 等 人 ［ 3 ］ 设计 了 两个 单层 神经网络 ， 采用 AHC ( Adaptive   Heuristic   Critic ) 学习 算法 实现 了 状态 离散 化 的 倒立 摆 控制 . 1989 年 ， Anderson ［ 2 ］ 进一步 用 两个 双层 神经网络 和 AHC 方法 实现 了 状态 未 离散 化 的 倒立 摆 的 平衡 控制 . 最近 ， Peng ［ 4 ］ 通过 将 状态 离散 化 成为 162 个 区域 ， 用 Lookup 表 表示 Q 值 的 方法 实现 了 基于 Q 学习 算法 的 倒立 摆 的 平衡 控制 . 然而 在 那些 有 连续 状态 的 问题 中 ， 如果 采用 离散 化 这些 连续 量 再 用 Lookup 表来 表示 的 方法 ， 则 Q 学习 算法 和 常规 动态 规划 方法 一样 ， 存在 状态变量 的 空间 复杂性 问题 ， 即 所谓 的 维数灾 问题 . 解决 方法 之一 是 用 参数 化 的 结构 来 表示 Q 值 ， 如 低阶 多项式 、 决策树 等 . 本文 通过 训练 BP 网络 来 逼近 Q 值 函数 并 利用 BP 网络 的 泛化 能力 ， 实现 了 基于 Q 学习 算法 的 状态 未 离散 化 的 确定 和 随机 倒立 摆 的 无 模型 学习 控制 . 本文 的 目的 在于 用 倒立 摆 控制 问题 来 证实 ： 用 Q 学习 和 神经网络 结合 的 方法 去 实现 某些 状态 连续 控制系统 的 无 模型 控制 的 可行性 . 
 2 　 Q 学习 
 　 　 在 介绍 Q 学习 方法 前 ， 先 简述 有限 马氏 决策问题 的 模型 ； 在 每个 时间 步 k = 1 , 2 , … , 控制器 观察 马氏 过程 的 状态 为 xk ， 选择 决策 ak ， 收到 即时 报酬 rk ， 并 使 系统 转移 到 下 一个 状态 yk ， 转移 概率 为 Pxkyk ( ak ) . 控制 的 目的 是 寻求 一个 最优控制 策略 ， 使 未来 每个 时间 步所获 报酬 的 折扣 和 的 期望 最大 ， 即 极大 化 ， 其中 0  γ < 1 为 折扣 因子 . 
 　 　 给定 一个 策略 π ， 定义 Q 值为 
 　 ( 1 ) 
 其中 
 　 　 换言之 ， Q 值 即 在 状态 x 执行 控制 a 及 后续 策略 π 的 报酬 折扣 和 的 期望 . Q 学习 的 目的 就是 在 转移 概率 和 所 获 报酬 未知 的 情况 下来 估计 最优 策略 的 Q 值 . 为 方便 起 见 ， 定义 Q * ( x , a ) ≡ Q π * ( x , a ) , x , a . 其中 π * 表示 最优 策略 . 
 　 　 在线 Q 学习 方法 实现 如下 ： 在 每个 时间 步 k ， 观察 当前 状态 xk ， 选择 和 执行 控制 ak ， 再 观察 后继 状态 yk 及 接受 即时 报酬 rk ， 然后 根据 下 式 调整 Qk - 1 值 ： 
 　 　 ( 2 ) 
 其中 β k 为 学习 因子 ， 
 　 　 Watkins ［ 1 ］ 证明 了 学习 因子 序列 ｛ β k ｝ 在 满足 一定 的 条件 下 ， 如果 任 一个 ( x , a ) 二元 组能 用 等式 ( 2 ) 进行 无穷 多次 迭代 ， 则 当 k → ∞ 时 ， Qk ( x , a ) 以 概率 1 收敛 于 Q * ( x , a ) . 
 3 　 倒立 摆 系统 
 　 　 倒立 摆 系统 是 指图 1 所示 的 非线性 系统 . 小车 可以 自由 地 在 限定 的 轨道 上 左右 移动 . 小车 上 的 倒立 摆 一端 被 铰链 在 小车 顶部 . 另一端 可以 在 小车 轨道 所在 的 垂直 平面 上 自由 转动 . 控制 的 目的 在于 通过 推动 小车 向 左 或 向 右 移动 ， 使 倒立 摆 平衡 并 保持 小车 不 和 轨道 两端 相撞 . 
 
 
 图 1 　 倒立 摆 系统 
 　 　 一般 情况 下 ， 倒立 摆 系统 有 四个 状态变量 
 　 　 x ： 小车 在 轨道 上 的 位置 ； 　 　 　 　 　 θ ： 倒立 摆 偏离 垂直 方向 的 角度 ； 
 　 　 ： 小车 的 运动 速度 ； 　 　 　 　 　 　 　 ： 倒立 摆 的 角速度 . 
 倒立 摆 系统 可以 用 以下 运动 方程 来 描述 
 　 　 ( 3 ) 
 　 　 ( 4 ) 
 其中 g = 9.8 m / s2 ， 重力 加速度 ； mc = 1.0 kg ， 小车 质量 ； m = 0.1 kg ， 倒立 摆 质量 ； l = 0.5 m ， 倒立 摆 的 一半 长度 ； μ c = 0.000   5 ， 小车 和 轨道 的 摩擦系数 ； μ p = 0.000   002 ， 倒立 摆 和 小车 的 摩擦系数 ； Ft = ± 10.0 N ， 在 时刻 t 作用 于 小车 质心 的 力 . 小车 轨道 长度 为 4.8 米 . 通过 Euler 方法 数值 近似 ， 可用 以下 差分 方程 来 仿真 倒立 摆 系统 
 x ( t + 1 ) = x ( t ) + τ ( t ) , 　 　 　 ( 5 ) 
 ( t + 1 ) = ( t ) + τ ( t ) , 　 　 ( 6 ) 
 θ ( t + 1 ) = θ ( t ) + τ ( t ) , 　 　 ( 7 ) 
 ( t + 1 ) = ( t ) + τ ( t ) . 　 　 ( 8 ) 
 时间 步 τ 一般 设为 0.02 秒 . 显然 以上 给出 的 倒立 摆 系统 是 一个 确定性 系统 . 本文 为了 说明 基于 Q 学习 和 神经网络 的 方法 同样 适用 于 连续 随机 系统 的 天 模型 控制 ， 在 以上 确定性 倒立 摆 模型 中 引入 一个 噪声 信号 来 构成 一个 随机 倒立 摆 模型 ， 即 在 仿真 中 用 以下 方程 来 代替 方程 ( 6 ) . 
 ( t + 1 ) = ( t ) + τ ( t ) + ξ ( μ , σ 2 ) , 　 　 ( 9 ) 
 其中 ξ ( μ , σ 2 ) 为 高斯 噪声 . 
 4 　 基于 Q 学习 和 BP 网络 的 倒立 摆 控制 
 　 　 和 其他 实现 倒立 摆 控制 的 方法 不同 ， 在 强化 学习 方法 中 ， 控制器 唯一 能 从 环境 得到 的 反馈 是 当 倒立 摆 偏离 垂直 方向 的 角度 超出 ± 12 ° 或 小车 在 ± 2.4 米处 和 轨道 两端 相撞 时 环境 给出 的 一个 失败 信号 . 因此 本文 定义 即时 报酬 rt 为 
 
 由于 控制器 是 在 执行 了 一系列 决策 后 才 得到 这个 延迟 的 失败 信号 ， 则 控制器 必须 解决 奖励 或 惩罚 随 时间 分配 的 问题 ， 即 确定 在 这 过程 中 哪些 决策 应该 对 最后 的 失败 负责 . 实际上 Q 学习 算法 是 在 各 时间 步 Q 值 的 更新 迭代 中将 这 失败 信号 进行 反传 并 根据 Q 值来 确定 相应 决策 的 优劣 . 本文 由于 设 倒立 摆 平衡 失败 时 的 即时 报酬 为 负 ( rt = - 1 ) ， 因此 对应 Q 值较 小 的 决策 就 更 有 可能 导致 倒立 摆 系统 的 平衡 失败 . 同时 在 实现 状态 未 离散 化 的 倒立 摆 控制 时 ， 控制器 还 涉及 状态 空间 很大 时 Q 值 函数 的 泛化 问题 ( 也 叫 奖励 或 惩罚 随 结构 分配 问题 ) . 
 　 　 本文 提出 的 基于 Q 学习 和 BP 网络 的 状态 未 离散 化 倒立 摆 控制系统 的 结构 如图 2 所示 . 为 方便 起 见 ， 在 图 中 定义 状态 X = ( x , , θ , ) T . BP 网络 的 输入 为 状态 X 和 决策 a ， 输出 为 Q ( X , a , W ) ， 其中 W 为 网络 权重 . 整个 控制系统 工作 如下 ： 在 每个 时间 步 k ， 观测 倒立 摆 的 当前 状态 为 Xk ， 根据 BP 网 的 实际 输出 Q ( Xk , a , Wk ) 值 并 按照 某种 探索 策略 来 选择 当前 决策 ak . 然后 观测 倒立 摆 的 后继 状态 Yk + 1 并 检测 是否 有 失败 信号 ( 确定 即时 报酬 rk ) . 系统 再 根据 ( 10 ) 式 更新 二元 组 ( Xk , ak ) 的 Q 值 ， 然后 利用 误差 信号 e = Q ( Xk , ak ) - Q ( Xk , ak , Wk ) 更新 BP 网 的 权重 Wk 为 Wk + 1 ， 使 BP 网 实际 输出 逼近 更新 后 的 理想 输出 Q ( Xk , ak ) ， 然后 再 转到 状态 Yk + 1 继续 以上 的 过程 . 由于 未 对 状态 空间 离散 化 ， 在 系统 中 利用 了 BP 网 的 泛化 能力 来 求解 未曾 训练 过 的 状态 - 决策 二元 组 的 Q 值 . 另外 ， 在 BP 网络 的 权重 学习 中 ， 对 任一 状态 和 决策 所 对应 的 Q 值 进行 逼近 都 可能 会 影响 该 状态 和 另 一 控制 所 对应 的 Q 值 ， 所以 图 2 中 所示 的 BP 网 实际上 可以 用 两个 BP 网来 代替 ( 每个 控制 一个 ) ， 这样 可以 期望 得到 更好 的 学习效果 
 . 　 　 ( 10 ) 
 
 
 图 2 　 基于 Q 学习 和 BP 网络 的 状态 未 离散 化 倒立 摆 控制系统 的 结构图 
 5 　 仿真 及 结果 
 　 　 如上节 所述 ， 在 实际 仿真 中 ， 采用 了 两个 BP 网络 . 每个 网络 分 三层 ， 输入 层 和 隐层 各有 五个 结点 ， 输出 层有 一个 结点 . 对 BP 网络 的 实际 输入 进行 了 标准化 ， 使 其 分布 在 ［ - 1.0 ， 1.0 ］ 之间 . Q 学习 算法 的 学习 因子 β = 0.2 ， 折扣 因子 γ = 0.95 . 在 随机 倒立 摆 模型 的 仿真 中 ， 高斯 噪声 ξ ( μ , σ 2 ) 的 均值 μ = 0 ， 标准差 σ = 0.1 ( 一般 ( t ) 的 值 在 ( - 1.5 ， 1.5 ) 之间 ) . 在 倒立 摆 控制系统 中 可行 控制 只有 两个 ( 左 推 或 右 推 ) ， 因此 本文 直接 选择 对应 Q 值 较大 的 控制 为 当前 控制 . 将 随机 发生器 的 种子 和 BP 网 初始 权重 设 为 不同 值 ， 对 确定性 和 随机 倒立 摆 模型 各 做 十次 试验 ， 每次 试验 当 倒立 摆 的 试探 次数 ( 失败 次数 ) 超过 100   00 次 或 一次 试探 的 平衡 步数 超过 500   000 步时 ， 中止 倒立 摆 的 学习 并 重新 开始 另 一次 试验 . 在 仿真 中 ， 如果 倒立 摆在 一次 试探 中 能 保持 500   000 步 不到 ， 就 认为 本次 试验 已经 能 成功 控制 倒立 摆 平衡 了 . 在 状态 离散 化 的 倒立 摆 控制 中 ， 每次 平衡 失败 后 ， 倒立 摆 的 初始状态 一般 设在 X = 0 的 位置 . 在 仿真 中 ， 为了 保证 倒立 摆 得到 在 各种 场合 的 控制 经验 ， 在 每次 平衡 失败 后 ， 将 初始状态 复位 为 一定 范围 内 的 随机 值 . 平均 十次 试验 的 结果 ， 得到 基于 Q 学习 和 BP 网络 的 状态 未 离散 化 倒立 摆 控制 的 结果 如图 3 所示 . 
 
 
 图 3 　 各 方法 实现 状态 未 离散 化 倒立 摆 控制 的 学习曲线 
 　 　 在 同样 条件 下 ， 将 基于 Q 学习 方法 、 AHC 方法 和 随机 控制 ( 无 学习 ) 方法 实行 状态 未 离散 化 的 确定性 倒立 摆 模型 控制 的 结果 进行 比较 ， 发现 基于 Q 学习 和 BP 网络 的 方法 学习效果 最好 ， 每次 试验 在 平均 1   000 次 失败 后 就 可以 成功 控制 倒立 摆 平衡 . 而用 Anderson ［ 2 ］ 的 两层 网络 和 AHC 方法 则 大约 要 6   000 次 . 随机 控制 方法 不能 控制 倒立 摆 平衡 ， 每次 试探 最 多 只能 运行 几百步 . 基于 Q 学习 和 BP 网络 的 方法 同样 可以 实现 随机 倒立 摆 模型 的 平衡 控制 ， 但 由于 随机噪声 的 引入 增加 了 学习 难度 ， 每次 试验 平均 要 在 2   600 次 失败 后 才能 控制 倒立 摆 平衡 . 
 　 　 需要 说明 的 是 ， 尽管 基于 Q 学习 和 BP 网络 的 方法 取得 较 好 控制 效果 ， 但 作者 更 关注 的 是 验证 了 这种 方法 在 实现 某些 状态 连续 控制系统 的 无 模型 控制 的 可行性 . 实际上 ， 倒立 摆 控制 问题 只是 以上 问题 的 一个 例子 . 
 1 ) 国家自然科学基金 重点 资助 项目 . 
 作者 单位 ： ( 北京理工大学 自动控制 系 　 北京 　 100081 ) 
 参考文献 
 　 1 　 Watkins   C   J   C   H .   Learning   from   delayed   rewards ［ Ph . D . Dissertation ］ . UK : King ' s   College , 1989 
 　 2 　 Anderson   C   W .   Learning   to   control   an   inverted   pendulum   using   nerual   networks .   IEEE   Control   System   Magazine ,   1989 ,   9 ( 3 ) : 31 — 37 
 　 3 　 Barto   A   G ,   Sutton   R   S ,   Anderson   C   W .   Neuronlike   adaptive   elements   that   can   solve   difficult   learning   control   problems .   IEEE   Trans .   on   SMC ,   1983 ,   13 ( 5 ) : 834 — 846 
 　 4 　 Peng   J .   Efficient   dynamic   programming - based   learning   for   control ［ Ph . D . thesis ］ . USA : Northeastern   University , 1993 
 收稿 日期 　 1997 - 01 - 22 
