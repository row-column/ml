计算机 应用 研究 
 APPLICATION   RESEARCH   OF   COMPUTERS 
 2000 　 Vol.17 　 No.5 　 P.6 - 8 
 
 
 
 基于 字 统计 语言 模型 的 汉语 语音 识别 研究 
 吴应良 　 韦岗 　 李 海洲 
 摘     要 　 隐 马尔可夫 模型 ( HMM ) 由于 较 好 地 描述 了 语音 的 特性 ， 在 语音 识别 的 研究 中 占 主导地位 ， 基于 HMM 的 识别 算法 也 因 取得 了 较 好 的 识别 效果 而 得到 广泛应用 。 但 其 仅仅 依靠 语音 信号 的 声学 模型 来 进行 识别 处理 ， 因此 存在 着 不能 利用 语言 的 非 声学 知识 进行 识别 的 固有 缺陷 。 该文 提出 的 新 方法 将 基于 N 元 文法 ( N - gram ) 的 统计 语言 模型 应用 于 汉语 语音 识别 ， 推导 了 模型 参数 的 估值 公式 ， 并 给出 了 模型 的 训练 和 识别 算法 。 初步 实验 表明 ： 引入 统计 语言 模型 有利于 降低 识别 难度 和 改善 语音 识别 性能 。 
 关键词 　 统计 语言 模型     N - gram 文法       汉语 语音 识别 
 1     引言 
 　 　 在 语音 识别 研究 领域 中 ， 由于 隐 马尔可夫 模型 ( HMM ) 较 好 地 描述 了 语音 现象 的 特性 ， 基于 HMM 的 语音 识别 算法 也 取得 了 比较 好 的 识别 效果 而 得到 广泛 的 应用 ， 因此 HMM 成为 当前 最为 流行 的 语音 识别 模型 ， 在 语音 识别 研究 领域 中 占据 主导地位 。 另一方面 ， 虽然 HMM 取得 了 很大 的 成功 ， 将 语音 识别 的 研究 向前 推进 了 一大步 。 但 在 大字 表 ( Large   vocabulary ) 等 语音 识别系统 的 研究 过程 中 ， 人们 发现 ， 现有 方法 和 系统 的 性能 远 不能 令人满意 。 这 说明 仅仅 依靠 基于 语音 信号处理 的 对 语音 信号 建立 声学 模型 ( Acoustic   model ) 的 方法 [ 1 ～ 4 ] ， 对 语音 信号 的 描述 和 处理 能力 还 不够 精确 和 完善 ， 还有 许多 能 用于 语音 识别 的 语言 信息 还 没有 加以 利用 。 对 人类 认知 机制 的 研究 表明 ， 人类 自身 在 进行 自然 语音 识别 时 ， 不仅仅 是 用人 耳 对 语声 进行 捕捉 和 辨认 ， 同时 还 利用 了 许多 非 声学 的 信息 ， 诸如 句法 、 语义 、 语境 等 方面 的 知识 来 进一步 对 话语 做出 识别 和 理解 。 因此 ， 对 这些 高层次 的 非 声学 知识 建立 适当 的 模型 ， 结合 到 语音 识别系统 中 ， 将 有利于 提高 现有 语音 识别系统 的 识别 能力 和 性能 。 
 　 　 目前 ， 正在 研究 的 语言 模型 有 两种 ： 统计 性 语言 模型 和 确定性 语言 模型 。 而 将 基于 语料库 和 概率 统计 方法 的 统计 语言 模型 ( Statistical   Language   Modeling ) 应用 于 语音 识别 ， 成为 语音 识别 特别 是 大字 表 和 连续 语音 识别 重要 研究 方向 之一 [ ５ ～ ７ ] 。 
 　 　 本文 论述 了 在 语音 识别系统 中 引入 语言 模型 的 原理 和 实现 技术 ， 讨论 了 两个 重要 的 概念 - 语言 熵 ( Entropy   of   Language ) 和 语言 模型 的 复杂度 ( Perplexity   of   Language ) 。 在 此基础 上 ， 本文 提出 了 基于 字 的 N - gram 模型 ( Character - based   N - gram   Model ) 的 汉语 语音 识别方法 。 该 方法 既 具有 单字 Bigram 占 空间 少 的 优点 ， 又 可 充分利用 基于 词 ( Word - based ) 的 Bigram 模型 及 算法 的 优点 。 实验 表明 ， 该 方法 能 有效 降低 语音 识别 的 难度 ， 容易 实现 且 具有 较 高 的 识别率 。 
 2     语音 识别 数学模型 和 汉语 N - gram 模型 
 2.1     语音 识别 数学模型 
 　 　 设 A 是 语音 的 声学 信号 ， 对应 的 拼音串 为 A = ( a1 ,   a2 ,   ... ,   an ) 。 W 是 句子 或词 的 序列 ， W = ( W1 ,   W2 ,   ... ,   Wn ) ,   Wi = { Wij } 是 拼音 ai 所 对应 的 汉字 ( 词 ) 候选 集 ， i = 1 ,   2 ,   .. ,   n ； j = 1 ,   2 ,   ... ,   mi ， 句子 S 是从 候选 集中 得到 的 识别 结果 。 语音 识别 的 任务 就是 求取 最佳 匹配 S 。 由 最大 后验 概率 准则 有 ： 
 　 　 ( 1 ) 
 由 Bayes 公式 有 ： 
 　 　 ( 2 ) 
 由于 A 在 这里 是 不变 的 ， 故可 不予考虑 ， 因此 上式 可 写成 ： 
 　 　 ( 3 ) 
 由 ( 3 ) 式 可以 看出 ， 式 中 P ( A | W ) 是 由 声学 模型 匹配 计算 得到 的 概率 ， 而 P ( W ) 则 与 声学 信号 无关 ， 即 是 由 非 声学 模型 即 语言 模型 ( Language   Modeling ) 得来 的 。 按照 ( 2 ) 式 ， 我们 可以 给出 语音 识别系统 的 一般 概念模型 框图 ， 如图 1 所示 。 
 
 图 1 语音 识别系统 一般性 模型 框图 
 在 没有 语言 模型 的 情况 下 ， 对于 所有 的 句子 W ， 其 P ( W ) 相同 ， 这时 有 ： 
 　 　 ( 4 ) 
 这 就是 仅用 声学 模型 进行 语音 信号 匹配 时 的 语音 识别 。 下面 仅 讨论 语音 识别 中 的 统计 语言 模型 。 
 　 　 为了 定量 刻划 语言 模型 的 特征 ， 下面 应用 信息论 讨论 自然语言 和 语言 模型 的 有关 概念 和 定义 ： 自然语言 和 语言 模型 的 熵 和 复杂度 。 
 　 　 根据 信息论 的 观点 ， 我们 可以 把 语言 看 成为 信息源 ， 其 输出 的 句子 可以 看作 是 词串 ， 由 n 个 词 组成 ， 即 W = ( W1 ,   W2 ,   ... ,   Wn ) ， 又 设 V 为 词汇表 ( Vocabulary ) 或 语料库 ( Corpus ) 中词 的 个数 为 V 。 
 　 　 我们 给出 如下 定义 ： 
 　 　 1 ) 语言 的 熵 ( LP ) 
 　 　 取 每个 词 的 熵 为 自然语言 的 熵 ， 记为 LP ， 即 ： 
 　 　 ( 5 ) 
 　 　 2 ) 语言 的 复杂度 ( PP ) 
 　 　 我们 定义 自然语言 的 复杂度 为 ： 
 　 　 ( 6 ) 
 　 　 下面 讨论 和 分析 语言 模型 的 熵 和 复杂度 及其 性质 。 
 　 　 3 ) 语言 模型 的 熵 ( LMP ) 
 　 　 对于 有 意义 的 句子 ， 句子 中词 与 词 之间 是 相关 的 ， 因此 考虑 到 这 一 相关性 ， 由 ( 5 ) 式 可以 得出 ： 
 　 　 ( 7 ) 
 　 　 式 ( 7 ) 中 各 条件 熵 从 概率 上 反映 了 词 与 词 之间 的 相关 信息 或 搭配 关系 ， 我们 将 它 定义 为 语言 模型 的 熵 。 在 不同 的 约束条件 下 ， 可以 得到 不同 的 简化 的 语言 模型 ， 因而 由 ( 7 ) 式 可 得到 不同 简化 结果 。 
 　 　 ① 只有 声学 模型 的 、 孤立 词 、 大字 表 语音 识别系统 
 这时 ， V 个 词 被 看成 等 概率 出现 ， 句子 这词 与 词 之间 互不 相关 ， 因此 由 ( 7 ) 式 有 ： 
 　 　 ( 8 ) 
 　 　 ② 孤立 词 大字 表 语音 识别系统 
 　 　 这时 ， 句子 中词 与 词 互不 相关 ， 但 词非 等 概率 出现 ， 因此 由 ( 7 ) 式 有 ： 
 　 　 ( 9 ) 
 　 　 4 ) 语言 模型 的 复杂度 ( 记为 LPP ) 
 　 　 类似 于 自然语言 复杂度 的 定义 ， 我们 定义语言 模型 的 复杂度 为 ： 
 　 　     ( 10 ) 
 　 　 对应 于 上述 第 3 点 的 两种 情况 下 相应 简化 语言 模型 的 复杂度 分别 是 ： 
 　 　     ( 11 ) 
 　 　     ( 12 ) 
 　 　 由 概率论 和 上述 有关 定义 ， 可以 推导 出 上述 熵 和 复杂度 的 性质 如下 ： 
 　 　 性质 1 ： 
 　 　     ( 13 ) 
 　 　 性质 2 ： 
 　 　     ( 14 ) 
 2.2     汉语 N - gram 模型 
 　 　 公式 ( 7 ) 中 的 条件 熵 反映 了 语言 中词 与 词 之间 的 搭配 关系 ， 需要 我们 通过 建立 语言 模型 来 求取 。 但 自然语言 的 丰富性 使 语言 的 建模 存在 很大 困难 。 在 实际 处理 中 ， 为了 得到 切实可行 的 语言 模型 ， 我们 必须 对 理论 上 的 语言 模型 做出 某些 简化 性 的 假设 ， 这 就是 语言 模型 实现 技术 的 研究 任务 。 
 　 　 近几年来 ， 基于 大规模 语料 统计 的 Markov 语言 模型 方法 在 语音 识别 方面 取得 了 较大 的 成功 [ ５ ～ ７ ] ， 其 克服 了 传统 的 基于 规则 的 语音 识别方法 的 缺点 ， 具有 可以 处理 大规模 真实 文本 、 处理 简单 、 速度 快 等 优点 。 N - gram 模型 就是 统计 语言 模型 的 实现 技术 模型 之一 。 下面 讨论 汉语 的 N - gram 模型 。 
 　 　 从 ( 3 ) 式 和 图 1 可以 看出 ， 语言 模型 的 实现 归结为 估计 概率 P ( W ) 。 根据 概率 公式 ， 概率 P ( W ) 可以 写成 ： 
 　 　     ( 15 ) 
 　 　 式 ( 15 ) 表明 ： 第 i 个 词 Wi 出现 的 概率 与 整个 上下文 W1 ， W2 ， ... , Wi - 1 即 历史 有关 。 显然 ， 在 实际 应用 中 计算 ( 15 ) 式 中 的 条件 概率 是 不 可能 的 ， 因为 当 n 很大 时 ， 其是 一个 NP 问题 。 因此 在 实际 应用 中 ， 我们 假设 自然语言 是 一个 Markov 链 ， 即 一句 话 中 的 某个 字 只 与其 前面 N - 1 个 最近 的 字 有关 。 这样 式 ( 15 ) 可 写成 ： 
 　 　     ( 16 ) 
 　 　 式 ( 16 ) 可 从 大规模 语料 统计 中 得到 。 这种 利用 前 N - 1 个 汉字 来 推测 当前 这个 汉字 的 Markov 模型 称为 N - gram ( N 元 文法 ) 模型 。 当 N = 1 ， 2 ， 3 时 ， 分别 称为 Unigram 模型 、 Bigram 模型 和 Trigram 模型 。 如果 统计 的 单元 是 单字 ， 则 称为 基于 字 N - gram 模型 。 在 实际 应用 中 ， N 较 小时 ( N ≤ 3 ) ， N - gram 模型 才 比较 切实可行 。 
 　 　 式 ( 16 ) 式 可以 通过 Viterbi 算法 求 出 。 
 3     训练 和 识别 算法 
 　 　 下面 我们 讨论 基于 单字 统计 的 Bigram 语言 模型 的 训练 和 识别 算法 。 
 3.1     训练 算法 
 　 　 N - gram 模型 当 N = 1 时 ， 成为 Unigram 模型 ， 即词 与 词 之间 互相 独立 ， 完全 没有 上下文 信息 ， 反映 的 只是 词频 统计 特性 。 这时 有 ： 
 　 　     ( 17 ) 
 　 　 当 N = 2 时 ， 即为 Bigram 模型 ， 这时 ： 
 　 　     ( 18 ) 
 　 　 通过 对 大量 训练 文本 里 词 对 ( Word   pairs ) ( Wi - 1 ,   Wi ) 出现 的 次数 C ( Wi - 1 , Wi ) 来 估计 统计 概率 ： 
 　 　     ( 19 ) 
 由此 可 进一步 对 P ( W ) 进行 估计 。 
 　 　 设 词汇表 为 V = { W1 , W2 , ... , WV } ， 个数 为 V 。 对于 Unigram 模型 和 Bigram 模型 的 模型 训练 算法 如下 ： 
 　 　 ( 1 ) 初始化 ; 
 　 　 ( 2 ) for   i = 1   to   V ; 
 　 　 ( 3 ) 从 训练 语料 里取 Wi 、 Wi - 1 ∈ V , 由式 ( 17 ) 和 式 ( 19 ) 分别 计算 词 频度 P ( Wi ) 和 词 二元 同现 频度 P ( Wi | Wi - 1 ) ； 
 　 　 ( 4 ) 如果 P ( Wi ) = 0 或 P ( Wi | Wi - 1 ) = 0 , 则 取 P ( Wi ) = ε 或 P ( Wi | Wi - 1 ) = ε ( ε 为 一 设定 的 最小 概率 ， 为 很小 的 非 零 常数 ) ; 
 　 　 ( 5 ) i = i + 1 ;   if   ( i & gt ; V ) 算法 结束 ; 
 　 　 ( 6 ) 否则 goto ( 2 ) ; 
 　 　 ( 7 ) 将 所有 的 概率 进行 规一化 计算 。 
 3.2     识别 算法 
 　 　 对于 给定 的 词 系列 W = ( W1 ,   W2 ,   ... ,   Wn ) ， 用 Viterbi 算法 进行 最优 路径 搜索 ， 求得 每个 模型 的 最佳 匹配 ， 即 由 
 　 　 ( 20 ) 
 求得 似然值 最大 的 模型 就是 识别 结果 。 
 4     实验 结果 和 讨论 
 4.1     实验 数据 
 　 　 实验 针对 大字 表 汉语 语音 识别 。 实验 中 使用 的 语料 大约 100 万 词次 ( 人民日报 文章 ) ， 语料 分成 两 部分 , 分别 记为 CORPUS1 和 CORPUS2 。 实验 分 两步 进行 ： 
 　 　 首先 ， 我们 用 第一 部分 进行 单字 二元 同现 统计 ， 选取 CORPUS1 中 出现 频度 最高 的 1 , 000 个 词 作为 高频词 集 ， 形成 统计 数据库 。 另外 ， 考虑 到 标点符号 与 句子 开头 情况 ， 将 句 中 标点 { “ ， ” ， “ 、 ” } 和 句末 标点 { “ ； ” ， “ ！ ” ， “ ？ ” ， “ ： ” ， “ 。 ” } 作为 特殊 的 高频词 放在 高频词 库中 ， 这样 高频词 库 共有 1 , 008 个 词 。 测试 中 还 用到 一个 一字 词库 ， 共 6 , 773 个 汉字 ( 即 国标 一 、 二 汉字库 ) ； 一个 二字词 库 ， 共 7 , 683 个 二字词 。 
 　 　 然后 ， 进行 识别 实验 。 我们 比较 了 两种 方法 ： 一是 没有 语音 模型 的 识别 ； 另 一种 是 加入 语言 模型 Unigram 和 Bigram 的 识别 。 测试 语料 中 ， CORPUS1 为 封闭 语料 ， CORPUS2 为 开放 语料 。 实验 时 ， 从 CORPUS1 和 CORPUS2 中 随机 抽取 若干组 数据 进行 测试 。 
 4.2     实验 结果 
 　 　 测试 结果 如表 1 和表 2 所示 。 其中 ， 表 1 是 复杂度 比较 表 。 
 表 1   复杂度 比较 表 
 
 文本 大小 复杂度 语言 模型 情况 
 无 语言 模型 Unigram 模型 Bigram 模型 
 40008PPu6773409.6166 . 3 
 29782PPb76832235.9885 . 0 
 
 表 2   识别率 测试 结果 
 
 前 n 选 识别 　 率 语言 模型 无 语言 模型 Unigram 模型 Bigram 模型 
 R1R2R1R2R1R2 
 170.6842 . 6380.7557 . 4980.6558 . 64 
 280.7258 . 7387.1167 . 8392.5969 . 07 
 390.3164 . 3893.5978 . 2696.7980 . 12 
 696.7788 . 5296.7788 . 5397.1790 . 10 
 8100.0090 . 80100.0090 . 80100.0090 . 80 
 
 其中 ， PPu 为 一 字词 的 复杂度 ， PPb 是 二字词 的 复杂度 。 结果表明 ： 有 语言 模型 时 的 复杂度 比 没有 语言 模型 时 大为 降低 ， 而 采用 高阶 N - gram 模型 比 采用 低阶 N - gram 模型 的 复杂度 有 进一步 地 降低 。 
 　 　 表 2 给出 了 从 CORPUS2 中 抽取 的 一段 文本 TEST ( 其中 含词 与 标点 180 个 ， 在 一 、 二 字字 表中 具有 的 字词 150 个 ) 进行 测试 的 识别率 结果 ， 其中 R1 为 一 字词 的 识别率 ， R2 为 二字词 的 识别率 。 
 　 　 ① 从 实验 结果 可以 看出 ， 引入 语言 模型 后 ， 语音 识别率 有 明显 的 提高 ， 而且 高阶 模型 比 低阶 模型 的 改进 效果 更 明显 ， 这 与 理论 预测 相一致 。 
 　 　 ② 统计 语言 模型 的 特点 是从 大量 的 实际 语言 材料 以自 组织 方式 获取 其中 的 语言 结构 信息 。 因此 ， 采用 不同 领域 类型 的 语料 ， 以及 保证 语料 具有 足够 的 规模 ， 是 建立 统计 语言 模型 的 重要 基础 。 本文 中 实验 用到 的 语料 的 选料 领域 比较 狭窄 ， 量 也 较 少 。 因此 不够 充分 ， 会 影响 到 语言 模型 的 性能 。 
 　 　 ③ N - gram 模型 的 一个 重要 问题 是 训练 数据 不足 引起 的 数据 稀疏 ， 因此 如何 克服 这一 问题 ， 又 不至于 导致系统 开销 的 增大 ， 是 一个 十分 重要 的 课题 。   
 5     结论 和 今后 的 工作 
 　 　 本文 提出 的 基于 统计 语言 模型 的 语音 识别方法 ， 与 基于 声学 模型 相结合 ， 能 有效 地 降低 语音 识别 的 难度 ， 并 能 有效 地 提高 识别 效果 。 在 今后 的 工作 中 ， 我们 将 进一步 研究 如下 问题 ： 
 　 　 ( 1 ) 采用 有效 的 概率 统计 技术 来 改进 N - gram 模型 ， 以 解决 数据 稀疏 问题 。 
 　 　 ( 2 ) 研究 采用 不同 统计 单元 ( 如词 、 类 ) 的 N - gram 模型 ， 以 进一步提高 识别率 。 
 　 　 ( 3 ) 改进 和 提高 统计 语言 模型 的 自适应性 。 
 本 项目 获 国家自然科学基金 部分 资助 
 吴应良 ( 华南理工大学 工商管理 学院 电子 与 通信 工程系       广州   510641 ) 
 韦岗 ( 华南理工大学 工商管理 学院 电子 与 通信 工程系     广州   510641 ) 
 李 海洲 ( 华南理工大学 工商管理 学院 电子 与 通信 工程系     广州   510641 ) 
 参考文献 
 1 ， P .   F .   Brown ,   The   Accoustic - Modeling   Problem   in   Automatic   Speech   Recongition ,   PH . D .   Thesis   CMU ,   May   1987 
 2 ， L .   R .   Rabiner ,   A   Tutorial   on   Hidden   Markov   Models   and   Selected   Applications   in   Speech   Recognition .   Proc . IEEE ,   Feb.1989 ,   77 ( 2 ) :   257 ～ 286 
 3 ， 梅     勇 ,   王 群生 ,   徐秉铮 .   基于 模糊 神经网络 的 声母 识别 .   电子 科学 学刊 ， 1998 ， 20 ( 2 ) ， 188 ～ 193 
 4 ， 杨浩荣 ， 王作英 ， 陆大 .   语音 识别 HMM 中 引入 帧 间 相关 信息 的 一种 参数 化 模型 .   电子学 报 ， 1998 ， 26 ( 10 ) :   50 ～ 54 
 5 ， 吴应良 ， 李 海洲 .   基于 N - gram 模型 的 汉语 分词 研究 .   广州 ： 中国 电子 学会 电路 与 系统 学会 第十五届 年会 论文集 ， 第 323 ～ 327 页 ， 1999 年 11 月 
 6 ， Yamamoto   M . :   A   Re - estimation   Method   for   Stochastic   Language   Modeling   from   Ambiguous   Observations ,   in   Proceeding   of   WVLC - 96 , pp.155 ～ 167 , 1996 
 7 ， Rosenfeld   R .   The   CMU   Statistical   Language   Modeling   Toolkit   and   Its   Use   in   the   1994   ARPA   CSR   Evaluation ,   In   the   Proceedings   of   ARPA   Spoken   Language   Systems   Technology   Workshop , pp.47 ～ 50 ,   1995 
 8 ， L .   R .   Bahl ,   P . F .   Brown ,   P . Desouza ,   R . L . Mercer .   A   Tree - Based   Statistic   Language   Model   for   Natural   Language   Speech   Recognition ,   K . F . Lee ,   Alex   Wabel ,   1990 
 9 ， 关     毅 ， 张     凯 ， 付国宏 .   基于 高阶 N - gram 的 单词 聚类 和 规则 获取 算法 的 研究 .   计算机 应用 研究 ， 1999 , ( 5 ) : 15 ～ 17 
 收稿 日期 ： 1999 - 12 - 18 
