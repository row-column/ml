自动化 学报 
 ACTA   AUTOMATICA   SINICA 
 1997 年 　 第 23 卷 　 第 4 期 　 Vol.23 　 No.4 　 1997 
 
 
 
 小脑 模型 神经网络 改进 算法 的 研究 1 ） 
 刘慧 　 许晓鸣 　 
 　 　 摘 　 要 　 该文 介绍 了 小脑 模型 神经网络 的 基本原理 ， 在 分析 Albus ［ 1 ］ 算法 的 基础 上 ， 指出 了 该 算法 在 批量 学习 时 的 缺陷 . 针对 批量 学习 提出 了 相应 的 改进 算法 ， 并 证明 了 该 算法 的 收敛性 ， 仿真 结果表明 了 该 改进 算法 具有 收敛 速度 快 的 特点 . 
 　 　 关键词 　 神经网络 ，   杂凑 编码 ，   联想 网络 ，   CMAC . 
 AN   IMPROVED   CMAC   NEURAL   NETWORK   ALGORITHM 
 LIU   HUI   XU   XIAOMING   
 ( Department   of   Automation   ,   Shanghai   Jiaotong   University ,   Shanghai   200030 ) 
 Abstract 　 The   basic   principle   of   CMAC   ( Cerebelllar   Model   Articulation   Controller )   is   introduced .   Based   on   a   detail   analysis   of   Albus   algorithm ［ 1 ］ ,   the   paper   points   out   its   drawback   in   batch   learning .   An   improved   algorithm   is   proposed   and   theoretical   proofs   are   also   given .   Simulation   results   show   that   the   improved   method   has   higher   speed   and   better   convergence   than   the   original . 
 Key   words 　   Neural   network ,   hash - coding ,   associative   network ,   CMAC . 
 1 　 引言 
 　 　 自 1982 年 Hopfield 发表 了 关于 反馈 神经网络 的 文章 以及 Rumelhart 等 人 发表 了 专著 PDP 以来 ， 在 世界 范围 内 掀起 了 研究 神经网络 的 热潮 . 近年来 ， 人工 神经元网络 和 人工智能 在 控制 界 中 的 应用 研究 正在 兴起 和 蓬勃发展 ， 特别 是 神经网络 具有 充分 逼近 任意 复杂 非线性 函数 的 能力 ， 为 解决 复杂 的 非线性 问题 开辟 了 一条 控制 的 具有 特殊 联想 功能 的 神经网络 CMAC ( Cerebellar   Model   Articulation   Controller ) . 
 　 　 CMAC 神经网络 是 由 J . S . Albus ［ 2 ］ 在 1975 年 提出 的 . 它 与 Perceptron 网 相似 ， 虽然 从 每个 神经元 看其 关系 是 一种 线性关系 ， 但 从 结果 总体 看 CMAC 模型 适合 于 非线性 的 映射 关系 . 同时 它 的 算法 是 十分 简单 的 δ 算法 ， 所以 速度 很快 . 它 把 输入 在 一个 多维 状态 空间 中 的 量 ， 映射 到 一个 比较 小 的 有限 区域 . 只要 对 多维 状态 空间 中 部分 样本 进行 学习 ， 就 可 达到 轨迹 学习 和 控制 的 解 ， 因此 特别 适合 于 机器人 的 轨变 学习 控制 ， 实时 学习 控制 ， 非线性 函数 映射 ， 以及 模式识别 等 领域 . CMAC 具有 自 适应 的 作用 ， 并且 易于 硬件 化 实现 . 
 2 　 CMAC 的 基本原理 
 　 　 CMAC 的 简单 结构 模型 如图 1 所示 ， 输入 空间 S 由 所有 可能 的 输入 向量 Si 组成 ， CMAC 网络 将 其 接受 到 的 任何 输入 ， 通过 感知器 M 映射 到 一个 很大 的 联想 存储器 A 中 的 c 个 单元 . 输入 空间 邻近 的 两个 输入 向量 在 存储器 A 中有 部分 重叠 的 单元 ； 距离 越近 ， 重叠 越多 . 反之 ， 若 输入 空间 远离 的 两个 输入 向量 在 A 中 并 不 重叠 . 对 一个 实际 系统 ， 输入 空间 中 的 向量 数是 很大 的 ， 例如 ， 某 系统 有 10 个 输入 ， 而 每个 输入 可取 100 个值 ， 则 在 输入 空间 有 10010 个 向量 ， A 的 存储量 需要 10010 个 单元 . 由于 绝大多数 学习 问题 并 不 包括 所有 输入 空间 中 的 状态 ， 故所 需 的 存储空间 可 通过 常用 的 计算机 存储 压缩 技术 — — 随机 杂凑 编码 ( Hash - coding ) 技术 映射 到 一个 小得多 的 物理 可 实现 的 存储器 A ′ . 任何 CMAC 网络 的 输入 激活 c 个 真实 存储 位置 ， 而 这些 位置 的 值 被 相加 得到 输出 向量 P . 
 
 
 图 1 　 CMAC 网格 的 模型 结构 
 3 　 Albus 的 CMAC 算法 
 　 　 设 共有 P 个 训练样本 ， 联想 向量 ai 为 M → A 的 映射 ， CMAC 网络 对于 第 i 个 训练样本 的 输出 为 f ( Si ） = wTai ， 理想 输出 为 di ， 误差 信号 δ i = di - f ( si ) ， CMAC 是 按 δ 学习律 调整 网络 的 权值 ， 是 在 梯度 法 的 基础 上 采用 LMS 算法 得到 的 ［ 2 ］ 
 
 ( 1 ) 
 其中 β 为 学习 步长 . 
 　 　 当 我们 在 第 l 次 循环 时 对 第 k 个 样本 进行 训练 时 ， 产生 的 修整 量 为 β / c ， 其中 为 第 k 个 样本 的 输出 误差 . 因 CMAC 网络 内在 的 泛化 能力 ， 在 输入 空间 相近 的 向量 在 实际 存储器 A ′ 中有 重叠 ， 故该 修正 量 势必 影响 其它 样本 的 输出 ， 如 它 使 第 i 个 样本 的 新 输出 为 
 
 ( 2 ) 
 其中 cik 为 第 i , k 个 样本 在 A ′ 中 的 重叠 单元 数 ， 显然 cik = cki . 经 I 次 循环 后 ， 输入 样本 k 产生 的 修正 量 对 其 本身 的 累积 输出 误差 为 ， 与 之 相关 的 每个 权 的 修正 量 为 Δ k ＝ Ek ／ c ， Δ k 称为 累积 权 误差 . 累积 权 误差 对 第 i 个 样本 的 输出 贡献 为 cik Δ k . 当 CMAC 算法 收敛 ， → 0 ， 则 Δ k → const . 若 我们 由 Δ i 得 wi ， 则 在 网络 算法 中可换 一个 角度 将 累积 权 误差 Δ i 而 不是 权 wi 视为 需要 学习 的 变量 ， 这样 CMAC 的 收敛 也 可 转化 为 Δ i 的 收敛 问题 . 设 网络 初始 权为 零 ， 则 对 每个 输入 向量 si ， 网络 的 输出 为 
 
 ( 3 ) 
 学习 的 目的 是 使 f ( si ) = di ， 即 ， 写成 矩阵 形式 
 C Δ ＝ D 
 ( 4 ) 
 令 　 A = ［ a1   a2 … ap ］ T ， 则 
 AW = D 
 ( 5 ) 
 　 　 通常 神经网络 权 的 学习 有 逐一 和 批量 两种 方法 ， 设 训练样本 集 为 ｛ （ s1 , d1 ) , ( s2 , d2 ) , … ， ( sp , dp ) ｝ . 逐一 学习 是 每 输入 一个 样本 便 对 整个 网络 的 权 进行 修正 . 即先 对 ( s1 , d1 ) 进行 学习 ， 利用 输出 误差 来 对 网络 的 权 进行 修正 ， 使 网络 的 对应 关系 满足 f ( s1 ) = d1 ， 设 此时 网络 的 权为 w1 . 然后 以 w1 为 基础 ， 对 样本 ( s2 , d2 ) 进行 学习 ， 直到 权值 w2 满足 f ( s2 ) = d2 . 但是 一般 情况 下 ， w1 ≠ w2 ， 故 此时 未必 有 f ( s1 ) = d1 成立 . 所以 逐一 学习 的 方法 会 出现 学 了 新 的 ， 忘 了 旧 的 “ 遗忘 ” 现象 . 为了 克服 这个 缺点 需 采用 反复 循环 学习 ， 但 这样 又 会 带来 收敛 慢 的 问题 . 我们 建议 用 批量 学习 的 方法 来 对权 进行 更新 ， 将 样本 一一 输入 ， 为 一批 样本 输入 后用 总 的 误差 来 修整 权 ， 这种 方法 可以 克服 “ 遗忘 ” 的 缺点 ， 又 有 较 快 的 收敛 速度 . 
 　 　 Albus 本人 并未 给出 CMAC 算法 收敛性 的 证明 ， Wong ［ 3 ］ 和 Parks ［ 4 ］ 从 线性方程组 迭代 解 的 角度 ， Wong ［ 5 ］ 还 从 频域 的 角度 阐述 了 Albus 算法 的 收敛性 问题 . 他们 的 研究 都 是 针对 逐一 学习 的 ， 对 批量 学习 的 情况 并未 进行 研究 . 作者 针对 批量 学习 情况 进行 了 一些 探索 ， 指出 了 Albus 算法 在 逐一 学习 时 适用 ， 而 在 批量 学习 时易 发散 的 缺陷 ， 并 提出 了 相应 的 改进 算法 . 
 4 　 批量 学习 时 Albus 算法 的 缺陷 及 改进 算法 
 　 　 定理 1 . 对于 多 输入输出 目标 中 的 一组 训练样本 ， 如果 输入 空间 被 量化 后 不 存在 两个 不同 训练样本 激活 相同 一组 神经元 的 情况 ， 则 Albus 算法 采用 批量 学习 方法 时 的 收敛 条件 为 
 　 　 证明 . 当 Albus 算法 采用 批量 学习 时 ， 累积 权 误差 的 修正 公式 为 
 
 ( 6 ) 
 其中 
 
 ( 7 ) 
 　 　 为 方便 起 见 ， 设 输入输出 空间 均 为 一维 ， 所有 训练样本 的 量化 值 都 在 ［ 0 ， R ］ 上 ， 则 所 需 神经元 为 R + c - 1 个 . 
 　 　 当 收敛 到 Δ * 时 ， 由式 ( 6 ) 得 
 
 ( 8 ) 
 令 　 ， 得 
 
 ( 9 ) 
 　 　 e 为 一 有限 长 的 N1 点非 周期 序列 ， N1 ＝ R + 2c - 2 . 将 其作 周期 延拓 ， e 可 写成 Fourier 级数 形式 
 
 ( 10 ) 
 　 　 将 ( 10 ) 代入 ( 9 ) 并 考虑 ( 7 ) 可 得 
 
 ( 11 ) 
 其中 
 
 ( 12 ) 
 c 一般 取为 32 ～ 256 ， 故 sin ( c ω ) / c 可 忽略 . 若 ω ≠ 2 π ， 则 
 
 ( 13 ) 
 　 　 当且 仅 当 ｜ H ( ej ω ) ｜ ＜ 1 时 算法 收敛 ， 在 一个 数字 频率 周期 ( 2 π ) 内 ｜ H ( ej ω ) ｜ ＜ 1 ， 即 
 
 ( 14 ) 
 　 　 当 ω = 2 π 时 ， ω ∈ Ker ( A ) ， 不 影响 w 和 Δ 的 收敛 . 所以 Albus 算法 采用 批量 学习 时 的 收敛 条件 为 
 　 　 推论 1 . 对于 多 输入输出 目标 中 的 一组 训练样本 ， 设 所有 训练样本 的 量化 值 都 在 ［ 0 ， R ］ 上 ， 感受 野 宽度 为 c . 如果 输入 空间 被 量化 后 不 存在 两个 不同 训练样本 激活 相同 一组 神经元 的 情况 ， 则 Albus 算法 采用 批量 学习 方法 时 的 收敛 条件 为 
 
 　 　 证明 . 　 将 代入 式 ( 14 ) ， 得 
 　 　 　 
 
 ( 15 ) 
 　 　 由 Fourier 级数 ( 10 ) 知 ， k 的 取值 为 ［ 0 , N , - 1 ］ 间 的 连续 整数 ， 而 采用 Albus 算法 批量 学习 时 的 收敛 条件 为 ， k 在 和 两段 取值 时 算法 不 收敛 . N1 越大 ， 即 量化 取值 数 R 和 感受 野 宽度 c 越大 ， 不 收敛 的 点 越 多 . 
 　 　 为了 克服 Albus 算法 对 批量 学习 不太 适合 的 缺陷 ， 提出 了 一种 改进 的 CMAC 算法 把 式 ( 1 ) 修改 为 
 
 ( 16 ) 
 其中 q 为 批量 学习 一次 每个 权 平均 被 更新 的 次数 . 
 　 　 定理 2 . 对于 多 输入输出 目标 中 的 一组 训练样本 ， 如果 输入 空间 被 量化 后 不 存在 两个 不同 训练样本 激活 相同 一组 神经元 的 情况 ， 改进 CMAC 算法 采用 批量 学习 方法 时 的 收敛 条件 为 2ctg - 1q ＜ ω ＜ 2 （ π - ctg - 1q ) . 
 　 　 证明 . 　 改进 CMAC 算法 采用 批量 方法 对权 进行 更新 时 ， 相应 的 算法 收敛 证明 中 的 公式 ( 13 ) 变为 
 
 ( 17 ) 
 　 　 当且 仅 当 ｜ H ( ej ω ) ｜ ＜ 1 时 算法 收敛 ， 在 一个 数字 频率 周期 ( 2 π ） 内 ｜ H ( ej ω ) ｜ ＜ 1 ， 即 
 2ctg － 1q ＜ ω ＜ 2 ( π - ctg - 1q ) . 
 ( 18 ) 
 　 　 当 ω = 2 π 时 ， ω ∈ Ker ( A ) ， 不 影响 w 和 Δ 的 收敛 . 所以 改进 CMAC 算法 采用 批量 学习 时 的 收敛 条件 为 2ctg - 1q ＜ ω ＜ 2 ( π - ctg - 1q ) . 
 　 　 讨论 . 由于 CMAC 网络 所 固有 的 局域 泛化 能力 ， 每个 输入 向量 使 其 对应 量化 值 周围 c 个 感知器 同时 被 激励 ； 相应 地 ， 当 一批 样本 被 学习 一遍 后 ， 每个 网络 的 权 也 不止一次 被 更新 . 所以 minq = 2 . 当 q 取 最小值 2 时 ， 由 ( 18 ) 得 改进 算法 的 收敛 条件 为 0.93 ＜ ω ＜ 5.35 弧度 ， 改进 算法 的 收敛 范围 比 Albus 算法 ( 14 ) 宽 ， q 越大 改进 算法 的 收敛 范围 将 越 宽 . 
 　 　 推论 2 . 对于 多 输入输出 目标 中 的 一组 训练样本 ， 设 所有 训练样本 的 量化 值 都 在 ［ 0 ， R ］ 上 ， 感受 野 宽度 为 c . 如果 输入 空间 被 量化 后 不 存在 两个 不同 训练样本 激活 相同 一组 神经元 的 情况 ， 改进 CMAC 算法 采用 批量 学习 方法 时 的 收敛 条件 为 
 　 　 证明 . 　 将 代入 式 ( 18 ) 即可 得 改进 算法 采用 批量 学习 时 的 收敛 条件 为 
 
 ( 19 ) 
 　 　 讨论 . 将 ( 19 ) 进一步 展开 为 
 
 ( 20 ) 
 其中 
 
 ( 21 ) 
 　 　 当 q 取 最小值 2 时 ， ctg - 1q = 0.46 弧度 ， 而 反 余切 函数 在 区间 ［ 0 ， π ］ 内 是 递减 的 ， 所以 ctg - 1q ＜ 0.46 ， 从而 b1 ＞ 0 , b2 ＞ 0 . 可见 改进 算法 的 收敛 范围 比 Albus 算法 宽 . q 越大则 b1 和 b2 越大 ， 改进 算法 的 收敛 范围 也 将 越 宽 . 
 　 　 改进 CMAC 算法 对 逐一 学习 也 是 适用 的 ， 其 收敛性 证明 可 参见 文献 ［ 4 ］ ， 其中 累积 权 误差 的 修正 类似 Jocobi 迭代法 解 线性方程组 . 
 5 　 仿真 结果 
 　 　 设某 CMAC 网络 用来 实现 下列 一维 非线性 函数 的 映射 ， 
 
 ( 22 ) 
 其中 输入 变量 si ∈ ［ 0 ， 180 ］ ， 分辨率 为 1 ， 所有 训练样本 的 量化 值 都 在 ［ 0 ， R ］ 上 ， R = 180 . CMAC 网络 的 感受 野 宽度 c = 32 , 批量 学习 一次 每个 权 平均 被 更新 的 次数 q 取为 6 . 则 N1 = 242 ， ctg - 1q = 0.17 . 按 Δ s = 5 的 间隔 均匀 取点 ， 由式 ( 22 ) 进行 理论 计算 ， 可得 37 组 输入输出 的 样本 数据 ， 运用 这些 样本 数据 分别 按 Albus 算法 和 本文 的 改进 算法 对 CMAC 神经网络 进行 训练 . 为了 显示 网络 的 泛化 能力 ， 计算 网络 输出 时 采用 了 不同于 训练 时 的 数据 Δ s = 6 . 由式 ( 15 ) 计算 得 Albus 算法 批量 训练 时 的 收敛 条件 为 60 ＜ k ＜ 182 ; 而由式 ( 19 ) 知 12 ＜ k ＜ 230 时 本文 的 改进 算法 批量 训练 时 收敛 ， 易 见 改进 后 的 算法 比 Albus 算法 收敛 条件 宽 . 
 　 　 图 2 所示 为 逐一 学习 时 的 误差 曲线 ， 两种 算法 都 可 取得 较 好 的 收敛 效果 ， 而 本文 提出 的 改进 方法 收敛 速度 更 快 一些 . 图 3 是 在 批量 学习 时 的 误差 曲线 ， 由该 图 可见 ， Albus 算法 易 导致 发散 ， 而 采用 本文 的 改进 算法 可以 获得 收敛 较 快 的 效果 . 
 
 
 图 2 　 逐一 学习 误差 曲线 
 
 
 图 3 　 批量 学习 误差 曲线 
 6 　 结论 
 　 　 本文 在 对 Albus 算法 研究 的 基础 上 ， 提出 了 适合 于 批量 学习 的 改进 算法 ， 并 在 理论 上 证明 了 该 算法 的 收敛性 ， 仿真 结果表明 了 该 算法 在 逐一 和 批量 学习 时 都 有 较 好 的 收敛性 和 快速性 . 
 1 ) 　 留学 回国 人员 科研 基金 资助 课题 . 
 作者简介 : 刘 　 慧 　 1968 年生 ， 1996 年 获 上海交通大学 自动控制 理论 及 应用 专业 博士学位 . 现为 上海交通大学 自动化系 讲师 . 主要 研究 兴趣 是 智能 控制 ， 学习 控制 及 神经网络 在 控制 中 的 应用 . 
 　 　 许晓鸣 　 1957 年生 ， 现任 上海交通大学 自动化系 教授 、 博士生 导师 . 主要 研究 兴趣 是 智能 控制 和 复杂 工业 系统 预测 控制 等 方面 的 研究 . 
 作者 单位 : 上海交通大学 自动化系 　 上海 　 200030 
 参考文献 
 ［ 1 ］ 　 Albus   J   S .   Data   storage   in   the   cerebellar   model   articulation   controller   ( CM . AC ) .   Trans .   ASME ,   J . Dyn .   Syst .   Meas .   Contr . ,   1975 , 97 : 228 — 233 . 
 ［ 2 ］ 　 Albus   J   S .   A   new   approach   to   manipulator   control :   The   cerebellar   model   articulation   Controller   ( CMAC ) .   Trans .   ASME ,   J . Dyn .   Syst .   Meas .   Contr . ,   1975 ,   97 : 220 — 227 . 
 ［ 3 ］ 　 Wong   Y   F , Sideris   A .   Learning   convergence   in   the   cerebellar   model   articulation   controller   . IEEE   Trans .   on   Neural   Networks ,   1992 ,   3 ( 1 ) : 115 — 121 . 
 ［ 4 ］ 　 Parks   P   C ,   Militzer   J .   Convergence   properties   of   associative   memory   storage   for   learning   control   system .   Automation   and   Remote   Control ,   1989 ,   50 ( 2 ) : 254 — 286 . 
 ［ 5 ］ 　 Wong   Y   F .   CMAC   learning   is   governed   by   a   single   parameter .   IEEE   Int .   Conf .   on   NN ,   1993 ,   1439 — 1443 . 
 收稿 日期 　 1995 - 04 - 10 
