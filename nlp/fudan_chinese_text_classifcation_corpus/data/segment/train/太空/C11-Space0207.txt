宇航学 报 
 JOURNAL   OF   ASTRONAUTICS 
 1999 年 　 第 1 期 　 No.1 　 1999 
 
 
 
 PVM 环境 下 提高 并行计算 加速 比 的 
 数值 实验 研究  * 
 李 　 桦 　 王承尧 　 王正华 
 　 　 摘 　 要 　 通过 采用 区域分割 技术 和 拼接 网络 的 方法 , 在 PVM 并行 环境 下 , 对 三维 高超音速 绕 流流 场 进行 了 多机 并行计算 , 得到 了 较 高 的 加速 比 。 通过 数值 实验 研究 , 讨论 了 负载平衡 、 不同 的 网格 规模 对 加速 比 的 影响 。 
 　 　 主题词 　 TVD 隐式 格式 　 三维 高超音速 流场 　 多机 并行计算 　 加速 比 
 NUMERICAL   RESEARCH   OF   RAISING   SPEEDUP   
 OF   PARALLEL   CALCULATION   ON   PVM 
 Li   Hua   Wang   Chengyao   Wang   Zhenghua 
 ( Department   of   Aerospace   Technology , NUDT . Changsha.410073 ) 
 　 　 Abstract 　 In   this   paper , used   the   domain   decomposition   technique   ( DDT )   and   patched   grid , Parallel   numerical   calculation   with   TVD   implicit   finite   volume   algorithm   is   carried   for   3D   hypersonic   viscous   flowfield   on   PVM . The   high   speedup   is   obtained , and   the   influnce   of   blancing   load , different   grid   scale   to   the   speedup   is   discussed . 
 　 　 Key   words 　 TVD   implicit   scheme   3D   hypersonic   flowfield   Parallel   calculation   Speedup 
 1 　 引言 
 　 　 在 计算 流体力学 ( CFD ) 中 , 由于 超级 并行 计算机 能够 用来 求解 大规模 的 数值 计算 问题 , 满足 工程 设计部门 的 需要 , 因此 , 多机 并行计算 成为 CDF 中 一个 重要 的 发展 方向 。 近年来 , 国内外 对 将 多机 并行计算 与 CFD 相结合 的 研究 十分重视 , 在 这方面 开展 了 许多 有益 的 工作 , 取得 了 很大 进展 ［ 1 , 2 , 3 ］ 。 
 　 　 本文 根据 并行 虚拟 计算机 ( Parallel   Virtual   Machine , 简称 PVM ) 并行 环境 下多 机系统 的 特点 和 要求 , 利用 区域分割 技术 和 拼接 网络 ( Patched   Grid ) 对 TVD 隐式 有限 体积 法 的 计算 程序 进行 了 重构 , 在 PVM 二 处理机 和 四 处理机 系统 上 实现 了 三维 钝头 双 锥体 绕 流流 场 的 多机 并行计算 , 得到 了 较 高 的 加速 比 ; 在 数值 实验 研究 中 , 讨论 了 负载平衡 、 不同 的 网络 规模 对 并行计算 加速 比 的 影响 。 
 2 　 并行 策略 
 　 　 为了 在 PVM 多 机系统 上 实现 TVD 稳式 有限 体积 法 的 多机 并行计算 , 我们 根据 PVM 环境 的 特点 , 采用 了 下列 措施 : 
 　 　 ( 1 ) 区域分割 技术 
 　 　 区域分割 技术 是 多机 并行计算 中 常用 的 并行 策略 之一 。 在 多机 并行计算 中 , 流场 分割 的 数目 一般 与 处理机 台数 相对 应 , 它 通过 将 要求 解 的 流场 划分 为 若干个 子 区 , 每个 处理机 分别 计算 一个 或 几个 子 区 , 来 完成 整个 流场 的 计算 工作 。 
 　 　 为了 进一步 说明 , 以用 一个 四 处理机 系统 求解 二维 流场 为例 。 通过 区域分割 技术 , 我们 将 流场 划分 为 四个 子 区 , 如图 1 所示 。 
 
 
 图 1 　 二维 流场 分区 示意图 
 　 　 二维 的 NS 方程 经过 离散 后 可以 写为 ［ 3 ］ : 
 
 上 式 左端 为 隐式 部分 , 右端 Ri , j 是 余量 , 用 n 层 的 值 求解 , 为 显式 部分 。 
 　 　 对于 整个 求解 区域 , 合成 的 代数方程 组 可以 写成 : 
 ［ A ］ { δ Q } = { R } 
 　 　 对于 二维 问题 , ［ A ］ 是 由 4 × 4 块 矩阵 组成 , 即 : 
 
 式 中 , 在 对角线 上 的 块 矩阵 { Aii } 是 子区 的 求解 矩阵 , 矩阵 { Aij } 表示 在 i 子区 的 求解 过程 中 , 在 区域 边界 上 与 j 子区 的 关联矩阵 。 
 　 　 显然 , 在 子区 的 内点 求解 过程 中 , { Aij } 均 为 零 矩阵 ; 另外 , 如果 i 子区 与 j 子区 之间 没有 公共 区域 边界 , 则 { Aij } 为 零 矩阵 。 例如 , 参照 图 1 , 在 子区 1 的 求解 中 , 由于 该区 与 第 3 子区 没有 公共 边界 区域 边界 , 则 { A13 } 为 零 矩阵 。 
 　 　 在 单个 处理机 上 , 对于 每个 子区 的 求解 , 采用 LU - SSOR 迭代 方法 , 即 : 
 L1N - 1U1 δ Q ( i ) = R ( i ) - { Aij } δ Qij 
 其中 , i 、 j 表示 子 区 序号 , 且 i , j = 1 , 2 , 3 , 4 , 且 δ Qij 为 在 子区 i 的 区域 边界 的 求解 中 用到 j 的 区域 边界 上 的 变量 增量 , Li 、 Ui 的 表达式 详见 文献 ［ 5 ］ 。 
 　 　 ( 2 ) 拼接 网络 
 　 　 在 多机 并行计算 中 , 开销 最大 的 是 处理机 之间 的 通讯 开销 。 对于 PVM 环境 来说 , 其 网络 性能 不高 , 传输数据 的 速度 较慢 , 因此 , 怎样 减少 处理机 之间 的 数据通讯 量 , 则 是 在 提高 多机 并行计算 加速 比时 重点 考虑 的 问题 。 
 　 　 采用 区域分割 技术 进行 多机 并行计算 , 在 子区 中 采用 什么样 的 网格 体系 将 直接 影响 到 区域 之间 , 也 就是 各 处理机 之间 的 数据通讯 量 的 大小 。 
 　 　 在 结构 网格 中 , 分区 的 网格 体系 有 两种 : 拼接 网络 ( Patched   Grid ) 和 重叠 网格 ( Overlap   Grid ) 。 在 重叠 网格 中 , 对于 n 维 问题 , 边界 上 插值 是 n 维 的 , 而 拼接 网格 的 边界 上 插值 为 ( n - 1 ) 维 ; 显然 , 拼接 网格 在 区域 边界 上 的 数据 传送 量 要 小于 重叠 网格 。 因此 , 拼接 网格 适合 于 本文 所 构造 的 多机 并行算法 ［ 4 ］ 。 
 3 　 数值 实验 及 结果 
 　 　 本文 在 PVM 多 机系统 上以 高超音速 钝头 双 锥体 绕 流流 场 的 分区 多机 并行计算 为算例 , 讨论 了 负载平衡 和 网格 规模 对 并行计算 加速 比 的 影响 。 在 并行计算 中 , 对应 于 处理机 台数 , 对流 场 进行 了 分区 , 流场 分区 示意图 如果 图 2 所示 。 
 　 　 计算 参数 为 : 
 M ∞ = 9.9 , 　 Re ∞ = 2.2 × 105 , 　 T ∞ = 49.8 K , 攻角 α = 4 ° 
 
 
 ( a ) 流场 分二区 示意图 　 　 ( b ) 流场 分 四区 示意图 
 图 2 　 流场 分区 示意图 
 3.1 　 负载平衡 对 算法 加速 比 的 影响 
 　 　 在 本文 研究 的 多区 多机 并行计算 中 , 当 每个 子 区 进行 完 一次 迭代 扫描 时 , 在 区域 边界 上要 进行 数据交换 , 也就是说 , 在 这个 算法 中 存在 一个 同步 点 。 在 这种 情况 下 , 如果 各个 子区 的 计算 规模 , 即 负载 不同 的话 , 那么 规模 小 的 子区 在 计算 完成 后 需要 等待 规模 大 的 子区 。 这样 就 影响 了 并行 效率 。 因此 , 在 该 多区 多机 并行算法 中 , 平衡 负载 是 提高 并行 效率 最 直接 有效 的 手段 。 
 　 　 本文 采用 的 平衡 负载 的 方法 是 静态平衡 负载 。 这种 方法 对 问题 的 分解 和 任务 的 分配 只 进行 一次 。 它 在 任务 之前 进行 负载 划分 。 任务 的 大小 及 分配 给 某台 处理机 的 任务 数据 每台 处理机 的 计算能力 确定 。 
 　 　 整个 流场 的 计算 网格 为 57 × 41 × 21 , 在 分区 多机 并行计算 中 , 各个 子区 的 网格 划分 如下 : 
 　 　 二 处理机 并行计算 网络 : 
 　 　 　 　 负载平衡 前 : Ⅰ 区 的 计算 网格 : 37 × 41 × 21 　 Ⅱ 区 的 计算 网格 : 22 × 41 × 21 
 　 　 　 　 负载平衡 后 : Ⅰ 区 的 计算 网格 : 30 × 41 × 21 　 Ⅱ 区 的 计算 网格 : 29 × 41 × 21 
 　 　 四 处理机 并行计算 网格 : 
 　 　 　 　 负载平衡 前 : Ⅰ 区 的 计算 网格 : 37 × 21 × 21 
 　 　 　 　 　 　 　 　 　 Ⅱ 区 的 计算 网格 : 37 × 22 × 21 
 　 　 　 　 　 　 　 　 　 Ⅲ 区 的 计算 网格 : 22 × 22 × 21 
 　 　 　 　 　 　 　 　 　 Ⅳ 区 的 计算 网格 : 22 × 21 × 21 
 　 　 　 　 负载平衡 后 : Ⅰ 区 的 计算 网格 : 30 × 21 × 21 
 　 　 　 　 　 　 　 　 　 Ⅱ 区 的 计算 网格 : 30 × 22 × 21 
 　 　 　 　 　 　 　 　 　 Ⅲ 区 的 计算 网格 : 29 × 22 × 21 
 　 　 　 　 　 　 　 　 　 Ⅳ 区 的 计算 网格 : 29 × 21 × 21 　 
 　 　 在 多机 并行计算 中 , 通过 平衡 负载 , 有效 地 提高 了 整个 算法 的 加速 比 , 其 结果 如表 1 所示 。 
 　 　 表 1 中 加速 比 Sp 的 计算公式 为 : 
 
 表 1 　 多机 并行计算 数值 实验 结果 
 加速 比二 处理机 四 处理机 
 负载平衡 前 1.613 . 06 
 负载平衡 后 1.843 . 44 
 　 　 从表 1 中 可以 看到 , 在 我们 研究 的 并行算法 中 , 负载平衡 是 提高 算法 的 加速 比 的 有效 手段 。 
 3.2 　 网格 规模 对 算法 加速 比 的 影响 
 　 　 本文 在 进行 计算 的 数值 实验 中 , 观察 了 不同 的 网格 规模 对 算法 加速 比 的 影响 , 具体 结果 如表 2 所示 。 
 　 　 从表 2 看到 , 在 小规模 的 PVM 并行 环境 中 , 当 处理机 台数 不变 时 , 所 计算 的 问题 规模 越大 , 并行算法 的 加速 比越 高 。 
 表 2 　 不同 网格 规模 对 加速 比 的 影响 
 加速 比单 处理机 二 处理机 四 处理机 
 网格 规模 : 41 × 31 × 131.01 . 773.21 
 网格 规模 : 57 × 41 × 211.01 . 843.44 
 网格 规模 : 71 × 51 × 311.01 . 883.59 
 4 　 结论 
 　 　 ( 1 ) 本文 发展 的 TVD 隐式 有限 体积 法 的 多机 并行算法 在 SGI 工作站 为 结点 , 以太网 连接 的 PVM 并行 环境 中 对 钝 头 双 锥体 高速 绕 流流 场 进行 了 多机 并行计算 数值 实验 , 通过 负载平衡 的 方法 较 好 地 提高 了 算法 的 加速 比 , 在 二 处理机 上 加速 比为 1.84 , 在 四 处理机 上 加速 比为 3.44 。 
 　 　 ( 2 ) 在 数值 实验 中 , 讨论 了 不同 的 网格 规模 对 算法 的 并行 效率 的 影响 。 发现 : 在 小规模 的 PVM 并行 环境 中 , 当 处理机 台数 不变 时 , 所 计算 的 问题 规模 越大 , 算法 的 加速 比越 高 。 
 * 国家自然科学基金 资助 ( 19482002 
 作者 单位 ： 李桦 　 王承尧 　 王正华 　 ( 国防科技大学 航天技术 系 。 长沙 。 410073 ) 
 参考文献 
 　 [ 1 ] Gustason   J   L , Montry   G   R   and   Benner   R   E . Development   of   parallel   methods   for   a   1024   processer   hypercube . SIAM   Journal   on   scientific   and   statistical   computing , 1988 , 3 ( 4 ) 
 　 [ 2 ] Olssen   P , and   Johnson   S . Adataparallel   implemstation   of   an   explicit   method   for   the   threedimension   comprseeible   NS   equation . Parallel   Computing , 1990 
 　 [ 3 ] 王正华 , 王承尧 . 显 格式 宏 任务 的 多机 并行计算 . 空气 动力学 报 , 1995 , ( 2 ) 
 　 [ 4 ] Li , H   and   Wang   C   Y . Implicit   TVD   schemes   for   the   solution   of   three   dimension   hypersonic   jet   flowfield . AIAA   paper , 96 - 03165 
 　 [ 5 ] 李桦 . 三维 超音速 / 高超音速 复杂 流场 分区 多机 并行计算 与 实验 验证 . 国防科技大学 博士论文 , 1996 
 收稿 日期 : 1996 年 9 月 6 日 , 修回 日期 : 1998 年 1 月 8 日 
 　 　 ) 
